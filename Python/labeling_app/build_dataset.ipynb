{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Pipeline: Test/Train Split & HuggingFace Upload\n",
    "\n",
    "Run this notebook after each annotation session to:\n",
    "1. Load and clean labeled data\n",
    "2. Create a fixed test split (once, never modified)\n",
    "3. Build the train split from remaining annotations\n",
    "4. Push both to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zorbeyozcan/news_articles_classification_thesis/Python/labeling_app/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw articles: ../../data/articles/cleaned_articles.csv\n",
      "Labeled articles: ../../data/articles/cleaned_articles_labeled.csv\n",
      "Test split: ../../data/articles/test_split.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# === CONFIG ===\n",
    "HF_REPO = \"Zorryy/news_articles_2025_elections_germany\"\n",
    "TEST_SAMPLES_PER_CLASS = 50\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# === PATHS ===\n",
    "DATA_DIR = os.path.join(\"..\", \"..\", \"data\", \"articles\")\n",
    "RAW_CSV = os.path.join(DATA_DIR, \"cleaned_articles.csv\")\n",
    "LABELED_CSV = os.path.join(DATA_DIR, \"cleaned_articles_labeled.csv\")\n",
    "TEST_SPLIT_CSV = os.path.join(DATA_DIR, \"test_split.csv\")\n",
    "\n",
    "# Verify files exist\n",
    "assert os.path.exists(RAW_CSV), f\"Raw CSV not found: {RAW_CSV}\"\n",
    "assert os.path.exists(LABELED_CSV), f\"Labeled CSV not found: {LABELED_CSV}\"\n",
    "print(f\"Raw articles: {RAW_CSV}\")\n",
    "print(f\"Labeled articles: {LABELED_CSV}\")\n",
    "print(f\"Test split: {TEST_SPLIT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load labeled data & remove non-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in labeled CSV: 2924\n",
      "After removing skipped/not_clean: 1921\n",
      "After deduplication: 1921\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "Ukraine/Krieg/Russland      204\n",
      "Zuwanderung                 201\n",
      "Andere                      200\n",
      "Wirtschaftslage             200\n",
      "Klima / Energie             200\n",
      "AfD/Rechte                  200\n",
      "Bundeswehr/Verteidigung     176\n",
      "Renten                      145\n",
      "Gesundheitswesen, Pflege    141\n",
      "Arbeitslosigkeit            107\n",
      "Soziales Gefälle             70\n",
      "Kosten/Löhne/Preise          59\n",
      "Politikverdruss              18\n"
     ]
    }
   ],
   "source": [
    "# Increase CSV field size limit for large text fields\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "labeled_df = pd.read_csv(LABELED_CSV, encoding=\"utf-8\")\n",
    "print(f\"Total rows in labeled CSV: {len(labeled_df)}\")\n",
    "\n",
    "# Remove non-labels (skipped, not_clean)\n",
    "non_labels = [\"skipped\", \"not_clean\"]\n",
    "labeled_df = labeled_df[~labeled_df[\"label\"].isin(non_labels)].copy()\n",
    "print(f\"After removing skipped/not_clean: {len(labeled_df)}\")\n",
    "\n",
    "# Deduplicate: keep last label per article ID\n",
    "labeled_df = labeled_df.drop_duplicates(subset=\"id\", keep=\"last\")\n",
    "print(f\"After deduplication: {len(labeled_df)}\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(labeled_df[\"label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create or load test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split already exists. Loaded 617 entries.\n",
      "(Test split is frozen and will not be modified.)\n",
      "\n",
      "Test split distribution:\n",
      "label\n",
      "AfD/Rechte                  50\n",
      "Andere                      50\n",
      "Arbeitslosigkeit            50\n",
      "Bundeswehr/Verteidigung     50\n",
      "Gesundheitswesen, Pflege    50\n",
      "Klima / Energie             50\n",
      "Kosten/Löhne/Preise         50\n",
      "Renten                      50\n",
      "Soziales Gefälle            50\n",
      "Ukraine/Krieg/Russland      50\n",
      "Wirtschaftslage             50\n",
      "Zuwanderung                 50\n",
      "Politikverdruss             17\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TEST_SPLIT_CSV):\n",
    "    # Load existing test split - NEVER modify it\n",
    "    test_ids_df = pd.read_csv(TEST_SPLIT_CSV, encoding=\"utf-8\")\n",
    "    print(f\"Test split already exists. Loaded {len(test_ids_df)} entries.\")\n",
    "    print(\"(Test split is frozen and will not be modified.)\")\n",
    "else:\n",
    "    # Create new test split: up to TEST_SAMPLES_PER_CLASS per class\n",
    "    print(f\"Creating new test split with up to {TEST_SAMPLES_PER_CLASS} samples per class...\")\n",
    "    test_samples = []\n",
    "    for label in sorted(labeled_df[\"label\"].unique()):\n",
    "        class_df = labeled_df[labeled_df[\"label\"] == label]\n",
    "        n = min(len(class_df), TEST_SAMPLES_PER_CLASS)\n",
    "        sampled = class_df.sample(n=n, random_state=RANDOM_SEED)\n",
    "        test_samples.append(sampled)\n",
    "        print(f\"  {label}: {n} samples (available: {len(class_df)})\")\n",
    "\n",
    "    test_ids_df = pd.concat(test_samples)[[\"id\", \"label\"]]\n",
    "    test_ids_df.to_csv(TEST_SPLIT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nTest split saved: {len(test_ids_df)} total entries\")\n",
    "\n",
    "print(\"\\nTest split distribution:\")\n",
    "print(test_ids_df[\"label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build train split (all labeled data NOT in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 1304 labeled articles\n",
      "Test split:  617 labeled articles\n",
      "Total:       1921\n",
      "\n",
      "Train split distribution:\n",
      "label\n",
      "Ukraine/Krieg/Russland      154\n",
      "Zuwanderung                 151\n",
      "Andere                      150\n",
      "Klima / Energie             150\n",
      "AfD/Rechte                  150\n",
      "Wirtschaftslage             150\n",
      "Bundeswehr/Verteidigung     126\n",
      "Renten                       95\n",
      "Gesundheitswesen, Pflege     91\n",
      "Arbeitslosigkeit             57\n",
      "Soziales Gefälle             20\n",
      "Kosten/Löhne/Preise           9\n",
      "Politikverdruss               1\n"
     ]
    }
   ],
   "source": [
    "test_ids = set(test_ids_df[\"id\"].tolist())\n",
    "train_labels_df = labeled_df[~labeled_df[\"id\"].isin(test_ids)][[\"id\", \"label\"]].copy()\n",
    "\n",
    "print(f\"Train split: {len(train_labels_df)} labeled articles\")\n",
    "print(f\"Test split:  {len(test_ids_df)} labeled articles\")\n",
    "print(f\"Total:       {len(train_labels_df) + len(test_ids_df)}\")\n",
    "\n",
    "print(\"\\nTrain split distribution:\")\n",
    "print(train_labels_df[\"label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Join with raw article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw articles loaded: 260797\n",
      "\n",
      "Final train:        1304 articles\n",
      "Final test:          617 articles\n",
      "Final raw:        258876 articles (unlabeled)\n",
      "Total in dataset: 260797\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv(RAW_CSV, encoding=\"utf-8\")\n",
    "print(f\"Raw articles loaded: {len(raw_df)}\")\n",
    "\n",
    "# Join test split with raw data (keep all columns)\n",
    "test_df = test_ids_df.merge(raw_df, on=\"id\", how=\"left\")\n",
    "missing_test = test_df[\"text\"].isna().sum()\n",
    "if missing_test > 0:\n",
    "    print(f\"WARNING: {missing_test} test articles not found in raw data!\")\n",
    "    test_df = test_df.dropna(subset=[\"text\"])\n",
    "\n",
    "# Join train split with raw data (keep all columns)\n",
    "train_df = train_labels_df.merge(raw_df, on=\"id\", how=\"left\")\n",
    "missing_train = train_df[\"text\"].isna().sum()\n",
    "if missing_train > 0:\n",
    "    print(f\"WARNING: {missing_train} train articles not found in raw data!\")\n",
    "    train_df = train_df.dropna(subset=[\"text\"])\n",
    "\n",
    "# Build raw split: all articles that have NO label yet\n",
    "all_labeled_ids = set(labeled_df[\"id\"].tolist())\n",
    "unlabeled_df = raw_df[~raw_df[\"id\"].isin(all_labeled_ids)].copy()\n",
    "unlabeled_df[\"label\"] = \"\"\n",
    "\n",
    "# Ensure consistent column order across all splits\n",
    "columns = [\"id\", \"domain\", \"url\", \"date_time\", \"headline\", \"author\", \"text\", \"text_length\", \"label\"]\n",
    "train_df = train_df[columns]\n",
    "test_df = test_df[columns]\n",
    "unlabeled_df = unlabeled_df[columns]\n",
    "\n",
    "print(f\"\\nFinal train:     {len(train_df):>7} articles\")\n",
    "print(f\"Final test:      {len(test_df):>7} articles\")\n",
    "print(f\"Final raw:       {len(unlabeled_df):>7} articles (unlabeled)\")\n",
    "print(f\"Total in dataset:{len(train_df) + len(test_df) + len(unlabeled_df):>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create HuggingFace Dataset & push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    raw: Dataset({\n",
      "        features: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label'],\n",
      "        num_rows: 258876\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label'],\n",
      "        num_rows: 1304\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label'],\n",
      "        num_rows: 617\n",
      "    })\n",
      "})\n",
      "\n",
      "raw columns: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label']\n",
      "\n",
      "train columns: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label']\n",
      "\n",
      "test columns: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label']\n"
     ]
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    \"raw\": Dataset.from_pandas(unlabeled_df, preserve_index=False),\n",
    "    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df, preserve_index=False),\n",
    "})\n",
    "\n",
    "print(ds)\n",
    "for split in ds:\n",
    "    print(f\"\\n{split} columns: {ds[split].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 4 old data files: ['data/raw-00000-of-00002.parquet', 'data/raw-00001-of-00002.parquet', 'data/test-00000-of-00001.parquet', 'data/train-00000-of-00001.parquet']\n",
      "Pushing to: https://huggingface.co/datasets/Zorryy/news_articles_2025_elections_germany\n",
      "Commit: Update 2026-02-20: 1304 train / 617 test / 258876 raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  4.17ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  196MB /  196MB, 1.63MB/s  \n",
      "New Data Upload: 100%|██████████|  196MB /  196MB, 1.63MB/s  \n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:01<00:00,  3.87ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  206MB /  206MB, 1.44MB/s  \n",
      "New Data Upload: 100%|██████████|  206MB /  206MB, 1.44MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [02:45<00:00, 82.81s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 35.10ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 3.74MB / 3.74MB, 1.46MB/s  \n",
      "New Data Upload: 100%|██████████| 3.74MB / 3.74MB, 1.46MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.85s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 61.72ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 1.38MB / 1.38MB,  708kB/s  \n",
      "New Data Upload: 100%|██████████| 1.38MB / 1.38MB,  708kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.75s/ shards]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Dataset available at:\n",
      "https://huggingface.co/datasets/Zorryy/news_articles_2025_elections_germany\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "commit_msg = (\n",
    "    f\"Update {today}: \"\n",
    "    f\"{len(train_df)} train / {len(test_df)} test / {len(unlabeled_df)} raw\"\n",
    ")\n",
    "\n",
    "# Delete old data files to force clean upload\n",
    "api = HfApi()\n",
    "existing_files = api.list_repo_files(HF_REPO, repo_type=\"dataset\")\n",
    "data_files = [f for f in existing_files if f.startswith(\"data/\")]\n",
    "if data_files:\n",
    "    from huggingface_hub import CommitOperationDelete\n",
    "    operations = [CommitOperationDelete(path_in_repo=f) for f in data_files]\n",
    "    api.create_commit(HF_REPO, operations=operations, repo_type=\"dataset\",\n",
    "                      commit_message=\"Clear old data before re-upload\")\n",
    "    print(f\"Deleted {len(data_files)} old data files: {data_files}\")\n",
    "\n",
    "print(f\"Pushing to: https://huggingface.co/datasets/{HF_REPO}\")\n",
    "print(f\"Commit: {commit_msg}\")\n",
    "\n",
    "ds.push_to_hub(HF_REPO, commit_message=commit_msg)\n",
    "\n",
    "print(f\"\\nDone! Dataset available at:\")\n",
    "print(f\"https://huggingface.co/datasets/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Label  Train  Test  Total\n",
      "              AfD/Rechte    150    50    200\n",
      "                  Andere    150    50    200\n",
      "        Arbeitslosigkeit     57    50    107\n",
      " Bundeswehr/Verteidigung    126    50    176\n",
      "Gesundheitswesen, Pflege     91    50    141\n",
      "         Klima / Energie    150    50    200\n",
      "     Kosten/Löhne/Preise      9    50     59\n",
      "         Politikverdruss      1    17     18\n",
      "                  Renten     95    50    145\n",
      "        Soziales Gefälle     20    50     70\n",
      "  Ukraine/Krieg/Russland    154    50    204\n",
      "         Wirtschaftslage    150    50    200\n",
      "             Zuwanderung    151    50    201\n",
      "                   TOTAL   1304   617   1921\n",
      "\n",
      "Raw (unlabeled): 258876 articles\n"
     ]
    }
   ],
   "source": [
    "# Final summary table\n",
    "all_labels = sorted(set(train_df[\"label\"].unique()) | set(test_df[\"label\"].unique()))\n",
    "summary = pd.DataFrame({\n",
    "    \"Label\": all_labels,\n",
    "    \"Train\": [len(train_df[train_df[\"label\"] == l]) for l in all_labels],\n",
    "    \"Test\": [len(test_df[test_df[\"label\"] == l]) for l in all_labels],\n",
    "})\n",
    "summary[\"Total\"] = summary[\"Train\"] + summary[\"Test\"]\n",
    "summary.loc[len(summary)] = [\"TOTAL\", summary[\"Train\"].sum(), summary[\"Test\"].sum(), summary[\"Total\"].sum()]\n",
    "print(summary.to_string(index=False))\n",
    "print(f\"\\nRaw (unlabeled): {len(unlabeled_df)} articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (labeling .venv)",
   "language": "python",
   "name": "labeling-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
