{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Pipeline: Test/Train Split & HuggingFace Upload\n",
    "\n",
    "Run this notebook after each annotation session to:\n",
    "1. Load and clean labeled data\n",
    "2. Create a fixed test split (once, never modified)\n",
    "3. Build the train split from remaining annotations\n",
    "4. Push both to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zorbeyozcan/news_articles_classification_thesis/Python/labeling_app/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw articles: ../../data/articles/cleaned_articles.csv\n",
      "Labeled articles: ../../data/articles/cleaned_articles_labeled.csv\n",
      "Test split: ../../data/articles/test_split.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# === CONFIG ===\n",
    "HF_REPO = \"Zorryy/news_articles_2025_elections_germany\"\n",
    "TEST_SAMPLES_PER_CLASS = 50\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# === PATHS ===\n",
    "DATA_DIR = os.path.join(\"..\", \"..\", \"data\", \"articles\")\n",
    "RAW_CSV = os.path.join(DATA_DIR, \"cleaned_articles.csv\")\n",
    "LABELED_CSV = os.path.join(DATA_DIR, \"cleaned_articles_labeled.csv\")\n",
    "TEST_SPLIT_CSV = os.path.join(DATA_DIR, \"test_split.csv\")\n",
    "\n",
    "# Verify files exist\n",
    "assert os.path.exists(RAW_CSV), f\"Raw CSV not found: {RAW_CSV}\"\n",
    "assert os.path.exists(LABELED_CSV), f\"Labeled CSV not found: {LABELED_CSV}\"\n",
    "print(f\"Raw articles: {RAW_CSV}\")\n",
    "print(f\"Labeled articles: {LABELED_CSV}\")\n",
    "print(f\"Test split: {TEST_SPLIT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load labeled data & remove non-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in labeled CSV: 1386\n",
      "After removing skipped/not_clean: 782\n",
      "After deduplication: 782\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "Andere                      193\n",
      "Gesundheitswesen, Pflege     62\n",
      "Klima / Energie              60\n",
      "Wirtschaftslage              50\n",
      "Ukraine/Krieg/Russland       50\n",
      "Soziales Gefälle             50\n",
      "Kosten/Löhne/Preise          50\n",
      "Zuwanderung                  50\n",
      "Renten                       50\n",
      "Arbeitslosigkeit             50\n",
      "Bundeswehr/Verteidigung      50\n",
      "AfD/Rechte                   50\n",
      "Politikverdruss              17\n"
     ]
    }
   ],
   "source": [
    "# Increase CSV field size limit for large text fields\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "labeled_df = pd.read_csv(LABELED_CSV, encoding=\"utf-8\")\n",
    "print(f\"Total rows in labeled CSV: {len(labeled_df)}\")\n",
    "\n",
    "# Remove non-labels (skipped, not_clean)\n",
    "non_labels = [\"skipped\", \"not_clean\"]\n",
    "labeled_df = labeled_df[~labeled_df[\"label\"].isin(non_labels)].copy()\n",
    "print(f\"After removing skipped/not_clean: {len(labeled_df)}\")\n",
    "\n",
    "# Deduplicate: keep last label per article ID\n",
    "labeled_df = labeled_df.drop_duplicates(subset=\"id\", keep=\"last\")\n",
    "print(f\"After deduplication: {len(labeled_df)}\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(labeled_df[\"label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create or load test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split already exists. Loaded 617 entries.\n",
      "(Test split is frozen and will not be modified.)\n",
      "\n",
      "Test split distribution:\n",
      "label\n",
      "AfD/Rechte                  50\n",
      "Andere                      50\n",
      "Arbeitslosigkeit            50\n",
      "Bundeswehr/Verteidigung     50\n",
      "Gesundheitswesen, Pflege    50\n",
      "Klima / Energie             50\n",
      "Kosten/Löhne/Preise         50\n",
      "Renten                      50\n",
      "Soziales Gefälle            50\n",
      "Ukraine/Krieg/Russland      50\n",
      "Wirtschaftslage             50\n",
      "Zuwanderung                 50\n",
      "Politikverdruss             17\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TEST_SPLIT_CSV):\n",
    "    # Load existing test split - NEVER modify it\n",
    "    test_ids_df = pd.read_csv(TEST_SPLIT_CSV, encoding=\"utf-8\")\n",
    "    print(f\"Test split already exists. Loaded {len(test_ids_df)} entries.\")\n",
    "    print(\"(Test split is frozen and will not be modified.)\")\n",
    "else:\n",
    "    # Create new test split: up to TEST_SAMPLES_PER_CLASS per class\n",
    "    print(f\"Creating new test split with up to {TEST_SAMPLES_PER_CLASS} samples per class...\")\n",
    "    test_samples = []\n",
    "    for label in sorted(labeled_df[\"label\"].unique()):\n",
    "        class_df = labeled_df[labeled_df[\"label\"] == label]\n",
    "        n = min(len(class_df), TEST_SAMPLES_PER_CLASS)\n",
    "        sampled = class_df.sample(n=n, random_state=RANDOM_SEED)\n",
    "        test_samples.append(sampled)\n",
    "        print(f\"  {label}: {n} samples (available: {len(class_df)})\")\n",
    "\n",
    "    test_ids_df = pd.concat(test_samples)[[\"id\", \"label\"]]\n",
    "    test_ids_df.to_csv(TEST_SPLIT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nTest split saved: {len(test_ids_df)} total entries\")\n",
    "\n",
    "print(\"\\nTest split distribution:\")\n",
    "print(test_ids_df[\"label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build train split (all labeled data NOT in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 165 labeled articles\n",
      "Test split:  617 labeled articles\n",
      "Total:       782\n",
      "\n",
      "Train split distribution:\n",
      "label\n",
      "Andere                      143\n",
      "Gesundheitswesen, Pflege     12\n",
      "Klima / Energie              10\n"
     ]
    }
   ],
   "source": [
    "test_ids = set(test_ids_df[\"id\"].tolist())\n",
    "train_labels_df = labeled_df[~labeled_df[\"id\"].isin(test_ids)][[\"id\", \"label\"]].copy()\n",
    "\n",
    "print(f\"Train split: {len(train_labels_df)} labeled articles\")\n",
    "print(f\"Test split:  {len(test_ids_df)} labeled articles\")\n",
    "print(f\"Total:       {len(train_labels_df) + len(test_ids_df)}\")\n",
    "\n",
    "print(\"\\nTrain split distribution:\")\n",
    "print(train_labels_df[\"label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Join with raw article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "raw_df = pd.read_csv(RAW_CSV, encoding=\"utf-8\")\nprint(f\"Raw articles loaded: {len(raw_df)}\")\n\n# Join test split with raw data (keep all columns)\ntest_df = test_ids_df.merge(raw_df, on=\"id\", how=\"left\")\nmissing_test = test_df[\"text\"].isna().sum()\nif missing_test > 0:\n    print(f\"WARNING: {missing_test} test articles not found in raw data!\")\n    test_df = test_df.dropna(subset=[\"text\"])\n\n# Join train split with raw data (keep all columns)\ntrain_df = train_labels_df.merge(raw_df, on=\"id\", how=\"left\")\nmissing_train = train_df[\"text\"].isna().sum()\nif missing_train > 0:\n    print(f\"WARNING: {missing_train} train articles not found in raw data!\")\n    train_df = train_df.dropna(subset=[\"text\"])\n\n# Build raw split: all articles that have NO label yet\nall_labeled_ids = set(labeled_df[\"id\"].tolist())\nunlabeled_df = raw_df[~raw_df[\"id\"].isin(all_labeled_ids)].copy()\nunlabeled_df[\"label\"] = \"\"\n\n# Ensure consistent column order across all splits\ncolumns = [\"id\", \"domain\", \"url\", \"date_time\", \"headline\", \"author\", \"text\", \"text_length\", \"label\"]\ntrain_df = train_df[columns]\ntest_df = test_df[columns]\nunlabeled_df = unlabeled_df[columns]\n\nprint(f\"\\nFinal train:     {len(train_df):>7} articles\")\nprint(f\"Final test:      {len(test_df):>7} articles\")\nprint(f\"Final raw:       {len(unlabeled_df):>7} articles (unlabeled)\")\nprint(f\"Total in dataset:{len(train_df) + len(test_df) + len(unlabeled_df):>7}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create HuggingFace Dataset & push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    raw: Dataset({\n",
      "        features: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label'],\n",
      "        num_rows: 260015\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label'],\n",
      "        num_rows: 165\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label'],\n",
      "        num_rows: 617\n",
      "    })\n",
      "})\n",
      "\n",
      "raw columns: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label']\n",
      "\n",
      "train columns: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label']\n",
      "\n",
      "test columns: ['id', 'domain', 'url', 'date_time', 'headline', 'author', 'text', 'text_length', 'label']\n"
     ]
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    \"raw\": Dataset.from_pandas(unlabeled_df, preserve_index=False),\n",
    "    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df, preserve_index=False),\n",
    "})\n",
    "\n",
    "print(ds)\n",
    "for split in ds:\n",
    "    print(f\"\\n{split} columns: {ds[split].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing to: https://huggingface.co/datasets/Zorryy/news_articles_2025_elections_germany\n",
      "Commit: Update 2026-02-14: 165 train / 617 test / 260015 raw\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All datasets in `DatasetDict` should have the same features but features for 'raw' and 'train' don't match: {'id': Value('int64'), 'domain': Value('string'), 'url': Value('string'), 'date_time': Value('string'), 'headline': Value('string'), 'author': Value('string'), 'text': Value('string'), 'text_length': Value('int64'), 'label': Value('null')} != {'id': Value('int64'), 'domain': Value('string'), 'url': Value('string'), 'date_time': Value('string'), 'headline': Value('string'), 'author': Value('string'), 'text': Value('string'), 'text_length': Value('int64'), 'label': Value('string')}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPushing to: https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHF_REPO\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommit_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHF_REPO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone! Dataset available at:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHF_REPO\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/news_articles_classification_thesis/Python/labeling_app/venv/lib/python3.9/site-packages/datasets/dataset_dict.py:1729\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, config_name, set_default, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files, num_proc)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide one `num_shards` per dataset in the dataset dictionary, e.g. \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 128, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 4}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1726\u001b[0m     )\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_values_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m total_uploaded_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1731\u001b[0m total_dataset_nbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/news_articles_classification_thesis/Python/labeling_app/venv/lib/python3.9/site-packages/datasets/dataset_dict.py:69\u001b[0m, in \u001b[0;36mDatasetDict._check_values_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_a, item_b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], items[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item_a[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m!=\u001b[39m item_b[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll datasets in `DatasetDict` should have the same features but features for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_a[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_b[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_a[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_b[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: All datasets in `DatasetDict` should have the same features but features for 'raw' and 'train' don't match: {'id': Value('int64'), 'domain': Value('string'), 'url': Value('string'), 'date_time': Value('string'), 'headline': Value('string'), 'author': Value('string'), 'text': Value('string'), 'text_length': Value('int64'), 'label': Value('null')} != {'id': Value('int64'), 'domain': Value('string'), 'url': Value('string'), 'date_time': Value('string'), 'headline': Value('string'), 'author': Value('string'), 'text': Value('string'), 'text_length': Value('int64'), 'label': Value('string')}"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "commit_msg = (\n",
    "    f\"Update {today}: \"\n",
    "    f\"{len(train_df)} train / {len(test_df)} test / {len(unlabeled_df)} raw\"\n",
    ")\n",
    "\n",
    "# Delete old data files to force clean upload\n",
    "api = HfApi()\n",
    "existing_files = api.list_repo_files(HF_REPO, repo_type=\"dataset\")\n",
    "data_files = [f for f in existing_files if f.startswith(\"data/\")]\n",
    "if data_files:\n",
    "    from huggingface_hub import CommitOperationDelete\n",
    "    operations = [CommitOperationDelete(path_in_repo=f) for f in data_files]\n",
    "    api.create_commit(HF_REPO, operations=operations, repo_type=\"dataset\",\n",
    "                      commit_message=\"Clear old data before re-upload\")\n",
    "    print(f\"Deleted {len(data_files)} old data files: {data_files}\")\n",
    "\n",
    "print(f\"Pushing to: https://huggingface.co/datasets/{HF_REPO}\")\n",
    "print(f\"Commit: {commit_msg}\")\n",
    "\n",
    "ds.push_to_hub(HF_REPO, commit_message=commit_msg)\n",
    "\n",
    "print(f\"\\nDone! Dataset available at:\")\n",
    "print(f\"https://huggingface.co/datasets/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "all_labels = sorted(set(train_df[\"label\"].unique()) | set(test_df[\"label\"].unique()))\n",
    "summary = pd.DataFrame({\n",
    "    \"Label\": all_labels,\n",
    "    \"Train\": [len(train_df[train_df[\"label\"] == l]) for l in all_labels],\n",
    "    \"Test\": [len(test_df[test_df[\"label\"] == l]) for l in all_labels],\n",
    "})\n",
    "summary[\"Total\"] = summary[\"Train\"] + summary[\"Test\"]\n",
    "summary.loc[len(summary)] = [\"TOTAL\", summary[\"Train\"].sum(), summary[\"Test\"].sum(), summary[\"Total\"].sum()]\n",
    "print(summary.to_string(index=False))\n",
    "print(f\"\\nRaw (unlabeled): {len(unlabeled_df)} articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (labeling venv)",
   "language": "python",
   "name": "labeling-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}