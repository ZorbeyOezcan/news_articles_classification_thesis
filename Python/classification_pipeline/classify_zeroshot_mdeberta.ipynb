{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification: mDeBERTa\n",
    "## Model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n",
    "\n",
    "Zero-Shot NLI-basierte Klassifikation auf Volltext.\n",
    "\n",
    "**Voraussetzung:** GPU-Runtime aktiviert, `HF_TOKEN` in Colab Secrets hinterlegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP (in jedem Notebook identisch) ===\n",
    "import os, sys\n",
    "\n",
    "# Repo klonen / aktualisieren\n",
    "REPO = \"/content/news_articles_classification_thesis\"\n",
    "if not os.path.exists(REPO):\n",
    "    !git clone https://github.com/ZorbeyOezcan/news_articles_classification_thesis.git {REPO}\n",
    "else:\n",
    "    !cd {REPO} && git pull -q\n",
    "\n",
    "# Dependencies\n",
    "!pip install -q transformers datasets huggingface_hub scikit-learn matplotlib seaborn tqdm pandas\n",
    "\n",
    "# Google Drive mounten (persistente Reports)\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "# pipeline_utils importierbar machen\n",
    "PIPELINE_DIR = f\"{REPO}/Python/classification_pipeline\"\n",
    "if PIPELINE_DIR not in sys.path:\n",
    "    sys.path.insert(0, PIPELINE_DIR)\n",
    "\n",
    "import importlib\n",
    "import pipeline_utils as pu\n",
    "importlib.reload(pu)\n",
    "\n",
    "# HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "login(token=userdata.get(\"HF_TOKEN\"))\n",
    "\n",
    "print(f\"Reports-Ordner: {pu.REPORTS_DIR}\")\n",
    "print(\"Setup abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL CONFIG =====\n",
    "MODEL_ID = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "MODEL_SHORT_NAME = \"mdeberta\"\n",
    "MODEL_TYPE = \"zero-shot\"  # \"zero-shot\" | \"few-shot\" | \"fine-tuned\"\n",
    "\n",
    "# Welcher Split wird evaluiert?\n",
    "EVALUATE_ON = \"test\"  # \"test\" oder \"eval\"\n",
    "\n",
    "# Batch-Größe für Klassifikation (kleiner = weniger VRAM, langsamer)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# ===== NLI CONFIG =====\n",
    "# Hypothesis Template für Zero-Shot NLI\n",
    "HYPOTHESIS_TEMPLATE = \"Dieser Text handelt von {}.\"\n",
    "\n",
    "# Candidate Labels: None = Labels aus dem Datensatz verwenden\n",
    "# Oder eigene Liste angeben: [\"Label A\", \"Label B\", ...]\n",
    "CANDIDATE_LABELS = None\n",
    "\n",
    "# ===== MODEL INFO (für Report) =====\n",
    "MODEL_INFO = {\n",
    "    \"huggingface_id\": MODEL_ID,\n",
    "    \"language\": \"Multilingual (100+ Sprachen, inkl. Deutsch)\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"parameters\": \"278M\",\n",
    "    \"notes\": \"mDeBERTa unterstützt KEIN FP16 (NaN). Immer FP32 verwenden.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "data = pu.load_data(\n",
    "    split_mode=\"percentage\",\n",
    "    eval_fraction=0.2,\n",
    "    random_seed=42,\n",
    "    load_raw=False,\n",
    ")\n",
    "\n",
    "eval_df = data[EVALUATE_ON]\n",
    "\n",
    "# Labels bestimmen\n",
    "if CANDIDATE_LABELS is None:\n",
    "    CANDIDATE_LABELS = list(data[\"label_mapping\"].values())\n",
    "\n",
    "print(f\"\\nEvaluiere auf '{EVALUATE_ON}' Split: {len(eval_df)} Artikel\")\n",
    "print(f\"Labels: {len(CANDIDATE_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell laden\n",
    "import torch\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "classifier = hf_pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=MODEL_ID,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Modell geladen: {MODEL_ID}\")\n",
    "print(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "print(f\"Tokenizer max length: {classifier.tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikation (Volltext)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def classify_batch(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Zero-Shot Klassifikation mit Progress-Tracking.\"\"\"\n",
    "    predictions = [None] * len(texts)\n",
    "    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]\n",
    "    non_empty_texts = [texts[i] for i in non_empty_indices]\n",
    "\n",
    "    for start in tqdm(range(0, len(non_empty_texts), batch_size), desc=\"Classifying\"):\n",
    "        batch_texts = non_empty_texts[start:start + batch_size]\n",
    "        batch_indices = non_empty_indices[start:start + batch_size]\n",
    "\n",
    "        results = classifier(\n",
    "            batch_texts,\n",
    "            candidate_labels=CANDIDATE_LABELS,\n",
    "            hypothesis_template=HYPOTHESIS_TEMPLATE,\n",
    "            multi_label=False,\n",
    "        )\n",
    "        if isinstance(results, dict):\n",
    "            results = [results]\n",
    "\n",
    "        for idx, r in zip(batch_indices, results):\n",
    "            predictions[idx] = r[\"labels\"][0]\n",
    "\n",
    "    empty_count = sum(1 for p in predictions if p is None)\n",
    "    if empty_count > 0:\n",
    "        print(f\"  {empty_count} leere Texte -> 'Andere'\")\n",
    "    return [p if p is not None else \"Andere\" for p in predictions]\n",
    "\n",
    "\n",
    "texts = eval_df[\"text\"].fillna(\"\").tolist()\n",
    "true_labels = eval_df[\"label\"].tolist()\n",
    "\n",
    "timer = pu.ExperimentTimer()\n",
    "with timer:\n",
    "    predictions = classify_batch(texts)\n",
    "\n",
    "print(f\"\\nKlassifikation abgeschlossen: {timer.duration_formatted}\")\n",
    "print(f\"Durchsatz: {timer.articles_per_second(len(texts)):.2f} Artikel/Sekunde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "metrics = pu.evaluate(\n",
    "    true_labels,\n",
    "    predictions,\n",
    "    labels=CANDIDATE_LABELS,\n",
    "    experiment_name=EVALUATE_ON,\n",
    ")\n",
    "\n",
    "pu.print_metrics(metrics, f\"Zero-Shot mDeBERTa — {EVALUATE_ON} Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "pu.plot_confusion_matrix(\n",
    "    metrics,\n",
    "    title=f\"Zero-Shot mDeBERTa ({EVALUATE_ON})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report generieren\n",
    "report_path = pu.generate_report(\n",
    "    model_name=f\"{MODEL_SHORT_NAME}_zeroshot\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    metrics=metrics,\n",
    "    timer=timer,\n",
    "    model_info=MODEL_INFO,\n",
    "    candidate_labels=CANDIDATE_LABELS,\n",
    "    hypothesis_template=HYPOTHESIS_TEMPLATE,\n",
    "    split_config=data[\"split_config\"],\n",
    "    label_mapping=data[\"label_mapping\"],\n",
    "    experiment_notes=(\n",
    "        \"Zero-Shot NLI-Klassifikation auf Volltext. \"\n",
    "        \"Texte werden automatisch auf 512 Tokens gekürzt (inverted pyramid).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"\\nReport gespeichert: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Model:           {MODEL_ID}\")\n",
    "print(f\"  Type:            {MODEL_TYPE}\")\n",
    "print(f\"  Split:           {EVALUATE_ON} ({len(eval_df)} Artikel)\")\n",
    "print(f\"  F1 Macro:        {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Weighted:     {metrics['f1_weighted']:.4f}\")\n",
    "print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Dauer:           {timer.duration_formatted}\")\n",
    "print(f\"  Artikel/Sek:     {timer.articles_per_second(len(eval_df)):.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}