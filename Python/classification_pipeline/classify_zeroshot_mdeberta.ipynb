{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification: mDeBERTa\n",
    "## Model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n",
    "\n",
    "Zero-Shot NLI-basierte Klassifikation auf Volltext.\n",
    "\n",
    "**Voraussetzung:** `init_data.ipynb` muss vorher ausgeführt worden sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL CONFIG =====\n",
    "MODEL_ID = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "MODEL_SHORT_NAME = \"mdeberta\"\n",
    "MODEL_TYPE = \"zero-shot\"  # \"zero-shot\" | \"few-shot\" | \"fine-tuned\"\n",
    "\n",
    "# Welcher Split wird evaluiert?\n",
    "EVALUATE_ON = \"test\"  # \"test\" oder \"eval\"\n",
    "\n",
    "# Batch-Größe für Klassifikation (kleiner = weniger VRAM, langsamer)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# ===== NLI CONFIG =====\n",
    "# Hypothesis Template für Zero-Shot NLI\n",
    "HYPOTHESIS_TEMPLATE = \"Dieser Text handelt von {}.\"\n",
    "\n",
    "# Candidate Labels: None = Labels aus init_data.ipynb verwenden\n",
    "# Oder eigene Liste angeben: [\"Label A\", \"Label B\", ...]\n",
    "CANDIDATE_LABELS = None\n",
    "\n",
    "# ===== MODEL INFO (für Report) =====\n",
    "MODEL_INFO = {\n",
    "    \"huggingface_id\": MODEL_ID,\n",
    "    \"language\": \"Multilingual (100+ Sprachen, inkl. Deutsch)\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"parameters\": \"278M\",\n",
    "    \"notes\": \"mDeBERTa unterstützt KEIN FP16 (NaN). Immer FP32 verwenden.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + Daten laden\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "import pipeline_utils as pu\n",
    "importlib.reload(pu)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Daten aus Runtime laden\n",
    "data = pu.get_runtime_data()\n",
    "eval_df = data[EVALUATE_ON]\n",
    "\n",
    "# Labels bestimmen\n",
    "if CANDIDATE_LABELS is None:\n",
    "    CANDIDATE_LABELS = list(data[\"label_mapping\"].values())\n",
    "\n",
    "print(f\"Evaluiere auf '{EVALUATE_ON}' Split: {len(eval_df)} Artikel\")\n",
    "print(f\"Labels: {len(CANDIDATE_LABELS)}\")\n",
    "print(f\"GPU: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell laden\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "classifier = hf_pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=MODEL_ID,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Modell geladen: {MODEL_ID}\")\n",
    "print(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "print(f\"Tokenizer max length: {classifier.tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikation (Volltext)\n",
    "def classify_batch(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Zero-Shot Klassifikation mit Progress-Tracking.\"\"\"\n",
    "    predictions = [None] * len(texts)\n",
    "    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]\n",
    "    non_empty_texts = [texts[i] for i in non_empty_indices]\n",
    "\n",
    "    for start in tqdm(range(0, len(non_empty_texts), batch_size), desc=\"Classifying\"):\n",
    "        batch_texts = non_empty_texts[start:start + batch_size]\n",
    "        batch_indices = non_empty_indices[start:start + batch_size]\n",
    "\n",
    "        results = classifier(\n",
    "            batch_texts,\n",
    "            candidate_labels=CANDIDATE_LABELS,\n",
    "            hypothesis_template=HYPOTHESIS_TEMPLATE,\n",
    "            multi_label=False,\n",
    "        )\n",
    "        if isinstance(results, dict):\n",
    "            results = [results]\n",
    "\n",
    "        for idx, r in zip(batch_indices, results):\n",
    "            predictions[idx] = r[\"labels\"][0]\n",
    "\n",
    "    empty_count = sum(1 for p in predictions if p is None)\n",
    "    if empty_count > 0:\n",
    "        print(f\"  {empty_count} leere Texte -> 'Andere'\")\n",
    "    return [p if p is not None else \"Andere\" for p in predictions]\n",
    "\n",
    "\n",
    "texts = eval_df[\"text\"].fillna(\"\").tolist()\n",
    "true_labels = eval_df[\"label\"].tolist()\n",
    "\n",
    "timer = pu.ExperimentTimer()\n",
    "with timer:\n",
    "    predictions = classify_batch(texts)\n",
    "\n",
    "print(f\"\\nKlassifikation abgeschlossen: {timer.duration_formatted}\")\n",
    "print(f\"Durchsatz: {timer.articles_per_second(len(texts)):.2f} Artikel/Sekunde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "metrics = pu.evaluate(\n",
    "    true_labels,\n",
    "    predictions,\n",
    "    labels=CANDIDATE_LABELS,\n",
    "    experiment_name=EVALUATE_ON,\n",
    ")\n",
    "\n",
    "pu.print_metrics(metrics, f\"Zero-Shot mDeBERTa — {EVALUATE_ON} Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "pu.plot_confusion_matrix(\n",
    "    metrics,\n",
    "    title=f\"Zero-Shot mDeBERTa ({EVALUATE_ON})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report generieren\n",
    "report_path = pu.generate_report(\n",
    "    model_name=f\"{MODEL_SHORT_NAME}_zeroshot\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    metrics=metrics,\n",
    "    timer=timer,\n",
    "    model_info=MODEL_INFO,\n",
    "    candidate_labels=CANDIDATE_LABELS,\n",
    "    hypothesis_template=HYPOTHESIS_TEMPLATE,\n",
    "    experiment_notes=(\n",
    "        \"Zero-Shot NLI-Klassifikation auf Volltext. \"\n",
    "        \"Texte werden automatisch auf 512 Tokens gekürzt (inverted pyramid).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"\\nReport gespeichert: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Model:           {MODEL_ID}\")\n",
    "print(f\"  Type:            {MODEL_TYPE}\")\n",
    "print(f\"  Split:           {EVALUATE_ON} ({len(eval_df)} Artikel)\")\n",
    "print(f\"  F1 Macro:        {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Weighted:     {metrics['f1_weighted']:.4f}\")\n",
    "print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Dauer:           {timer.duration_formatted}\")\n",
    "print(f\"  Artikel/Sek:     {timer.articles_per_second(len(eval_df)):.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
