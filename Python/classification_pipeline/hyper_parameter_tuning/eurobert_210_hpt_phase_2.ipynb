{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Phase 2: EuroBERT-210M\n",
    "## Eingeengter Suchbereich basierend auf Phase 1\n",
    "\n",
    "Verfeinerte Hyperparameter-Suche basierend auf den Top-5-Trials aus Phase 1.\n",
    "\n",
    "**Phase 1 Ergebnisse (Best F1 Macro CV Mean: 0.8393):**\n",
    "- Learning Rate: 3.7e-5 bis 5e-5 (oberes Ende performt besser)\n",
    "- Scheduler: `linear` dominiert (4/5 Top Trials)\n",
    "- Epochs: 13-15 (mehr Training = besser)\n",
    "- Dropout: 0.26-0.40 (mittlerer Bereich)\n",
    "- Batch Size: 4 dominiert (4/5 Top Trials)\n",
    "\n",
    "**Phase 2 Strategie:**\n",
    "- Eingeengte Suchbereiche um Top-5-Region\n",
    "- `linear` Scheduler und Batch Size 4 fixiert\n",
    "- NaN-Detection fuer sofortiges Pruning instabiler Trials\n",
    "- 20 Trials fuer feinere Suche im engen Raum\n",
    "\n",
    "**Voraussetzung:** GPU-Runtime (L4 empfohlen), `HF_TOKEN` in Colab Secrets hinterlegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === SETUP ===\nimport os, sys\n\n# Repo klonen / aktualisieren\nREPO = \"/content/news_articles_classification_thesis\"\nif not os.path.exists(REPO):\n    !git clone https://github.com/ZorbeyOezcan/news_articles_classification_thesis.git {REPO}\nelse:\n    !cd {REPO} && git pull -q\n\n# Dependencies (+ optuna, plotly, kaleido fuer HPT)\n!pip install -q transformers[sentencepiece] datasets huggingface_hub \\\n    scikit-learn matplotlib seaborn tqdm pandas accelerate evaluate \\\n    optuna plotly kaleido\n\n# Google Drive mounten (persistente Reports + Optuna DB)\nfrom google.colab import drive\ndrive.mount(\"/content/drive\", force_remount=False)\n\n# pipeline_utils importierbar machen\nPIPELINE_DIR = f\"{REPO}/Python/classification_pipeline\"\nif PIPELINE_DIR not in sys.path:\n    sys.path.insert(0, PIPELINE_DIR)\n\n# hpt_utils importierbar machen\nHPT_DIR = f\"{REPO}/Python/classification_pipeline/hyper_parameter_tuning\"\nif HPT_DIR not in sys.path:\n    sys.path.insert(0, HPT_DIR)\n\nimport importlib\nimport pipeline_utils as pu\nimportlib.reload(pu)\nimport hpt_utils as hu\nimportlib.reload(hu)\n\n# HuggingFace Login\nfrom huggingface_hub import login\nfrom google.colab import userdata\nlogin(token=userdata.get(\"HF_TOKEN\"))\n\n# Auto-Shutdown Watchdog\nimport threading, time as _time\nMAX_RUNTIME_HOURS = 6  # Phase 2 braucht evtl. etwas laenger (20 Trials)\n\ndef _auto_shutdown():\n    \"\"\"Beendet Runtime nach MAX_RUNTIME_HOURS als Sicherheitsnetz.\"\"\"\n    _time.sleep(MAX_RUNTIME_HOURS * 3600)\n    print(f\"\\n[WATCHDOG] Max Runtime ({MAX_RUNTIME_HOURS}h) erreicht. Runtime wird beendet.\")\n    try:\n        from google.colab import runtime\n        runtime.unassign()\n    except Exception:\n        pass\nthreading.Thread(target=_auto_shutdown, daemon=True).start()\n\nprint(f\"Reports-Ordner: {pu.REPORTS_DIR}\")\nprint(f\"Setup abgeschlossen. Watchdog: Runtime wird nach max {MAX_RUNTIME_HOURS}h beendet.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== HPT PHASE 2 CONFIGURATION =====\nimport torch\nimport numpy as np\n\nMODEL_ID = \"EuroBERT/EuroBERT-210m\"\nMODEL_SHORT_NAME = \"eurobert_210m\"\nMAX_LENGTH = 2048\nRANDOM_SEED = 42\n\n# ----- Cross-Validation -----\nN_FOLDS = 3\n\n# ----- Optuna -----\nN_TRIALS = 20  # Mehr Trials im engeren Suchraum\nSTUDY_NAME = \"eurobert_210m_hpt_phase2\"\nOPTUNA_SEED = 42\nOPTUNA_TIMEOUT = None\n\n# ----- Database Management -----\n# \"new\"      = neue DB erstellen (auto-Nummerierung wenn Name existiert)\n# \"continue\" = vorhandene DB laden, fehlende Trials nachlaufen\nDB_MODE = \"new\"\n\n# ----- Fixed Training Parameters -----\nFIXED_GRADIENT_CHECKPOINTING = False\nFIXED_LOGGING_STEPS = 10\nFIXED_REPORT_TO = \"tensorboard\"\nFIXED_DATALOADER_NUM_WORKERS = 4\nFIXED_EARLY_STOPPING_PATIENCE = 3\nFIXED_GROUP_BY_LENGTH = True\n\n# Mixed Precision: GPU-adaptiv\nif torch.cuda.is_available():\n    _gpu_cap = torch.cuda.get_device_capability()\n    _gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    if _gpu_cap[0] >= 8:\n        FIXED_BF16 = True\n        FIXED_FP16 = False\n    else:\n        FIXED_BF16 = False\n        FIXED_FP16 = True\n    FIXED_OPTIM = \"adamw_torch_fused\"\n    if _gpu_mem >= 40:\n        FIXED_BATCH_SIZE_EVAL = 32\n    elif _gpu_mem >= 20:\n        FIXED_BATCH_SIZE_EVAL = 16\n    else:\n        FIXED_BATCH_SIZE_EVAL = 8\n    print(f\"GPU: {torch.cuda.get_device_name(0)} ({_gpu_mem:.1f} GB, CC {_gpu_cap[0]}.{_gpu_cap[1]})\")\n    print(f\"  FP16={FIXED_FP16}, BF16={FIXED_BF16}, Eval Batch={FIXED_BATCH_SIZE_EVAL}\")\n    print(f\"  Gradient Checkpointing: {FIXED_GRADIENT_CHECKPOINTING}\")\nelse:\n    raise RuntimeError(\"HPT benoetigt eine GPU! Bitte Colab Runtime aendern.\")\n\n# ----- Phase 2: Eingeengte Suchbereiche (basierend auf Phase 1 Top 5) -----\n# Phase 1 Top 5 Ranges:\n#   learning_rate:      3.7e-5 .. 5.0e-5\n#   weight_decay:       0.029  .. 0.071\n#   warmup_ratio:       0.083  .. 0.145\n#   label_smoothing:    0.023  .. 0.067\n#   classifier_dropout: 0.264  .. 0.395\n#   epochs:             13 .. 15\n#   scheduler:          linear (4/5)\n#   batch_size:         4 (4/5)\nHP_RANGES = {\n    \"learning_rate\": (2.5e-5, 5.5e-5),              # leicht erweitert (log scale)\n    \"weight_decay\": (0.02, 0.08),\n    \"warmup_ratio\": (0.06, 0.16),\n    \"label_smoothing_factor\": (0.01, 0.08),\n    \"classifier_dropout\": (0.20, 0.45),\n    \"num_train_epochs\": (12, 16),                    # int range\n}\n\n# Fixiert aus Phase 1 Erkenntnissen:\nFIXED_LR_SCHEDULER_TYPE = \"linear\"       # 4/5 Top Trials\nFIXED_BATCH_SIZE_TRAIN = 4               # 4/5 Top Trials\nEFFECTIVE_BATCH_SIZE = 16\nFIXED_GRAD_ACCUM = EFFECTIVE_BATCH_SIZE // FIXED_BATCH_SIZE_TRAIN  # = 4\n\n# ----- Labels -----\nALL_LABELS = [\n    \"Klima / Energie\", \"Zuwanderung\", \"Renten\", \"Soziales Gef\\u00e4lle\",\n    \"AfD/Rechte\", \"Arbeitslosigkeit\", \"Wirtschaftslage\", \"Politikverdruss\",\n    \"Gesundheitswesen, Pflege\", \"Kosten/L\\u00f6hne/Preise\",\n    \"Ukraine/Krieg/Russland\", \"Bundeswehr/Verteidigung\", \"Andere\",\n]\n\n# Split-Konfiguration\nTEST_PER_CLASS = 30\n\nprint(f\"\\nOptuna HPT Phase 2: {N_TRIALS} Trials, {N_FOLDS}-Fold CV\")\nprint(f\"Modell: {MODEL_ID}\")\nprint(f\"Max Length: {MAX_LENGTH}\")\nprint(f\"Fixiert: scheduler={FIXED_LR_SCHEDULER_TYPE}, batch_size={FIXED_BATCH_SIZE_TRAIN}, grad_accum={FIXED_GRAD_ACCUM}\")\nprint(f\"Effektive Batch Size: {EFFECTIVE_BATCH_SIZE}\")\nprint(f\"DB Mode: {DB_MODE}\")\nprint(f\"Labels: {len(ALL_LABELS)} Klassen\")\nprint(f\"\\nEingeengte Suchbereiche:\")\nfor k, v in HP_RANGES.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATEN LADEN & CUSTOM SPLIT =====\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "ds = load_dataset(pu.DATASET_ID)\n",
    "train_hf = ds[\"train\"].to_pandas()\n",
    "test_hf = ds[\"test\"].to_pandas()\n",
    "all_labelled = pd.concat([train_hf, test_hf], ignore_index=True)\n",
    "\n",
    "print(f\"Gesamtpool gelabelter Artikel: {len(all_labelled)}\")\n",
    "print(f\"Klassen im Datensatz: {all_labelled['label'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# --- Test-Split (identisch wie Phase 1) ---\n",
    "test_indices = []\n",
    "rest_indices = []\n",
    "\n",
    "for label in ALL_LABELS:\n",
    "    label_mask = all_labelled[\"label\"] == label\n",
    "    label_indices = all_labelled[label_mask].index.tolist()\n",
    "    n_total = len(label_indices)\n",
    "\n",
    "    if n_total < 60:\n",
    "        n_test = n_total // 2\n",
    "        print(f\"  {label}: nur {n_total} Artikel -> {n_test} fuer Test (Haelfte)\")\n",
    "    else:\n",
    "        n_test = TEST_PER_CLASS\n",
    "\n",
    "    np.random.shuffle(label_indices)\n",
    "    test_indices.extend(label_indices[:n_test])\n",
    "    rest_indices.extend(label_indices[n_test:])\n",
    "\n",
    "test_df = all_labelled.loc[test_indices].reset_index(drop=True)\n",
    "cv_pool_df = all_labelled.loc[rest_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTest (eingefroren):  {len(test_df)} Artikel\")\n",
    "print(f\"CV-Pool (fuer Folds): {len(cv_pool_df)} Artikel\")\n",
    "\n",
    "# Klassenverteilung\n",
    "print(\"\\nCV-Pool Klassenverteilung:\")\n",
    "cv_dist = cv_pool_df[\"label\"].value_counts().sort_index()\n",
    "for label, count in cv_dist.items():\n",
    "    print(f\"  {label}: {count}\")\n",
    "print(f\"  TOTAL: {len(cv_pool_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LABEL ENCODING =====\n",
    "label2id = {label: idx for idx, label in enumerate(ALL_LABELS)}\n",
    "id2label = {idx: label for idx, label in enumerate(ALL_LABELS)}\n",
    "\n",
    "cv_pool_df[\"label_id\"] = cv_pool_df[\"label\"].map(label2id)\n",
    "test_df[\"label_id\"] = test_df[\"label\"].map(label2id)\n",
    "\n",
    "assert cv_pool_df[\"label_id\"].isna().sum() == 0, \"Unbekannte Labels im CV-Pool!\"\n",
    "assert test_df[\"label_id\"].isna().sum() == 0, \"Unbekannte Labels im Test-Set!\"\n",
    "\n",
    "print(\"Label-Mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {idx:>2}: {label}\")\n",
    "print(f\"\\nAnzahl Klassen: {len(ALL_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TOKENIZER + EUROBERT ROPE FIX =====\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "def _default_rope_init(config, device=None, **kwargs):\n",
    "    base = getattr(config, \"rope_theta\", 10000.0)\n",
    "    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n",
    "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "    dim = int(head_dim * partial_rotary_factor)\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "    return inv_freq, 1.0\n",
    "\n",
    "ROPE_INIT_FUNCTIONS[\"default\"] = _default_rope_init\n",
    "print(\"ROPE_INIT_FUNCTIONS gepatcht.\")\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "print(f\"Tokenizer geladen: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== OPTUNA OBJECTIVE MIT K-FOLD CV =====\nimport gc\nimport shutil\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoConfig,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    DataCollatorWithPadding,\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Extended compute_metrics (per-class scores + confusion matrix support)\ncompute_metrics = hu.make_compute_metrics(ALL_LABELS, id2label)\n\n# Fold-Indizes vorab berechnen (gleiche Folds fuer jeden Trial -> fairer Vergleich)\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\nfold_indices = list(skf.split(cv_pool_df, cv_pool_df[\"label_id\"]))\n\n# Sanity-Check: Fold-Verteilung\nprint(f\"Stratified {N_FOLDS}-Fold CV:\")\nfor i, (train_idx, val_idx) in enumerate(fold_indices):\n    val_labels = cv_pool_df.iloc[val_idx][\"label\"].value_counts()\n    min_val_count = val_labels.min()\n    min_val_class = val_labels.idxmin()\n    print(f\"  Fold {i+1}: Train={len(train_idx)}, Val={len(val_idx)}, \"\n          f\"min Val-Klasse: {min_val_class} ({min_val_count} Samples)\")\n\n\ndef create_model(classifier_dropout):\n    \"\"\"Erstellt ein frisches Modell mit dem angegebenen Dropout.\"\"\"\n    config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n    config.num_labels = len(ALL_LABELS)\n    config.id2label = id2label\n    config.label2id = label2id\n    config.classifier_dropout = classifier_dropout\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_ID,\n        config=config,\n        ignore_mismatched_sizes=True,\n        trust_remote_code=True,\n    )\n    if FIXED_GRADIENT_CHECKPOINTING and torch.cuda.is_available():\n        model.gradient_checkpointing_enable()\n    return model\n\n\ndef _cleanup_cuda():\n    \"\"\"VRAM aufraeumen nach OOM oder normalem Fold-Ende.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n\ndef objective(trial):\n    \"\"\"Optuna objective: mean F1 Macro across k folds.\n\n    Collects per-epoch metrics (per-class scores, confusion matrices) via\n    HPTMetricsCallback. Detects NaN/LR=0 at epoch level for early pruning.\n    \"\"\"\n\n    # --- Hyperparameter samplen (eingeengter Bereich) ---\n    lr = trial.suggest_float(\"learning_rate\", *HP_RANGES[\"learning_rate\"], log=True)\n    wd = trial.suggest_float(\"weight_decay\", *HP_RANGES[\"weight_decay\"])\n    warmup = trial.suggest_float(\"warmup_ratio\", *HP_RANGES[\"warmup_ratio\"])\n    label_smooth = trial.suggest_float(\"label_smoothing_factor\", *HP_RANGES[\"label_smoothing_factor\"])\n    dropout = trial.suggest_float(\"classifier_dropout\", *HP_RANGES[\"classifier_dropout\"])\n    epochs = trial.suggest_int(\"num_train_epochs\", *HP_RANGES[\"num_train_epochs\"])\n\n    # Fixierte Parameter aus Phase 1\n    scheduler = FIXED_LR_SCHEDULER_TYPE\n    batch_size = FIXED_BATCH_SIZE_TRAIN\n    grad_accum = FIXED_GRAD_ACCUM\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Trial {trial.number}: lr={lr:.2e}, wd={wd:.3f}, warmup={warmup:.3f}, \"\n          f\"ls={label_smooth:.3f}, dropout={dropout:.3f}, epochs={epochs}\")\n    print(f\"  (fixiert: sched={scheduler}, batch={batch_size}, grad_accum={grad_accum})\")\n    print(f\"{'='*60}\")\n\n    fold_scores = []\n\n    for fold_idx, (train_idx, val_idx) in enumerate(fold_indices):\n        print(f\"  Fold {fold_idx + 1}/{N_FOLDS}...\", end=\" \", flush=True)\n\n        # VRAM sauber starten\n        _cleanup_cuda()\n\n        # --- Fold Datasets erstellen ---\n        fold_train_df = cv_pool_df.iloc[train_idx]\n        fold_val_df = cv_pool_df.iloc[val_idx]\n\n        train_ds = Dataset.from_pandas(\n            fold_train_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"labels\"})\n        )\n        val_ds = Dataset.from_pandas(\n            fold_val_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"labels\"})\n        )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n        train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n        # --- Output / Logging Verzeichnisse ---\n        fold_output_dir = f\"/content/hpt_tmp/trial_{trial.number}_fold_{fold_idx}\"\n        fold_logging_dir = f\"/content/hpt_tmp/tb_logs/trial_{trial.number:03d}_fold_{fold_idx}\"\n\n        # --- Per-epoch metrics callback ---\n        metrics_callback = hu.HPTMetricsCallback(\n            trial_number=trial.number,\n            fold_idx=fold_idx,\n            all_labels=ALL_LABELS,\n            id2label=id2label,\n        )\n\n        # --- TrainingArguments ---\n        training_args = TrainingArguments(\n            output_dir=fold_output_dir,\n            num_train_epochs=epochs,\n            learning_rate=lr,\n            weight_decay=wd,\n            warmup_ratio=warmup,\n            label_smoothing_factor=label_smooth,\n            lr_scheduler_type=scheduler,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=FIXED_BATCH_SIZE_EVAL,\n            gradient_accumulation_steps=grad_accum,\n            bf16=FIXED_BF16,\n            fp16=FIXED_FP16,\n            gradient_checkpointing=FIXED_GRADIENT_CHECKPOINTING,\n            optim=FIXED_OPTIM,\n            group_by_length=FIXED_GROUP_BY_LENGTH,\n            eval_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            save_total_limit=1,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"f1_macro\",\n            greater_is_better=True,\n            logging_strategy=\"steps\",\n            logging_steps=FIXED_LOGGING_STEPS,\n            logging_dir=fold_logging_dir,\n            report_to=FIXED_REPORT_TO,\n            seed=RANDOM_SEED + fold_idx,\n            dataloader_num_workers=FIXED_DATALOADER_NUM_WORKERS,\n            dataloader_pin_memory=True,\n            disable_tqdm=False,\n        )\n\n        try:\n            # --- Modell + Trainer ---\n            model = create_model(dropout)\n\n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=train_ds,\n                eval_dataset=val_ds,\n                data_collator=data_collator,\n                compute_metrics=compute_metrics,\n                callbacks=[\n                    EarlyStoppingCallback(early_stopping_patience=FIXED_EARLY_STOPPING_PATIENCE),\n                    metrics_callback,\n                ],\n            )\n\n            # --- Training ---\n            trainer.train()\n\n            # --- Check callback for NaN/LR=0 detection ---\n            if metrics_callback.nan_detected or metrics_callback.lr_zero_detected:\n                reason = \"NaN\" if metrics_callback.nan_detected else \"LR=0\"\n                print(f\"{reason} detected — pruning Trial {trial.number}\")\n                # Store partial fold data for analysis\n                hu.store_fold_metrics_partial(trial, fold_idx, metrics_callback, ALL_LABELS)\n                trial.set_user_attr(\"nan_trial\", True)\n                trial.set_user_attr(\"nan_reason\", reason)\n                trial.set_user_attr(\"nan_fold_idx\", fold_idx)\n                raise optuna.TrialPruned()\n\n            # --- Evaluation (bestes Modell dank load_best_model_at_end) ---\n            eval_result = trainer.evaluate()\n            fold_f1 = eval_result[\"eval_f1_macro\"]\n\n            # Safety net: NaN in final eval\n            if np.isnan(eval_result.get(\"eval_loss\", 0)) or fold_f1 < 0.05:\n                print(f\"NaN/collapse in final eval (F1={fold_f1:.4f}) — pruning\")\n                hu.store_fold_metrics(trial, fold_idx, metrics_callback, eval_result, ALL_LABELS, id2label)\n                trial.set_user_attr(\"nan_trial\", True)\n                trial.set_user_attr(\"nan_reason\", \"eval_nan\")\n                trial.set_user_attr(\"nan_fold_idx\", fold_idx)\n                raise optuna.TrialPruned()\n\n            # Store fold metrics (per-class scores, confusion matrix, epoch history)\n            hu.store_fold_metrics(trial, fold_idx, metrics_callback, eval_result, ALL_LABELS, id2label)\n\n            fold_scores.append(fold_f1)\n            print(f\"F1 Macro = {fold_f1:.4f}\")\n\n        except torch.cuda.OutOfMemoryError:\n            print(f\"\\n  OOM in Trial {trial.number}, Fold {fold_idx + 1}! Ueberspringe Trial.\")\n            _cleanup_cuda()\n            if os.path.exists(fold_output_dir):\n                shutil.rmtree(fold_output_dir, ignore_errors=True)\n            raise optuna.TrialPruned()\n\n        finally:\n            # --- Cleanup (immer ausfuehren) ---\n            for var in [\"trainer\", \"model\", \"training_args\", \"train_ds\", \"val_ds\"]:\n                try:\n                    exec(f\"del {var}\")\n                except NameError:\n                    pass\n            _cleanup_cuda()\n            if os.path.exists(fold_output_dir):\n                shutil.rmtree(fold_output_dir, ignore_errors=True)\n\n        # --- Pruning: Zwischenergebnis melden ---\n        trial.report(np.mean(fold_scores), fold_idx)\n        if trial.should_prune():\n            print(f\"  Trial {trial.number} PRUNED nach Fold {fold_idx + 1}\")\n            raise optuna.TrialPruned()\n\n    # Store aggregated trial summary\n    hu.store_trial_summary(trial, N_FOLDS, ALL_LABELS)\n\n    mean_f1 = np.mean(fold_scores)\n    std_f1 = np.std(fold_scores)\n    print(f\"\\n  -> Trial {trial.number}: F1 Macro = {mean_f1:.4f} +/- {std_f1:.4f}\")\n\n    return mean_f1\n\n\nprint(\"Objective Function definiert.\")\nprint(f\"Folds: {N_FOLDS}, Trials: {N_TRIALS}\")\nprint(f\"Effektive Batch Size: {EFFECTIVE_BATCH_SIZE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== OPTUNA STUDY STARTEN =====\nfrom optuna.samplers import TPESampler\nfrom optuna.pruners import MedianPruner\nfrom pathlib import Path\n\n# DB Management: Pfad aufloesen basierend auf DB_MODE\nstorage_url, storage_path = hu.resolve_db_path(\n    db_mode=DB_MODE,\n    study_name=STUDY_NAME,\n    base_dir=Path(pu.REPORTS_DIR),\n)\n\nprint(f\"Optuna DB: {storage_path}\")\nprint(f\"  -> Bei Crash: DB_MODE='continue' setzen, 'Run All' -> Trials werden fortgesetzt\")\n\nsampler = TPESampler(seed=OPTUNA_SEED, n_startup_trials=5)\npruner = MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n\nstudy, n_remaining, n_existing = hu.setup_study(\n    storage_url=storage_url,\n    study_name=STUDY_NAME,\n    db_mode=DB_MODE,\n    n_trials_target=N_TRIALS,\n    sampler=sampler,\n    pruner=pruner,\n)\n\nif n_remaining == 0:\n    print(\"Alle Trials bereits abgeschlossen — ueberspringe optimize().\")\nelse:\n    timer_hpt = pu.ExperimentTimer()\n    with timer_hpt:\n        n_valid = hu.run_hpt_with_nan_exclusion(\n            study=study,\n            objective=objective,\n            n_valid_target=n_remaining,\n            timeout=OPTUNA_TIMEOUT,\n        )\n    print(f\"\\nOptuna HPT Phase 2 abgeschlossen: {timer_hpt.duration_formatted}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Beste Trial: {study.best_trial.number}\")\nprint(f\"Bester F1 Macro (CV Mean): {study.best_value:.4f}\")\nprint(f\"\\nBeste Hyperparameter:\")\nfor key, val in study.best_params.items():\n    print(f\"  {key}: {val}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ERGEBNISSE ANALYSIEREN =====\n",
    "\n",
    "trials_df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\", \"duration\"))\n",
    "trials_df = trials_df.sort_values(\"value\", ascending=False)\n",
    "\n",
    "completed = trials_df[trials_df[\"state\"] == \"COMPLETE\"].copy()\n",
    "pruned = trials_df[trials_df[\"state\"] == \"PRUNED\"].copy()\n",
    "\n",
    "print(f\"Trials: {len(completed)} abgeschlossen, {len(pruned)} gepruned\")\n",
    "print(f\"\\nTop 5 Trials:\")\n",
    "top5_cols = [\"number\", \"value\"] + [c for c in completed.columns if c.startswith(\"params_\")]\n",
    "print(completed[top5_cols].head().to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistiken ueber alle abgeschlossenen Trials:\")\n",
    "print(f\"  Mean F1:   {completed['value'].mean():.4f}\")\n",
    "print(f\"  Std F1:    {completed['value'].std():.4f}\")\n",
    "print(f\"  Min F1:    {completed['value'].min():.4f}\")\n",
    "print(f\"  Max F1:    {completed['value'].max():.4f}\")\n",
    "\n",
    "# Vergleich mit Phase 1\n",
    "print(f\"\\n--- Vergleich mit Phase 1 ---\")\n",
    "print(f\"  Phase 1 Best: 0.8393\")\n",
    "print(f\"  Phase 2 Best: {study.best_value:.4f}\")\n",
    "print(f\"  Differenz:    {study.best_value - 0.8393:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== OPTUNA QUICK SANITY CHECK (minimal — full analysis in analyse_hyperparameter_tuning.ipynb) =====\nimport optuna.visualization as vis\n\n# 1. Optimization History\nfig_history = vis.plot_optimization_history(study)\nfig_history.update_layout(title=\"Phase 2: Optimization History\")\nfig_history.show()\n\n# 2. Parameter Importances (fANOVA)\ntry:\n    fig_importance = vis.plot_param_importances(study)\n    fig_importance.update_layout(title=\"Phase 2: Parameter Importance (fANOVA)\")\n    fig_importance.show()\nexcept Exception as e:\n    print(f\"Parameter Importance nicht verfuegbar: {e}\")\n\nprint(\"Sanity-Check Plots erstellt.\")\nprint(\"Fuer vollstaendige Analyse: analyse_hyperparameter_tuning.ipynb lokal ausfuehren.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== QUICK RESULTS OVERVIEW =====\n# Detaillierte Visualisierungen: analyse_hyperparameter_tuning.ipynb\n\ntrials_df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\", \"duration\"))\ntrials_df = trials_df.sort_values(\"value\", ascending=False)\n\ncompleted = trials_df[trials_df[\"state\"] == \"COMPLETE\"].copy()\npruned = trials_df[trials_df[\"state\"] == \"PRUNED\"].copy()\n\nprint(f\"Trials: {len(completed)} abgeschlossen, {len(pruned)} gepruned\")\nprint(f\"\\nTop 5 Trials:\")\ntop5_cols = [\"number\", \"value\"] + [c for c in completed.columns if c.startswith(\"params_\")]\nprint(completed[top5_cols].head().to_string(index=False))\n\nprint(f\"\\nStatistiken ueber alle abgeschlossenen Trials:\")\nprint(f\"  Mean F1:   {completed['value'].mean():.4f}\")\nprint(f\"  Std F1:    {completed['value'].std():.4f}\")\nprint(f\"  Min F1:    {completed['value'].min():.4f}\")\nprint(f\"  Max F1:    {completed['value'].max():.4f}\")\n\n# Vergleich mit Phase 1\nprint(f\"\\n--- Vergleich mit Phase 1 ---\")\nprint(f\"  Phase 1 Best: 0.8393\")\nprint(f\"  Phase 2 Best: {study.best_value:.4f}\")\nprint(f\"  Differenz:    {study.best_value - 0.8393:+.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== HPT REPORT GENERIEREN =====\nimport json\nfrom datetime import datetime\n\nnow = datetime.now()\nreport_name = f\"{now.strftime('%d%m%y')}_{MODEL_SHORT_NAME}_hpt_phase2\"\n\n# Timer-Dauer (falls optimize() gelaufen ist)\nduration_str = timer_hpt.duration_formatted if 'timer_hpt' in dir() else \"N/A (aus DB geladen)\"\n\n# --- Markdown Report ---\nreport_lines = [\n    f\"# HPT Report: EuroBERT-210M Phase 2\",\n    f\"**Generated:** {now.strftime('%Y-%m-%d %H:%M:%S')}\",\n    \"\",\n    \"---\",\n    \"\",\n    \"## Phase 1 -> Phase 2\",\n    f\"- Phase 1 Best F1: **0.8393** (Trial 11)\",\n    f\"- Phase 2 Best F1: **{study.best_value:.4f}** (Trial {study.best_trial.number})\",\n    f\"- Verbesserung: **{study.best_value - 0.8393:+.4f}**\",\n    \"\",\n    \"## Configuration\",\n    \"| Property | Value |\",\n    \"|---|---|\",\n    f\"| Model | {MODEL_ID} |\",\n    f\"| N Trials | {N_TRIALS} |\",\n    f\"| N Folds | {N_FOLDS} |\",\n    f\"| Completed Trials | {len(completed)} |\",\n    f\"| Pruned Trials | {len(pruned)} |\",\n    f\"| Duration | {duration_str} |\",\n    f\"| GPU | {pu.get_gpu_info()['gpu_name']} |\",\n    f\"| CV Pool Size | {len(cv_pool_df)} |\",\n    f\"| Test Size (frozen) | {len(test_df)} |\",\n    f\"| Effective Batch Size | {EFFECTIVE_BATCH_SIZE} |\",\n    f\"| DB Mode | {DB_MODE} |\",\n    f\"| DB Path | {storage_path} |\",\n    \"\",\n    \"## Fixed Parameters (aus Phase 1)\",\n    \"| Parameter | Value |\",\n    \"|---|---|\",\n    f\"| lr_scheduler_type | {FIXED_LR_SCHEDULER_TYPE} |\",\n    f\"| per_device_train_batch_size | {FIXED_BATCH_SIZE_TRAIN} |\",\n    f\"| gradient_accumulation_steps | {FIXED_GRAD_ACCUM} |\",\n    f\"| bf16 | {FIXED_BF16} |\",\n    f\"| fp16 | {FIXED_FP16} |\",\n    f\"| gradient_checkpointing | {FIXED_GRADIENT_CHECKPOINTING} |\",\n    f\"| group_by_length | {FIXED_GROUP_BY_LENGTH} |\",\n    f\"| optim | {FIXED_OPTIM} |\",\n    f\"| early_stopping_patience | {FIXED_EARLY_STOPPING_PATIENCE} |\",\n    f\"| max_length | {MAX_LENGTH} |\",\n    \"\",\n    \"## Eingeengte Suchbereiche (Phase 2)\",\n    \"| Parameter | Range |\",\n    \"|---|---|\",\n]\nfor param, range_val in HP_RANGES.items():\n    report_lines.append(f\"| {param} | {range_val} |\")\n\nreport_lines += [\n    \"\",\n    f\"**Best F1 Macro (CV Mean): {study.best_value:.4f}**\",\n    f\"**Best Trial: {study.best_trial.number}**\",\n    \"\",\n    \"## Alle Trials\",\n    \"\",\n]\n\ntable_cols = [\"number\", \"value\", \"state\"] + [c for c in trials_df.columns if c.startswith(\"params_\")]\nreport_lines.append(trials_df[table_cols].to_markdown(index=False))\nreport_lines += [\n    \"\",\n    \"---\",\n    \"*Best parameter extraction: see analyse_hyperparameter_tuning.ipynb*\",\n    \"*Generated by eurobert_210_hpt_phase_2.ipynb*\",\n]\n\nreport_md = \"\\n\".join(report_lines)\nreport_path = Path(pu.REPORTS_DIR) / f\"{report_name}.md\"\nreport_path.write_text(report_md, encoding=\"utf-8\")\n\n# --- JSON Sidecar ---\njson_data = {\n    \"report_id\": report_name,\n    \"model_id\": MODEL_ID,\n    \"phase\": 2,\n    \"phase1_best_value\": 0.8393,\n    \"n_trials\": N_TRIALS,\n    \"n_folds\": N_FOLDS,\n    \"best_value\": round(study.best_value, 4),\n    \"best_trial_number\": study.best_trial.number,\n    \"best_params\": {\n        **study.best_params,\n        \"lr_scheduler_type\": FIXED_LR_SCHEDULER_TYPE,\n        \"per_device_train_batch_size\": FIXED_BATCH_SIZE_TRAIN,\n    },\n    \"all_trials\": [\n        {\n            \"number\": t.number,\n            \"value\": round(t.value, 4) if t.value is not None else None,\n            \"params\": t.params,\n            \"state\": str(t.state.name),\n            \"duration_s\": round(t.duration.total_seconds(), 1) if t.duration else None,\n        }\n        for t in study.trials\n    ],\n    \"search_ranges\": {k: str(v) for k, v in HP_RANGES.items()},\n    \"fixed_params\": {\n        \"lr_scheduler_type\": FIXED_LR_SCHEDULER_TYPE,\n        \"per_device_train_batch_size\": FIXED_BATCH_SIZE_TRAIN,\n        \"gradient_accumulation_steps\": FIXED_GRAD_ACCUM,\n        \"bf16\": FIXED_BF16,\n        \"fp16\": FIXED_FP16,\n        \"gradient_checkpointing\": FIXED_GRADIENT_CHECKPOINTING,\n        \"optim\": FIXED_OPTIM,\n        \"early_stopping_patience\": FIXED_EARLY_STOPPING_PATIENCE,\n        \"group_by_length\": FIXED_GROUP_BY_LENGTH,\n        \"max_length\": MAX_LENGTH,\n        \"effective_batch_size\": EFFECTIVE_BATCH_SIZE,\n    },\n    \"duration_formatted\": duration_str,\n    \"gpu\": pu.get_gpu_info(),\n    \"db_mode\": DB_MODE,\n    \"db_path\": str(storage_path),\n}\njson_path = Path(pu.REPORTS_DIR) / f\"{report_name}.json\"\njson_path.write_text(json.dumps(json_data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n\nprint(f\"Report: {report_path}\")\nprint(f\"JSON:   {json_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== HINWEIS: PARAMETER-EXTRAKTION =====\n# Beste Hyperparameter werden NICHT hier extrahiert.\n# -> Verwende analyse_hyperparameter_tuning.ipynb lokal fuer:\n#    - Vollstaendige Analyse (per-epoch, per-class, confusion matrices)\n#    - Multi-DB Vergleich\n#    - Alle Optuna-Visualisierungen\n#    - Copy-paste-ready beste Hyperparameter\nprint(\"Best parameter extraction: see analyse_hyperparameter_tuning.ipynb\")\nprint(f\"DB location: {storage_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== CLEANUP + AUTO-SHUTDOWN =====\nimport shutil\n\n# Temporaere HPT-Dateien loeschen (Checkpoints etc.)\nif os.path.exists(\"/content/hpt_tmp\"):\n    # TensorBoard Logs behalten (auf lokaler Disk, nicht Drive)\n    # Nur Checkpoint-Ordner loeschen\n    for item in Path(\"/content/hpt_tmp\").iterdir():\n        if item.name != \"tb_logs\":\n            shutil.rmtree(item, ignore_errors=True)\n    print(\"Temporaere Checkpoint-Dateien geloescht.\")\n    print(\"TensorBoard Logs behalten: /content/hpt_tmp/tb_logs/\")\n\n# GPU-Speicher freigeben\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    gc.collect()\n    free_mem = torch.cuda.mem_get_info()[0] / 1e9\n    print(f\"GPU VRAM frei: {free_mem:.1f} GB\")\n\nprint(f\"\\nErgebnisse auf Google Drive:\")\nprint(f\"  Report: {report_path}\")\nprint(f\"  JSON:   {json_path}\")\nprint(f\"  DB:     {storage_path}\")\n\n# Runtime beenden (Colab Pro — spart Kosten)\nprint(\"\\nRuntime wird beendet...\")\nfrom google.colab import runtime\nruntime.unassign()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}