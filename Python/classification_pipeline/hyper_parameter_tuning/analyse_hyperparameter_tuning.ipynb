{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Analyse Hyperparameter Tuning (Optuna DB)\n",
    "\n",
    "Dieses Notebook liest eine oder mehrere Optuna SQLite-Datenbanken und liefert:\n",
    "1. **Uebersicht** — Anzahl Trials, Status-Verteilung, Dauer\n",
    "2. **Trial-Zusammenfassungen & Ranking**\n",
    "3. **Beste Parameter** — Copy-paste-ready Konfigurationen\n",
    "4. **Problemanalyse** — NaN, FAIL, PRUNED mit Ursachendiagnose\n",
    "5. **Optuna-Visualisierungen** — Alle 9 Standard-Plots + Multi-Study-Vergleich\n",
    "6. **Epoch-Level-Analyse** — Per-Epoch Lernkurven, Per-Class F1, Confusion Matrices (neues DB-Format)\n",
    "7. **Custom-Visualisierungen** — Korrelationen, Scatter, Box-Plots, Fold-Heatmap\n",
    "8. **Phase-2-Suchbereiche** — Empfohlene Parameter-Ranges\n",
    "\n",
    "---\n",
    "**Nutzung:** Nur die erste Code-Zelle (`KONFIGURATION`) anpassen, dann alle Zellen ausfuehren."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# KONFIGURATION — Hier anpassen\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# DB_PATHS: Eine oder mehrere Datenbanken (relativ zum Notebook oder absolut)\n",
    "DB_PATHS = [\n",
    "    \"../performance_reports/hpt_eurobert_210m_phase1.db\",\n",
    "]\n",
    "\n",
    "# Optional: Display-Namen fuer die Studies (None = automatisch aus DB)\n",
    "STUDY_DISPLAY_NAMES = None  # z.B. [\"Phase 1\", \"Phase 2\"]\n",
    "\n",
    "TOP_N = 5           # Anzahl Top-Trials fuer Detail-Analyse\n",
    "SAVE_PLOTS = False  # True = Plots als PNG speichern\n",
    "\n",
    "# --- Auto-Resolve Paths ---\n",
    "_resolved = []\n",
    "for _p_str in DB_PATHS:\n",
    "    _p = Path(_p_str)\n",
    "    if not _p.exists():\n",
    "        for _base in [Path(\"../performance_reports\"),\n",
    "                      Path(\"Python/classification_pipeline/performance_reports\"),\n",
    "                      Path(\"performance_reports\")]:\n",
    "            _candidate = _base / _p.name\n",
    "            if _candidate.exists():\n",
    "                _p = _candidate\n",
    "                break\n",
    "    if not _p.exists():\n",
    "        print(f\"WARNING: DB nicht gefunden: {_p_str}\")\n",
    "    else:\n",
    "        _resolved.append(str(_p.resolve()))\n",
    "\n",
    "DB_PATHS = _resolved\n",
    "print(f\"{len(DB_PATHS)} Datenbank(en) geladen:\")\n",
    "for _p in DB_PATHS:\n",
    "    print(f\"  {_p}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Imports und Setup\n",
    "# ============================================================\n",
    "import sqlite3\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_slice,\n",
    "    plot_contour,\n",
    "    plot_intermediate_values,\n",
    "    plot_rank,\n",
    "    plot_timeline,\n",
    "    plot_edf,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 40)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6g}\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (14, 6),\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"figure.dpi\": 100,\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Hilfsfunktionen\n",
    "# ============================================================\n",
    "\n",
    "def load_studies(db_paths, display_names=None):\n",
    "    \"\"\"Load Optuna studies from multiple DB files via Optuna API.\"\"\"\n",
    "    studies = []\n",
    "    for i, db_path in enumerate(db_paths):\n",
    "        storage_url = f\"sqlite:///{db_path}\"\n",
    "        summaries = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "        if not summaries:\n",
    "            print(f\"  WARNING: Keine Studies in {db_path}\")\n",
    "            continue\n",
    "        summary = summaries[0]\n",
    "        study = optuna.load_study(study_name=summary.study_name, storage=storage_url)\n",
    "        name = display_names[i] if display_names and i < len(display_names) else summary.study_name\n",
    "        studies.append({\n",
    "            \"study\": study,\n",
    "            \"name\": name,\n",
    "            \"db_path\": db_path,\n",
    "            \"storage_url\": storage_url,\n",
    "        })\n",
    "        print(f\"  Geladen: {name} ({len(study.trials)} trials)\")\n",
    "    return studies\n",
    "\n",
    "\n",
    "def load_raw_data(db_path):\n",
    "    \"\"\"Load raw trial data via sqlite3 for custom analysis.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    studies_df = pd.read_sql(\"SELECT * FROM studies\", conn)\n",
    "    study_id = int(studies_df.iloc[0][\"study_id\"])\n",
    "\n",
    "    direction_row = pd.read_sql(\n",
    "        f\"SELECT direction FROM study_directions WHERE study_id = {study_id}\", conn\n",
    "    ).iloc[0]\n",
    "    direction = direction_row[\"direction\"]\n",
    "    ascending = direction == \"MINIMIZE\"\n",
    "\n",
    "    trials = pd.read_sql(f\"SELECT * FROM trials WHERE study_id = {study_id}\", conn)\n",
    "    trials[\"datetime_start\"] = pd.to_datetime(trials[\"datetime_start\"])\n",
    "    trials[\"datetime_complete\"] = pd.to_datetime(trials[\"datetime_complete\"])\n",
    "    trials[\"duration_min\"] = (\n",
    "        (trials[\"datetime_complete\"] - trials[\"datetime_start\"]).dt.total_seconds() / 60\n",
    "    )\n",
    "\n",
    "    values = pd.read_sql(\"SELECT * FROM trial_values\", conn)\n",
    "    params_raw = pd.read_sql(\"SELECT * FROM trial_params\", conn)\n",
    "    intermediates = pd.read_sql(\"SELECT * FROM trial_intermediate_values\", conn)\n",
    "\n",
    "    # User attributes (may not exist in old DBs)\n",
    "    try:\n",
    "        user_attrs = pd.read_sql(\"SELECT * FROM trial_user_attributes\", conn)\n",
    "    except Exception:\n",
    "        user_attrs = pd.DataFrame()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Decode parameters\n",
    "    def decode_param(row):\n",
    "        dist = json.loads(row[\"distribution_json\"])\n",
    "        if dist[\"name\"] == \"CategoricalDistribution\":\n",
    "            choices = dist[\"attributes\"][\"choices\"]\n",
    "            idx = int(row[\"param_value\"])\n",
    "            return choices[idx]\n",
    "        if dist[\"name\"] == \"IntDistribution\":\n",
    "            return int(row[\"param_value\"])\n",
    "        return row[\"param_value\"]\n",
    "\n",
    "    if len(params_raw) > 0:\n",
    "        params_raw[\"decoded_value\"] = params_raw.apply(decode_param, axis=1)\n",
    "\n",
    "        param_distributions = {}\n",
    "        for _, row in params_raw.drop_duplicates(\"param_name\").iterrows():\n",
    "            dist = json.loads(row[\"distribution_json\"])\n",
    "            param_distributions[row[\"param_name\"]] = dist\n",
    "\n",
    "        params_pivot = params_raw.pivot(\n",
    "            index=\"trial_id\", columns=\"param_name\", values=\"decoded_value\"\n",
    "        ).reset_index()\n",
    "\n",
    "        df = trials.merge(values[[\"trial_id\", \"value\", \"value_type\"]], on=\"trial_id\", how=\"left\")\n",
    "        df = df.merge(params_pivot, on=\"trial_id\", how=\"left\")\n",
    "        param_names = sorted(params_pivot.columns.drop(\"trial_id\").tolist())\n",
    "    else:\n",
    "        df = trials.merge(values[[\"trial_id\", \"value\", \"value_type\"]], on=\"trial_id\", how=\"left\")\n",
    "        param_distributions = {}\n",
    "        param_names = []\n",
    "\n",
    "    df = df.sort_values(\"number\").reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        \"df\": df,\n",
    "        \"param_names\": param_names,\n",
    "        \"param_distributions\": param_distributions,\n",
    "        \"intermediates\": intermediates,\n",
    "        \"user_attrs\": user_attrs,\n",
    "        \"direction\": direction,\n",
    "        \"ascending\": ascending,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_rich_metrics(study):\n",
    "    \"\"\"Load per-epoch/fold rich metrics from trial user_attrs (neues Format von hpt_utils).\"\"\"\n",
    "    rich_data = []\n",
    "    has_rich = False\n",
    "    for trial in study.trials:\n",
    "        attrs = trial.user_attrs\n",
    "        summary = None\n",
    "        if \"trial_summary\" in attrs:\n",
    "            has_rich = True\n",
    "            summary_raw = attrs[\"trial_summary\"]\n",
    "            summary = json.loads(summary_raw) if isinstance(summary_raw, str) else summary_raw\n",
    "\n",
    "        fold_metrics = []\n",
    "        for key in sorted(attrs.keys()):\n",
    "            if key.startswith(\"fold_\") and key.endswith(\"_metrics\"):\n",
    "                val = attrs[key]\n",
    "                parsed = json.loads(val) if isinstance(val, str) else val\n",
    "                fold_metrics.append(parsed)\n",
    "\n",
    "        rich_data.append({\n",
    "            \"trial_number\": trial.number,\n",
    "            \"value\": trial.value,\n",
    "            \"state\": str(trial.state),\n",
    "            \"trial_summary\": summary,\n",
    "            \"fold_metrics\": fold_metrics,\n",
    "            \"nan_trial\": attrs.get(\"nan_trial\", False),\n",
    "        })\n",
    "    return rich_data, has_rich"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Alle Studies laden\n",
    "# ============================================================\n",
    "all_studies = load_studies(DB_PATHS, STUDY_DISPLAY_NAMES)\n",
    "all_raw = {s[\"name\"]: load_raw_data(s[\"db_path\"]) for s in all_studies}\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"  {len(all_studies)} Study/Studies geladen\")\n",
    "print(f\"{'=' * 70}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Uebersicht"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 1. UEBERSICHT\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    raw = all_raw[name]\n",
    "    df = raw[\"df\"]\n",
    "    ascending = raw[\"ascending\"]\n",
    "    param_distributions = raw[\"param_distributions\"]\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  STUDY: {name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Status\n",
    "    state_counts = df[\"state\"].value_counts()\n",
    "    print(f\"\\nTotal Trials: {len(df)}\")\n",
    "    for state, count in state_counts.items():\n",
    "        print(f\"  {state:12s}: {count}\")\n",
    "\n",
    "    # Dauer\n",
    "    completed_mask = df[\"duration_min\"].notna()\n",
    "    if completed_mask.any():\n",
    "        dur = df.loc[completed_mask, \"duration_min\"]\n",
    "        print(f\"\\nDauer (Min):\")\n",
    "        print(f\"  Min:    {dur.min():.1f}\")\n",
    "        print(f\"  Max:    {dur.max():.1f}\")\n",
    "        print(f\"  Mean:   {dur.mean():.1f}\")\n",
    "        print(f\"  Total:  {dur.sum():.0f} min ({dur.sum()/60:.1f} h)\")\n",
    "\n",
    "    # Metrik\n",
    "    valid = df[(df[\"state\"] == \"COMPLETE\") & (df[\"value\"].notna())]\n",
    "    if len(valid) > 0:\n",
    "        print(f\"\\nZielmetrik (nur COMPLETE):\")\n",
    "        print(f\"  Min:    {valid['value'].min():.6f}\")\n",
    "        print(f\"  Max:    {valid['value'].max():.6f}\")\n",
    "        print(f\"  Mean:   {valid['value'].mean():.6f}\")\n",
    "        print(f\"  Std:    {valid['value'].std():.6f}\")\n",
    "\n",
    "    # Suchbereiche\n",
    "    if param_distributions:\n",
    "        print(f\"\\nParameter-Suchbereiche:\")\n",
    "        for pname, dist in sorted(param_distributions.items()):\n",
    "            attrs = dist[\"attributes\"]\n",
    "            dtype = dist[\"name\"]\n",
    "            if dtype == \"CategoricalDistribution\":\n",
    "                print(f\"  {pname:30s}: {attrs['choices']}\")\n",
    "            elif dtype == \"IntDistribution\":\n",
    "                print(f\"  {pname:30s}: [{attrs['low']}, {attrs['high']}] (int)\")\n",
    "            else:\n",
    "                log_str = \" (log)\" if attrs.get(\"log\") else \"\"\n",
    "                print(f\"  {pname:30s}: [{attrs['low']}, {attrs['high']}]{log_str}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Trial-Zusammenfassungen & Ranking"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 2. TRIAL-ZUSAMMENFASSUNGEN & RANKING\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    raw = all_raw[name]\n",
    "    df = raw[\"df\"]\n",
    "    param_names = raw[\"param_names\"]\n",
    "    ascending = raw[\"ascending\"]\n",
    "    intermediates = raw[\"intermediates\"]\n",
    "\n",
    "    display_cols = [\"number\", \"state\", \"value\", \"duration_min\"] + param_names\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — ALLE TRIALS\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "    print(df[display_cols].to_string(index=False))\n",
    "\n",
    "    # Detail pro Trial (Fold-Ergebnisse)\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — DETAIL PRO TRIAL\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        trial_num = int(row[\"number\"])\n",
    "        state = row[\"state\"]\n",
    "        value = row[\"value\"]\n",
    "        dur = row[\"duration_min\"]\n",
    "        trial_ints = intermediates[intermediates[\"trial_id\"] == row[\"trial_id\"]].sort_values(\"step\")\n",
    "\n",
    "        print(f\"\\n--- Trial #{trial_num} [{state}] ---\")\n",
    "        if pd.notna(value):\n",
    "            print(f\"  Zielmetrik (CV Mean): {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"  Zielmetrik: KEIN WERT\")\n",
    "        if pd.notna(dur):\n",
    "            print(f\"  Dauer: {dur:.1f} min\")\n",
    "\n",
    "        if len(trial_ints) > 0:\n",
    "            fold_vals = trial_ints[\"intermediate_value\"].values\n",
    "            print(f\"  Fold-Ergebnisse: {['%.6f' % v for v in fold_vals]}\")\n",
    "            if len(fold_vals) > 1:\n",
    "                std = np.std(fold_vals)\n",
    "                spread = max(fold_vals) - min(fold_vals)\n",
    "                print(f\"  Fold-Std: {std:.6f}  |  Spread: {spread:.6f}\")\n",
    "\n",
    "    # Ranking\n",
    "    ranked = df[df[\"value\"].notna()].sort_values(\"value\", ascending=ascending).reset_index(drop=True)\n",
    "    ranked.index = ranked.index + 1\n",
    "    ranked.index.name = \"Rang\"\n",
    "\n",
    "    direction_str = raw[\"direction\"]\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — RANKING ({direction_str})\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "    print(ranked[display_cols].to_string())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Beste Parameter (Copy-Paste-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 3. BESTE PARAMETER — Kanonische Extraktion\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    raw = all_raw[name]\n",
    "    df = raw[\"df\"]\n",
    "    param_names = raw[\"param_names\"]\n",
    "    ascending = raw[\"ascending\"]\n",
    "\n",
    "    completed = df[df[\"state\"] == \"COMPLETE\"].sort_values(\"value\", ascending=ascending)\n",
    "    top_n = completed.head(TOP_N)\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — TOP {TOP_N} TRIALS\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    for rank, (_, row) in enumerate(top_n.iterrows(), 1):\n",
    "        print(f\"  #{rank}  Trial {int(row['number']):2d}  |  Metrik: {row['value']:.6f}  |  Dauer: {row['duration_min']:.1f} min\")\n",
    "\n",
    "    # Parameter-Bereiche der Top-N\n",
    "    print(f\"\\n  PARAMETER-BEREICHE DER TOP {TOP_N}:\")\n",
    "    print(f\"  {'-' * 60}\")\n",
    "    for p in param_names:\n",
    "        vals = top_n[p]\n",
    "        numeric = pd.to_numeric(vals, errors=\"coerce\")\n",
    "        if numeric.notna().all():\n",
    "            print(f\"  {p:30s}: min={numeric.min():.6g}  max={numeric.max():.6g}  mean={numeric.mean():.6g}\")\n",
    "        else:\n",
    "            print(f\"  {p:30s}: {vals.value_counts().to_dict()}\")\n",
    "\n",
    "    # Bester Trial — Copy-Paste-Ready\n",
    "    best = top_n.iloc[0]\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  BESTER TRIAL: #{int(best['number'])}  |  Metrik: {best['value']:.6f}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"\\n  Copy-paste-ready Parameter:\")\n",
    "    print(f\"  {'-' * 60}\")\n",
    "    for p in param_names:\n",
    "        val = best[p]\n",
    "        if isinstance(val, float):\n",
    "            if \"learning_rate\" in p:\n",
    "                print(f'  \"{p}\": {val:.2e},')\n",
    "            elif val == int(val):\n",
    "                print(f'  \"{p}\": {int(val)},')\n",
    "            else:\n",
    "                print(f'  \"{p}\": {val:.6g},')\n",
    "        else:\n",
    "            print(f'  \"{p}\": \"{val}\",')\n",
    "\n",
    "    # Top-N vs. Bottom-N Vergleich\n",
    "    bottom_n = completed.tail(TOP_N)\n",
    "    print(f\"\\n  PARAMETER-VERGLEICH: Top {TOP_N} vs. Bottom {TOP_N}\")\n",
    "    print(f\"  {'-' * 60}\")\n",
    "\n",
    "    for p in param_names:\n",
    "        top_vals = pd.to_numeric(top_n[p], errors=\"coerce\")\n",
    "        bot_vals = pd.to_numeric(bottom_n[p], errors=\"coerce\")\n",
    "        if top_vals.notna().all() and bot_vals.notna().all():\n",
    "            diff_pct = ((top_vals.mean() - bot_vals.mean()) / bot_vals.mean() * 100) if bot_vals.mean() != 0 else 0\n",
    "            flag = \" <--\" if abs(diff_pct) > 30 else \"\"\n",
    "            print(f\"  {p:30s}: Top={top_vals.mean():.6g}  Bot={bot_vals.mean():.6g}  ({diff_pct:+.1f}%){flag}\")\n",
    "        else:\n",
    "            top_mode = top_n[p].mode().iloc[0] if len(top_n[p].mode()) > 0 else \"N/A\"\n",
    "            bot_mode = bottom_n[p].mode().iloc[0] if len(bottom_n[p].mode()) > 0 else \"N/A\"\n",
    "            print(f\"  {p:30s}: Top=mode({top_mode})  Bot=mode({bot_mode})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Problemanalyse"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 4. PROBLEMANALYSE — NaN, FAIL, PRUNED, niedrige Scores\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    raw = all_raw[name]\n",
    "    df = raw[\"df\"]\n",
    "    param_names = raw[\"param_names\"]\n",
    "    param_distributions = raw[\"param_distributions\"]\n",
    "    intermediates = raw[\"intermediates\"]\n",
    "\n",
    "    # Problematische Trials identifizieren\n",
    "    problem_mask = (\n",
    "        (df[\"state\"] != \"COMPLETE\") |\n",
    "        (df[\"value\"].isna()) |\n",
    "        (df[\"value\"] < 0.5)\n",
    "    )\n",
    "    problems = df[problem_mask].copy()\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — PROBLEMANALYSE\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"\\nProblematische Trials: {len(problems)} von {len(df)}\")\n",
    "\n",
    "    if len(problems) == 0:\n",
    "        print(\"  Keine problematischen Trials gefunden!\")\n",
    "        continue\n",
    "\n",
    "    for _, row in problems.iterrows():\n",
    "        trial_num = int(row[\"number\"])\n",
    "        state = row[\"state\"]\n",
    "        value = row[\"value\"]\n",
    "        trial_ints = intermediates[intermediates[\"trial_id\"] == row[\"trial_id\"]].sort_values(\"step\")\n",
    "\n",
    "        print(f\"\\n{'─' * 60}\")\n",
    "        print(f\"  Trial #{trial_num}  |  Status: {state}  |  Metrik: {value if pd.notna(value) else 'KEIN WERT'}\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "\n",
    "        if len(trial_ints) > 0:\n",
    "            fold_vals = trial_ints[\"intermediate_value\"].values\n",
    "            print(f\"  Fold-Ergebnisse:\")\n",
    "            for _, iv in trial_ints.iterrows():\n",
    "                v = iv[\"intermediate_value\"]\n",
    "                flag = \" <-- NEAR ZERO!\" if v < 0.05 else (\" <-- NIEDRIG\" if v < 0.5 else \"\")\n",
    "                print(f\"    Fold {int(iv['step'])}: {v:.6f}{flag}\")\n",
    "\n",
    "        # Diagnose\n",
    "        reasons = []\n",
    "        lr = pd.to_numeric(row.get(\"learning_rate\"), errors=\"coerce\") if \"learning_rate\" in row.index else np.nan\n",
    "        wd = pd.to_numeric(row.get(\"weight_decay\"), errors=\"coerce\") if \"weight_decay\" in row.index else np.nan\n",
    "        cd = pd.to_numeric(row.get(\"classifier_dropout\"), errors=\"coerce\") if \"classifier_dropout\" in row.index else np.nan\n",
    "        warmup = pd.to_numeric(row.get(\"warmup_ratio\"), errors=\"coerce\") if \"warmup_ratio\" in row.index else np.nan\n",
    "\n",
    "        if pd.notna(cd) and cd > 0.4:\n",
    "            reasons.append(f\"Sehr hoher classifier_dropout ({cd:.3f})\")\n",
    "        if pd.notna(lr) and lr < 1.5e-5:\n",
    "            reasons.append(f\"Niedrige learning_rate ({lr:.2e})\")\n",
    "        if pd.notna(wd) and wd > 0.08:\n",
    "            reasons.append(f\"Hoher weight_decay ({wd:.4f})\")\n",
    "        if pd.notna(warmup) and warmup > 0.12:\n",
    "            reasons.append(f\"Hohe warmup_ratio ({warmup:.4f})\")\n",
    "        if pd.notna(cd) and cd > 0.3 and pd.notna(wd) and wd > 0.05:\n",
    "            reasons.append(\"Kombination Dropout + Weight Decay (doppelte Regularisierung)\")\n",
    "\n",
    "        if len(trial_ints) > 0:\n",
    "            fold_vals = trial_ints[\"intermediate_value\"].values\n",
    "            if fold_vals[0] < 0.05 and len(fold_vals) >= 2 and fold_vals[1] < 0.05:\n",
    "                reasons.append(\"Konsistent near-zero ueber Folds (systematisch)\")\n",
    "            if fold_vals[0] < 0.05 and len(fold_vals) >= 2 and fold_vals[1] > 0.3:\n",
    "                reasons.append(\"Erster Fold near-zero, spaetere besser (numerische Instabilitaet)\")\n",
    "\n",
    "        if state == \"FAIL\":\n",
    "            reasons.append(\"Trial FAIL: wahrscheinlich NaN in Gradienten\")\n",
    "        if state == \"RUNNING\":\n",
    "            reasons.append(\"RUNNING ohne Ergebnis: Session vermutlich abgebrochen\")\n",
    "\n",
    "        if reasons:\n",
    "            print(f\"  Diagnose:\")\n",
    "            for r in reasons:\n",
    "                print(f\"    -> {r}\")\n",
    "\n",
    "    # Schlussfolgerungen\n",
    "    good_trials = df[(df[\"state\"] == \"COMPLETE\") & (df[\"value\"] > 0.7)]\n",
    "    bad_trials = df[(df[\"state\"] == \"COMPLETE\") & (df[\"value\"] < 0.6)]\n",
    "    conclusions = []\n",
    "\n",
    "    if len(good_trials) > 0 and len(bad_trials) > 0:\n",
    "        good_lr = pd.to_numeric(good_trials.get(\"learning_rate\", pd.Series(dtype=float)), errors=\"coerce\").mean()\n",
    "        bad_lr = pd.to_numeric(bad_trials.get(\"learning_rate\", pd.Series(dtype=float)), errors=\"coerce\").mean()\n",
    "        if pd.notna(good_lr) and pd.notna(bad_lr) and bad_lr > 0:\n",
    "            if good_lr > bad_lr * 1.5:\n",
    "                conclusions.append(f\"Hoehere LR performen besser (gut: {good_lr:.2e} vs schlecht: {bad_lr:.2e})\")\n",
    "\n",
    "        good_ep = pd.to_numeric(good_trials.get(\"num_train_epochs\", pd.Series(dtype=float)), errors=\"coerce\").mean()\n",
    "        bad_ep = pd.to_numeric(bad_trials.get(\"num_train_epochs\", pd.Series(dtype=float)), errors=\"coerce\").mean()\n",
    "        if pd.notna(good_ep) and pd.notna(bad_ep) and bad_ep > 0:\n",
    "            if good_ep > bad_ep * 1.3:\n",
    "                conclusions.append(f\"Mehr Epochs helfen (gut: {good_ep:.0f} vs schlecht: {bad_ep:.0f})\")\n",
    "\n",
    "    if len(good_trials) > 0 and \"per_device_train_batch_size\" in good_trials.columns:\n",
    "        good_bs = good_trials[\"per_device_train_batch_size\"].value_counts()\n",
    "        if len(good_bs) > 0:\n",
    "            dominant_bs = good_bs.index[0]\n",
    "            pct = good_bs.iloc[0] / len(good_trials) * 100\n",
    "            if pct >= 70:\n",
    "                conclusions.append(f\"Batch Size {dominant_bs} dominiert bei guten Trials ({pct:.0f}%)\")\n",
    "\n",
    "    if conclusions:\n",
    "        print(f\"\\n  SCHLUSSFOLGERUNGEN:\")\n",
    "        for i, c in enumerate(conclusions, 1):\n",
    "            print(f\"    {i}. {c}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Optuna-Visualisierungen"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 5a. OPTUNA-VISUALISIERUNGEN (Single-Study)\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    study = entry[\"study\"]\n",
    "    name = entry[\"name\"]\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — Optuna-Plots\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # 1. Optimization History\n",
    "    try:\n",
    "        fig = plot_optimization_history(study)\n",
    "        fig.update_layout(title=f\"{name} — Optimization History\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_optimization_history: {e}\")\n",
    "\n",
    "    # 2. Parameter Importances\n",
    "    try:\n",
    "        fig = plot_param_importances(study)\n",
    "        fig.update_layout(title=f\"{name} — Parameter Importances\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_param_importances: {e}\")\n",
    "\n",
    "    # 3. Parallel Coordinate\n",
    "    try:\n",
    "        fig = plot_parallel_coordinate(study)\n",
    "        fig.update_layout(title=f\"{name} — Parallel Coordinate\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_parallel_coordinate: {e}\")\n",
    "\n",
    "    # 4. Slice Plot\n",
    "    try:\n",
    "        fig = plot_slice(study)\n",
    "        fig.update_layout(title=f\"{name} — Slice Plot\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_slice: {e}\")\n",
    "\n",
    "    # 5. Contour Plot\n",
    "    try:\n",
    "        fig = plot_contour(study)\n",
    "        fig.update_layout(title=f\"{name} — Contour Plot\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_contour: {e}\")\n",
    "\n",
    "    # 6. Intermediate Values (Fold-Level)\n",
    "    try:\n",
    "        fig = plot_intermediate_values(study)\n",
    "        fig.update_layout(title=f\"{name} — Intermediate Values (Folds)\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_intermediate_values: {e}\")\n",
    "\n",
    "    # 7. Rank Plot\n",
    "    try:\n",
    "        fig = plot_rank(study)\n",
    "        fig.update_layout(title=f\"{name} — Rank\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_rank: {e}\")\n",
    "\n",
    "    # 8. Timeline\n",
    "    try:\n",
    "        fig = plot_timeline(study)\n",
    "        fig.update_layout(title=f\"{name} — Timeline\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_timeline: {e}\")\n",
    "\n",
    "    # 9. EDF (Empirical Distribution Function)\n",
    "    try:\n",
    "        fig = plot_edf(study)\n",
    "        fig.update_layout(title=f\"{name} — EDF\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  plot_edf: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 5b. MULTI-STUDY-VERGLEICH (nur wenn >1 DB geladen)\n",
    "# ============================================================\n",
    "\n",
    "if len(all_studies) > 1:\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"  MULTI-STUDY-VERGLEICH\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # 1. EDF Comparison (native Optuna multi-study)\n",
    "    try:\n",
    "        optuna_studies = [e[\"study\"] for e in all_studies]\n",
    "        study_names = [e[\"name\"] for e in all_studies]\n",
    "        fig = plot_edf(optuna_studies, target_name=\"F1 Macro\")\n",
    "        fig.update_layout(title=\"Multi-Study EDF Comparison\")\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  Multi-study EDF: {e}\")\n",
    "\n",
    "    # 2. Optimization History Overlay\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for entry in all_studies:\n",
    "        raw = all_raw[entry[\"name\"]]\n",
    "        df = raw[\"df\"]\n",
    "        ascending = raw[\"ascending\"]\n",
    "        valid = df[df[\"value\"].notna()].sort_values(\"number\")\n",
    "        if len(valid) == 0:\n",
    "            continue\n",
    "        cum_best = valid[\"value\"].cummin() if ascending else valid[\"value\"].cummax()\n",
    "        ax.plot(valid[\"number\"], cum_best, \"o-\", label=entry[\"name\"], markersize=5)\n",
    "    ax.set_xlabel(\"Trial #\")\n",
    "    ax.set_ylabel(\"Kumulativ beste Metrik\")\n",
    "    ax.set_title(\"Optimization History — Vergleich\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Best Parameters Side-by-Side\n",
    "    print(f\"\\n  BESTE PARAMETER IM VERGLEICH:\")\n",
    "    print(f\"  {'Parameter':30s}\", end=\"\")\n",
    "    for entry in all_studies:\n",
    "        print(f\"  {entry['name']:>20s}\", end=\"\")\n",
    "    print()\n",
    "    print(f\"  {'-' * (30 + 22 * len(all_studies))}\")\n",
    "\n",
    "    all_param_names = set()\n",
    "    for entry in all_studies:\n",
    "        all_param_names.update(all_raw[entry[\"name\"]][\"param_names\"])\n",
    "\n",
    "    for p in sorted(all_param_names):\n",
    "        print(f\"  {p:30s}\", end=\"\")\n",
    "        for entry in all_studies:\n",
    "            raw = all_raw[entry[\"name\"]]\n",
    "            df = raw[\"df\"]\n",
    "            ascending = raw[\"ascending\"]\n",
    "            completed = df[df[\"state\"] == \"COMPLETE\"].sort_values(\"value\", ascending=ascending)\n",
    "            if len(completed) > 0 and p in completed.columns:\n",
    "                best = completed.iloc[0]\n",
    "                val = best[p]\n",
    "                if isinstance(val, float):\n",
    "                    print(f\"  {val:>20.6g}\", end=\"\")\n",
    "                else:\n",
    "                    print(f\"  {str(val):>20s}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"  {'N/A':>20s}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    # 4. Per-Class F1 Comparison (if rich metrics available)\n",
    "    has_any_rich = False\n",
    "    for entry in all_studies:\n",
    "        _, has_rich = load_rich_metrics(entry[\"study\"])\n",
    "        if has_rich:\n",
    "            has_any_rich = True\n",
    "\n",
    "    if has_any_rich:\n",
    "        fig, ax = plt.subplots(figsize=(16, 6))\n",
    "        x_offset = 0\n",
    "        bar_width = 0.8 / len(all_studies)\n",
    "\n",
    "        class_names = None\n",
    "        for i, entry in enumerate(all_studies):\n",
    "            rich_data, has_rich = load_rich_metrics(entry[\"study\"])\n",
    "            if not has_rich:\n",
    "                continue\n",
    "            # Find best trial\n",
    "            best_rd = max(\n",
    "                [rd for rd in rich_data if rd[\"trial_summary\"] is not None],\n",
    "                key=lambda rd: rd[\"trial_summary\"].get(\"mean_f1_macro\", 0),\n",
    "                default=None,\n",
    "            )\n",
    "            if best_rd is None:\n",
    "                continue\n",
    "            mean_pc = best_rd[\"trial_summary\"].get(\"mean_per_class\", {})\n",
    "            if not mean_pc:\n",
    "                continue\n",
    "            if class_names is None:\n",
    "                class_names = sorted(mean_pc.keys())\n",
    "            f1_vals = [mean_pc.get(c, {}).get(\"f1\", 0) for c in class_names]\n",
    "            x = np.arange(len(class_names)) + i * bar_width\n",
    "            ax.bar(x, f1_vals, bar_width, label=entry[\"name\"], alpha=0.8)\n",
    "\n",
    "        if class_names is not None:\n",
    "            ax.set_xticks(np.arange(len(class_names)) + bar_width * (len(all_studies) - 1) / 2)\n",
    "            ax.set_xticklabels([c.replace(\"_\", \" \") for c in class_names], rotation=45, ha=\"right\")\n",
    "            ax.set_ylabel(\"F1 Score\")\n",
    "            ax.set_title(\"Per-Class F1 — Bester Trial pro Study\")\n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"Nur eine Study geladen — Multi-Study-Vergleich uebersprungen.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Epoch-Level-Analyse (neues DB-Format)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 6. EPOCH-LEVEL-ANALYSE\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    study = entry[\"study\"]\n",
    "    raw = all_raw[name]\n",
    "    ascending = raw[\"ascending\"]\n",
    "\n",
    "    rich_data, has_rich = load_rich_metrics(study)\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — Epoch-Level-Analyse\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    if not has_rich:\n",
    "        print(\"  Keine Rich Metrics vorhanden (altes DB-Format).\")\n",
    "        print(\"  Epoch-Level-Plots werden uebersprungen.\")\n",
    "        continue\n",
    "\n",
    "    # Filter trials with rich data\n",
    "    rich_trials = [rd for rd in rich_data if rd[\"trial_summary\"] is not None]\n",
    "    if not rich_trials:\n",
    "        print(\"  Keine Trials mit trial_summary gefunden.\")\n",
    "        continue\n",
    "\n",
    "    # Sort by metric\n",
    "    rich_trials.sort(key=lambda rd: rd[\"trial_summary\"].get(\"mean_f1_macro\", 0), reverse=not ascending)\n",
    "    top_rich = rich_trials[:TOP_N]\n",
    "\n",
    "    print(f\"  {len(rich_trials)} Trials mit Rich Metrics, zeige Top {min(TOP_N, len(top_rich))}\")\n",
    "\n",
    "    # --- 6a. Learning Curves: F1 Macro per Epoch ---\n",
    "    n_folds = max(len(rd[\"fold_metrics\"]) for rd in top_rich)\n",
    "    fig, axes = plt.subplots(1, n_folds, figsize=(6 * n_folds, 5), squeeze=False)\n",
    "    axes = axes[0]\n",
    "\n",
    "    for rd in top_rich:\n",
    "        trial_num = rd[\"trial_number\"]\n",
    "        for fm in rd[\"fold_metrics\"]:\n",
    "            fold_idx = fm.get(\"fold_idx\", 0)\n",
    "            if fold_idx >= n_folds:\n",
    "                continue\n",
    "            epoch_hist = fm.get(\"epoch_history\", [])\n",
    "            if not epoch_hist:\n",
    "                continue\n",
    "            epochs = [eh[\"epoch\"] for eh in epoch_hist]\n",
    "            f1_vals = [eh.get(\"eval_f1_macro\", 0) for eh in epoch_hist]\n",
    "            axes[fold_idx].plot(epochs, f1_vals, \"o-\", label=f\"Trial #{trial_num}\", markersize=4)\n",
    "\n",
    "    for fold_idx in range(n_folds):\n",
    "        axes[fold_idx].set_xlabel(\"Epoch\")\n",
    "        axes[fold_idx].set_ylabel(\"F1 Macro\")\n",
    "        axes[fold_idx].set_title(f\"Fold {fold_idx}\")\n",
    "        axes[fold_idx].legend(fontsize=8)\n",
    "\n",
    "    fig.suptitle(f\"{name} — F1 Macro Lernkurven (Top {TOP_N})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 6b. Per-Class F1 at Final Epoch (Best Trial) ---\n",
    "    best_rd = top_rich[0]\n",
    "    best_summary = best_rd[\"trial_summary\"]\n",
    "    mean_pc = best_summary.get(\"mean_per_class\", {})\n",
    "\n",
    "    if mean_pc:\n",
    "        class_names = sorted(mean_pc.keys())\n",
    "        f1_vals = [mean_pc[c].get(\"f1\", 0) for c in class_names]\n",
    "        prec_vals = [mean_pc[c].get(\"precision\", 0) for c in class_names]\n",
    "        rec_vals = [mean_pc[c].get(\"recall\", 0) for c in class_names]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        x = np.arange(len(class_names))\n",
    "        w = 0.25\n",
    "        ax.bar(x - w, prec_vals, w, label=\"Precision\", alpha=0.8)\n",
    "        ax.bar(x, f1_vals, w, label=\"F1\", alpha=0.8)\n",
    "        ax.bar(x + w, rec_vals, w, label=\"Recall\", alpha=0.8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([c.replace(\"_\", \" \") for c in class_names], rotation=45, ha=\"right\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_title(f\"{name} — Per-Class Metriken (Bester Trial #{best_rd['trial_number']})\")\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- 6c. Confusion Matrix (Best Trial, gemittelt ueber Folds) ---\n",
    "    mean_cm = best_summary.get(\"mean_confusion_matrix\")\n",
    "    if mean_cm is not None:\n",
    "        cm = np.array(mean_cm)\n",
    "        # Get class labels\n",
    "        if mean_pc:\n",
    "            labels = sorted(mean_pc.keys())\n",
    "        else:\n",
    "            labels = [str(i) for i in range(cm.shape[0])]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        im = ax.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "\n",
    "        # Normalisierte Werte in Zellen\n",
    "        thresh = cm.max() / 2\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                val = cm[i, j]\n",
    "                if val > 0.001:\n",
    "                    color = \"white\" if val > thresh else \"black\"\n",
    "                    ax.text(j, i, f\"{val:.1f}\", ha=\"center\", va=\"center\", fontsize=8, color=color)\n",
    "\n",
    "        short_labels = [l.replace(\"_\", \"\\n\")[:15] for l in labels]\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_xticklabels(short_labels, rotation=45, ha=\"right\", fontsize=8)\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        ax.set_yticklabels(short_labels, fontsize=8)\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "        ax.set_title(f\"{name} — Confusion Matrix (Bester Trial #{best_rd['trial_number']}, Mean ueber Folds)\")\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- 6d. Early Stopping Analyse ---\n",
    "    print(f\"\\n  EARLY STOPPING ANALYSE:\")\n",
    "    print(f\"  {'Trial':>8s}  {'Best Epoch':>10s}  {'Total Epochs':>12s}  {'Peak F1':>10s}  {'Final F1':>10s}\")\n",
    "    print(f\"  {'-' * 55}\")\n",
    "\n",
    "    for rd in top_rich:\n",
    "        trial_num = rd[\"trial_number\"]\n",
    "        summary = rd[\"trial_summary\"]\n",
    "        fold_metrics_list = rd[\"fold_metrics\"]\n",
    "\n",
    "        best_epochs = []\n",
    "        total_epochs = []\n",
    "        peak_f1s = []\n",
    "        final_f1s = []\n",
    "\n",
    "        for fm in fold_metrics_list:\n",
    "            epoch_hist = fm.get(\"epoch_history\", [])\n",
    "            if not epoch_hist:\n",
    "                continue\n",
    "            f1_values = [eh.get(\"eval_f1_macro\", 0) for eh in epoch_hist]\n",
    "            best_ep = np.argmax(f1_values) + 1\n",
    "            best_epochs.append(best_ep)\n",
    "            total_epochs.append(len(epoch_hist))\n",
    "            peak_f1s.append(max(f1_values))\n",
    "            final_f1s.append(f1_values[-1])\n",
    "\n",
    "        if best_epochs:\n",
    "            print(f\"  #{trial_num:>6d}  {np.mean(best_epochs):>10.1f}  {np.mean(total_epochs):>12.1f}  \"\n",
    "                  f\"{np.mean(peak_f1s):>10.4f}  {np.mean(final_f1s):>10.4f}\")\n",
    "\n",
    "    # --- 6e. Eval Loss per Epoch (Best Trial) ---\n",
    "    has_loss = False\n",
    "    for fm in best_rd[\"fold_metrics\"]:\n",
    "        for eh in fm.get(\"epoch_history\", []):\n",
    "            if \"eval_loss\" in eh:\n",
    "                has_loss = True\n",
    "                break\n",
    "\n",
    "    if has_loss:\n",
    "        fig, axes = plt.subplots(1, n_folds, figsize=(6 * n_folds, 4), squeeze=False)\n",
    "        axes = axes[0]\n",
    "        for fm in best_rd[\"fold_metrics\"]:\n",
    "            fold_idx = fm.get(\"fold_idx\", 0)\n",
    "            if fold_idx >= n_folds:\n",
    "                continue\n",
    "            epoch_hist = fm.get(\"epoch_history\", [])\n",
    "            epochs = [eh[\"epoch\"] for eh in epoch_hist]\n",
    "            losses = [eh.get(\"eval_loss\", np.nan) for eh in epoch_hist]\n",
    "            axes[fold_idx].plot(epochs, losses, \"o-\", color=\"red\", markersize=4)\n",
    "            axes[fold_idx].set_xlabel(\"Epoch\")\n",
    "            axes[fold_idx].set_ylabel(\"Eval Loss\")\n",
    "            axes[fold_idx].set_title(f\"Fold {fold_idx}\")\n",
    "\n",
    "        fig.suptitle(f\"{name} — Eval Loss (Bester Trial #{best_rd['trial_number']})\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Custom-Visualisierungen"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 7a. Metrik-Verlauf + Parameter Scatter Plots\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    raw = all_raw[name]\n",
    "    df = raw[\"df\"]\n",
    "    param_names = raw[\"param_names\"]\n",
    "    param_distributions = raw[\"param_distributions\"]\n",
    "    ascending = raw[\"ascending\"]\n",
    "\n",
    "    # --- Metrik-Verlauf ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    has_value = df[\"value\"].notna()\n",
    "    ax = axes[0]\n",
    "    colors = []\n",
    "    for _, row in df[has_value].iterrows():\n",
    "        if row[\"state\"] == \"PRUNED\":\n",
    "            colors.append(\"orange\")\n",
    "        elif row[\"value\"] < 0.5:\n",
    "            colors.append(\"red\")\n",
    "        else:\n",
    "            colors.append(\"steelblue\")\n",
    "\n",
    "    ax.bar(df.loc[has_value, \"number\"], df.loc[has_value, \"value\"], color=colors, alpha=0.8)\n",
    "    if has_value.any():\n",
    "        best_val = df.loc[has_value, \"value\"].max() if not ascending else df.loc[has_value, \"value\"].min()\n",
    "        ax.axhline(y=best_val, color=\"green\", linestyle=\"--\", alpha=0.5, label=f\"Best: {best_val:.4f}\")\n",
    "    ax.set_xlabel(\"Trial #\")\n",
    "    ax.set_ylabel(\"Zielmetrik\")\n",
    "    ax.set_title(f\"{name} — Metrik pro Trial\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    valid_sorted = df[has_value].sort_values(\"number\")\n",
    "    if len(valid_sorted) > 0:\n",
    "        cum_best = valid_sorted[\"value\"].cummin() if ascending else valid_sorted[\"value\"].cummax()\n",
    "        ax.plot(valid_sorted[\"number\"], cum_best, \"o-\", color=\"green\", markersize=6)\n",
    "        ax.fill_between(valid_sorted[\"number\"], cum_best, alpha=0.1, color=\"green\")\n",
    "    ax.set_xlabel(\"Trial #\")\n",
    "    ax.set_ylabel(\"Beste Metrik bisher\")\n",
    "    ax.set_title(f\"{name} — Kumulativer Fortschritt\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Parameter vs. Metrik Scatter ---\n",
    "    numeric_params = [p for p in param_names if param_distributions.get(p, {}).get(\"name\") != \"CategoricalDistribution\"]\n",
    "    if not numeric_params:\n",
    "        continue\n",
    "\n",
    "    plot_df = df[(df[\"value\"].notna()) & (df[\"state\"].isin([\"COMPLETE\", \"PRUNED\"]))].copy()\n",
    "    if len(plot_df) < 3:\n",
    "        continue\n",
    "\n",
    "    n_params = len(numeric_params)\n",
    "    ncols = min(3, n_params)\n",
    "    nrows = (n_params + ncols - 1) // ncols\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4.5 * nrows))\n",
    "    if n_params == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes_flat = np.array(axes).flatten()\n",
    "\n",
    "    for i, p in enumerate(numeric_params):\n",
    "        ax = axes_flat[i]\n",
    "        vals = pd.to_numeric(plot_df[p], errors=\"coerce\")\n",
    "        metrics = plot_df[\"value\"]\n",
    "        colors_scatter = [\"orange\" if s == \"PRUNED\" else \"steelblue\" for s in plot_df[\"state\"]]\n",
    "        ax.scatter(vals, metrics, c=colors_scatter, alpha=0.7, s=60, edgecolors=\"black\", linewidths=0.5)\n",
    "\n",
    "        valid_mask = vals.notna() & metrics.notna()\n",
    "        if valid_mask.sum() > 2:\n",
    "            corr = vals[valid_mask].corr(metrics[valid_mask])\n",
    "            ax.set_title(f\"{p}\\n(r = {corr:.3f})\")\n",
    "        else:\n",
    "            ax.set_title(p)\n",
    "\n",
    "        ax.set_xlabel(p)\n",
    "        ax.set_ylabel(\"Metrik\")\n",
    "        if param_distributions.get(p, {}).get(\"attributes\", {}).get(\"log\"):\n",
    "            ax.set_xscale(\"log\")\n",
    "\n",
    "    for j in range(n_params, len(axes_flat)):\n",
    "        axes_flat[j].set_visible(False)\n",
    "\n",
    "    fig.suptitle(f\"{name} — Parameter vs. Metrik\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 7b. Kategorische Box-Plots, Fold-Heatmap, Dauer\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    raw = all_raw[name]\n",
    "    df = raw[\"df\"]\n",
    "    param_names = raw[\"param_names\"]\n",
    "    param_distributions = raw[\"param_distributions\"]\n",
    "    intermediates = raw[\"intermediates\"]\n",
    "\n",
    "    plot_df = df[(df[\"value\"].notna()) & (df[\"state\"].isin([\"COMPLETE\", \"PRUNED\"]))].copy()\n",
    "\n",
    "    # --- Kategorische Parameter Box-Plots ---\n",
    "    cat_params = [p for p in param_names if param_distributions.get(p, {}).get(\"name\") == \"CategoricalDistribution\"]\n",
    "    if cat_params and len(plot_df) > 0:\n",
    "        fig, axes = plt.subplots(1, len(cat_params), figsize=(6 * len(cat_params), 5))\n",
    "        if len(cat_params) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for i, p in enumerate(cat_params):\n",
    "            ax = axes[i]\n",
    "            plot_data = plot_df[[p, \"value\"]].dropna()\n",
    "            groups = plot_data.groupby(p)[\"value\"]\n",
    "            labels = []\n",
    "            data = []\n",
    "            for gname, group in groups:\n",
    "                labels.append(f\"{gname}\\n(n={len(group)})\")\n",
    "                data.append(group.values)\n",
    "            bp = ax.boxplot(data, labels=labels, patch_artist=True)\n",
    "            for patch in bp[\"boxes\"]:\n",
    "                patch.set_facecolor(\"lightsteelblue\")\n",
    "            ax.set_title(p)\n",
    "            ax.set_ylabel(\"Metrik\")\n",
    "\n",
    "        fig.suptitle(f\"{name} — Kategorische Parameter\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Fold-Heatmap ---\n",
    "    if len(intermediates) > 0:\n",
    "        pivot = intermediates.pivot(index=\"trial_id\", columns=\"step\", values=\"intermediate_value\")\n",
    "        trial_num_map = df.set_index(\"trial_id\")[\"number\"].to_dict()\n",
    "        pivot.index = [f\"Trial #{trial_num_map.get(tid, tid)}\" for tid in pivot.index]\n",
    "        pivot.columns = [f\"Fold {c}\" for c in pivot.columns]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, max(6, len(pivot) * 0.4)))\n",
    "        im = ax.imshow(pivot.values, cmap=\"RdYlGn\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "        ax.set_xticks(range(len(pivot.columns)))\n",
    "        ax.set_xticklabels(pivot.columns)\n",
    "        ax.set_yticks(range(len(pivot.index)))\n",
    "        ax.set_yticklabels(pivot.index)\n",
    "\n",
    "        for i in range(len(pivot.index)):\n",
    "            for j in range(len(pivot.columns)):\n",
    "                val = pivot.values[i, j]\n",
    "                if not np.isnan(val):\n",
    "                    color = \"white\" if val < 0.3 else \"black\"\n",
    "                    ax.text(j, i, f\"{val:.3f}\", ha=\"center\", va=\"center\", fontsize=9, color=color)\n",
    "\n",
    "        plt.colorbar(im, ax=ax, label=\"Metrik\")\n",
    "        ax.set_title(f\"{name} — Fold-Ergebnisse pro Trial\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Parameter-Korrelationsmatrix ---\n",
    "    numeric_params = [p for p in param_names if param_distributions.get(p, {}).get(\"name\") != \"CategoricalDistribution\"]\n",
    "    if len(numeric_params) > 1 and len(plot_df) > 3:\n",
    "        corr_cols = numeric_params + [\"value\"]\n",
    "        corr_data = plot_df[corr_cols].apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "        if len(corr_data) > 3:\n",
    "            corr_matrix = corr_data.corr()\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            im = ax.imshow(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "            labels = [c.replace(\"_\", \"\\n\") for c in corr_matrix.columns]\n",
    "            ax.set_xticks(range(len(labels)))\n",
    "            ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=9)\n",
    "            ax.set_yticks(range(len(labels)))\n",
    "            ax.set_yticklabels(labels, fontsize=9)\n",
    "            for i in range(len(corr_matrix)):\n",
    "                for j in range(len(corr_matrix)):\n",
    "                    val = corr_matrix.values[i, j]\n",
    "                    color = \"white\" if abs(val) > 0.6 else \"black\"\n",
    "                    ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=9, color=color)\n",
    "            plt.colorbar(im, ax=ax, label=\"Korrelation\")\n",
    "            ax.set_title(f\"{name} — Parameter-Korrelationsmatrix (inkl. Zielmetrik)\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # --- Dauer pro Trial ---\n",
    "    dur_df = df[df[\"duration_min\"].notna()].copy()\n",
    "    if len(dur_df) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(14, 4))\n",
    "        colors = []\n",
    "        for _, row in dur_df.iterrows():\n",
    "            if row[\"state\"] == \"FAIL\":\n",
    "                colors.append(\"red\")\n",
    "            elif row[\"state\"] == \"PRUNED\":\n",
    "                colors.append(\"orange\")\n",
    "            else:\n",
    "                colors.append(\"steelblue\")\n",
    "\n",
    "        ax.bar(dur_df[\"number\"], dur_df[\"duration_min\"], color=colors, alpha=0.8)\n",
    "        ax.set_xlabel(\"Trial #\")\n",
    "        ax.set_ylabel(\"Dauer (min)\")\n",
    "        ax.set_title(f\"{name} — Trial-Dauer\")\n",
    "        ax.legend(handles=[\n",
    "            Patch(facecolor=\"steelblue\", label=\"COMPLETE\"),\n",
    "            Patch(facecolor=\"orange\", label=\"PRUNED\"),\n",
    "            Patch(facecolor=\"red\", label=\"FAIL\"),\n",
    "        ])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Empfohlene Phase-2-Suchbereiche"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 8. VORGESCHLAGENE PHASE-2-SUCHBEREICHE\n",
    "# ============================================================\n",
    "\n",
    "for entry in all_studies:\n",
    "    name = entry[\"name\"]\n",
    "    raw = all_raw[name]\n",
    "    df = raw[\"df\"]\n",
    "    param_names = raw[\"param_names\"]\n",
    "    param_distributions = raw[\"param_distributions\"]\n",
    "    ascending = raw[\"ascending\"]\n",
    "\n",
    "    completed = df[df[\"state\"] == \"COMPLETE\"].sort_values(\"value\", ascending=ascending)\n",
    "    top_n = completed.head(TOP_N)\n",
    "\n",
    "    if len(top_n) == 0:\n",
    "        print(f\"  {name}: Keine COMPLETE Trials fuer Phase-2-Empfehlung.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {name} — PHASE-2-SUCHBEREICHE (basierend auf Top {TOP_N})\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    for p in param_names:\n",
    "        vals = top_n[p]\n",
    "        dist = param_distributions.get(p, {})\n",
    "        dist_name = dist.get(\"name\", \"\")\n",
    "        attrs = dist.get(\"attributes\", {})\n",
    "\n",
    "        if dist_name == \"CategoricalDistribution\":\n",
    "            unique = vals.unique().tolist()\n",
    "            print(f'  \"{p}\": {unique},')\n",
    "        elif dist_name == \"IntDistribution\":\n",
    "            numeric = pd.to_numeric(vals, errors=\"coerce\")\n",
    "            lo = int(numeric.min())\n",
    "            hi = int(numeric.max())\n",
    "            spread = hi - lo\n",
    "            buffer = max(1, int(spread * 0.2))\n",
    "            orig_lo = attrs.get(\"low\", lo)\n",
    "            orig_hi = attrs.get(\"high\", hi)\n",
    "            suggested_lo = max(orig_lo, lo - buffer)\n",
    "            suggested_hi = min(orig_hi, hi + buffer)\n",
    "            print(f'  \"{p}\": ({suggested_lo}, {suggested_hi}),')\n",
    "        else:\n",
    "            numeric = pd.to_numeric(vals, errors=\"coerce\")\n",
    "            lo = numeric.min()\n",
    "            hi = numeric.max()\n",
    "            spread = hi - lo\n",
    "            buffer = spread * 0.2\n",
    "            orig_lo = attrs.get(\"low\", lo)\n",
    "            orig_hi = attrs.get(\"high\", hi)\n",
    "            suggested_lo = max(orig_lo, lo - buffer)\n",
    "            suggested_hi = min(orig_hi, hi + buffer)\n",
    "            if attrs.get(\"log\"):\n",
    "                print(f'  \"{p}\": ({suggested_lo:.2e}, {suggested_hi:.2e}),  # log scale')\n",
    "            else:\n",
    "                print(f'  \"{p}\": ({suggested_lo:.4f}, {suggested_hi:.4f}),')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}