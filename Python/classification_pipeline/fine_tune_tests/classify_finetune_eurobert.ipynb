{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Classification: EuroBERT-210m\n",
    "## Model: EuroBERT/EuroBERT-210m\n",
    "\n",
    "Single-label Klassifikation (13 Klassen) mit Hugging Face Trainer API.\n",
    "Fine-Tuning auf gelabelten deutschen Nachrichtenartikeln (Bundestagswahl 2025).\n",
    "\n",
    "**Voraussetzung:** GPU-Runtime aktiviert (T4 / L4), `HF_TOKEN` in Colab Secrets hinterlegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP (in jedem Notebook identisch) ===\n",
    "import os, sys\n",
    "\n",
    "# Repo klonen / aktualisieren\n",
    "REPO = \"/content/news_articles_classification_thesis\"\n",
    "if not os.path.exists(REPO):\n",
    "    !git clone https://github.com/ZorbeyOezcan/news_articles_classification_thesis.git {REPO}\n",
    "else:\n",
    "    !cd {REPO} && git pull -q\n",
    "\n",
    "# Dependencies\n",
    "!pip install -q transformers[sentencepiece] datasets huggingface_hub scikit-learn matplotlib seaborn tqdm pandas accelerate evaluate\n",
    "\n",
    "# Google Drive mounten (persistente Reports)\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "# pipeline_utils importierbar machen\n",
    "PIPELINE_DIR = f\"{REPO}/Python/classification_pipeline\"\n",
    "if PIPELINE_DIR not in sys.path:\n",
    "    sys.path.insert(0, PIPELINE_DIR)\n",
    "\n",
    "import importlib\n",
    "import pipeline_utils as pu\n",
    "importlib.reload(pu)\n",
    "\n",
    "# HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "login(token=userdata.get(\"HF_TOKEN\"))\n",
    "\n",
    "print(f\"Reports-Ordner: {pu.REPORTS_DIR}\")\n",
    "print(\"Setup abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL CONFIG =====\n",
    "MODEL_ID = \"EuroBERT/EuroBERT-210m\"\n",
    "MODEL_SHORT_NAME = \"eurobert_210m\"\n",
    "MODEL_TYPE = \"fine-tuned\"\n",
    "\n",
    "# Trainings-Hyperparameter\n",
    "MAX_LENGTH = 2048\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_EVAL = 64\n",
    "NUM_EPOCHS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # effektive Batch Size = 16\n",
    "FP16 = True  # auf T4; fuer A100/L4 ggf. bf16\n",
    "\n",
    "# Split-Konfiguration\n",
    "TEST_PER_CLASS = 30   # Anzahl Testartikel pro Klasse (fix)\n",
    "VAL_FRACTION = 0.2    # Anteil der restlichen Daten fuer Validation\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Label-Liste (muss exakt mit dem Datensatz uebereinstimmen)\n",
    "ALL_LABELS = [\n",
    "    \"Klima / Energie\", \"Zuwanderung\", \"Renten\", \"Soziales Gefälle\",\n",
    "    \"AfD/Rechte\", \"Arbeitslosigkeit\", \"Wirtschaftslage\", \"Politikverdruss\",\n",
    "    \"Gesundheitswesen, Pflege\", \"Kosten/Löhne/Preise\",\n",
    "    \"Ukraine/Krieg/Russland\", \"Bundeswehr/Verteidigung\", \"Andere\",\n",
    "]\n",
    "\n",
    "# ===== MODEL INFO (fuer Report) =====\n",
    "MODEL_INFO = {\n",
    "    \"huggingface_id\": MODEL_ID,\n",
    "    \"language\": \"Multilingual (inkl. Deutsch)\",\n",
    "    \"max_tokens\": MAX_LENGTH,\n",
    "    \"parameters\": \"210M\",\n",
    "    \"notes\": \"EuroBERT-210m, fine-tuned for single-label classification. FP16.\",\n",
    "}\n",
    "\n",
    "print(f\"Modell: {MODEL_ID}\")\n",
    "print(f\"Max Length: {MAX_LENGTH}\")\n",
    "print(f\"Effektive Batch Size: {BATCH_SIZE_TRAIN * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Epochen: {NUM_EPOCHS}\")\n",
    "print(f\"Labels: {len(ALL_LABELS)} Klassen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATEN LADEN & CUSTOM SPLIT =====\n",
    "# Eigene Split-Logik: Test (fix pro Klasse), Rest -> Train/Val stratifiziert\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Datensatz laden\n",
    "ds = load_dataset(pu.DATASET_ID)\n",
    "\n",
    "# Train + Test kombinieren zu einem Gesamtpool\n",
    "train_hf = ds[\"train\"].to_pandas()\n",
    "test_hf = ds[\"test\"].to_pandas()\n",
    "all_labelled = pd.concat([train_hf, test_hf], ignore_index=True)\n",
    "\n",
    "print(f\"Gesamtpool gelabelter Artikel: {len(all_labelled)}\")\n",
    "print(f\"Klassen im Datensatz: {all_labelled['label'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# --- Schritt 1: Test-Split (fix, stratifiziert) ---\n",
    "test_indices = []\n",
    "rest_indices = []\n",
    "\n",
    "for label in ALL_LABELS:\n",
    "    label_mask = all_labelled[\"label\"] == label\n",
    "    label_indices = all_labelled[label_mask].index.tolist()\n",
    "    n_total = len(label_indices)\n",
    "\n",
    "    if n_total < 60:\n",
    "        # Weniger als 60 Artikel -> Haelfte fuer Test\n",
    "        n_test = n_total // 2\n",
    "        print(f\"  {label}: nur {n_total} Artikel -> {n_test} fuer Test (Haelfte)\")\n",
    "    else:\n",
    "        n_test = TEST_PER_CLASS\n",
    "\n",
    "    np.random.shuffle(label_indices)\n",
    "    test_indices.extend(label_indices[:n_test])\n",
    "    rest_indices.extend(label_indices[n_test:])\n",
    "\n",
    "test_df = all_labelled.loc[test_indices].reset_index(drop=True)\n",
    "rest_df = all_labelled.loc[rest_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTest-Split: {len(test_df)} Artikel\")\n",
    "print(f\"Verbleibend fuer Train/Val: {len(rest_df)} Artikel\")\n",
    "\n",
    "# --- Schritt 2: Train/Validation-Split (stratifiziert) ---\n",
    "# Klassen mit <2 Artikeln komplett in Train\n",
    "class_counts = rest_df[\"label\"].value_counts()\n",
    "small_classes = class_counts[class_counts < 2].index.tolist()\n",
    "\n",
    "if small_classes:\n",
    "    print(f\"\\nKlassen mit <2 Artikeln (komplett in Train): {small_classes}\")\n",
    "    small_mask = rest_df[\"label\"].isin(small_classes)\n",
    "    train_small = rest_df[small_mask]\n",
    "    rest_for_split = rest_df[~small_mask]\n",
    "else:\n",
    "    train_small = pd.DataFrame(columns=rest_df.columns)\n",
    "    rest_for_split = rest_df\n",
    "\n",
    "train_main, val_df = train_test_split(\n",
    "    rest_for_split,\n",
    "    test_size=VAL_FRACTION,\n",
    "    stratify=rest_for_split[\"label\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "train_df = pd.concat([train_main, train_small], ignore_index=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"  Train:      {len(train_df):>5} Artikel\")\n",
    "print(f\"  Validation: {len(val_df):>5} Artikel\")\n",
    "print(f\"  Test:       {len(test_df):>5} Artikel\")\n",
    "print(f\"  Gesamt:     {len(train_df) + len(val_df) + len(test_df):>5} Artikel\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Klassenverteilung\n",
    "split_overview = pd.DataFrame({\n",
    "    \"Train\": train_df[\"label\"].value_counts(),\n",
    "    \"Val\": val_df[\"label\"].value_counts(),\n",
    "    \"Test\": test_df[\"label\"].value_counts(),\n",
    "}).fillna(0).astype(int)\n",
    "split_overview[\"Gesamt\"] = split_overview.sum(axis=1)\n",
    "split_overview.loc[\"TOTAL\"] = split_overview.sum()\n",
    "print(\"\\nKlassenverteilung:\")\n",
    "print(split_overview.to_string())\n",
    "\n",
    "# Split-Config fuer Report\n",
    "split_config = {\n",
    "    \"dataset_id\": pu.DATASET_ID,\n",
    "    \"split_mode\": \"custom_finetune\",\n",
    "    \"test_per_class\": TEST_PER_CLASS,\n",
    "    \"val_fraction\": VAL_FRACTION,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"train_size\": len(train_df),\n",
    "    \"eval_size\": len(val_df),\n",
    "    \"test_size\": len(test_df),\n",
    "    \"raw_size\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LABEL ENCODING =====\n",
    "label2id = {label: idx for idx, label in enumerate(ALL_LABELS)}\n",
    "id2label = {idx: label for idx, label in enumerate(ALL_LABELS)}\n",
    "\n",
    "# Numerische label_id Spalte hinzufuegen\n",
    "train_df[\"label_id\"] = train_df[\"label\"].map(label2id)\n",
    "val_df[\"label_id\"] = val_df[\"label\"].map(label2id)\n",
    "test_df[\"label_id\"] = test_df[\"label\"].map(label2id)\n",
    "\n",
    "# Kontrolle: keine NaN-Labels\n",
    "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    n_missing = df[\"label_id\"].isna().sum()\n",
    "    if n_missing > 0:\n",
    "        print(f\"WARNUNG: {n_missing} unbekannte Labels in {name}!\")\n",
    "        print(df[df[\"label_id\"].isna()][\"label\"].unique())\n",
    "\n",
    "print(\"Label-Mapping:\")\n",
    "print(\"-\" * 40)\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {idx:>2}: {label}\")\n",
    "print(f\"\\nAnzahl Klassen: {len(ALL_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== TOKENISIERUNG =====\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n\n# DataFrames -> HuggingFace Datasets\ntrain_dataset = Dataset.from_pandas(train_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"labels\"}))\nval_dataset = Dataset.from_pandas(val_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"labels\"}))\ntest_dataset = Dataset.from_pandas(test_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"labels\"}))\n\ndef tokenize_fn(examples):\n    return tokenizer(\n        examples[\"text\"],\n        max_length=MAX_LENGTH,\n        truncation=True,\n        # Kein padding hier — wird dynamisch per DataCollatorWithPadding gemacht\n    )\n\nprint(\"Tokenisiere Trainings-Daten...\")\ntrain_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\nprint(\"Tokenisiere Validierungs-Daten...\")\nval_dataset = val_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\nprint(\"Tokenisiere Test-Daten...\")\ntest_dataset = test_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n\n# Format setzen\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Beispiel-Tokenisierung\nexample_text = train_df[\"text\"].iloc[0]\nexample_tokens = tokenizer(example_text, max_length=MAX_LENGTH, truncation=True)\nprint(f\"\\nBeispiel-Tokenisierung:\")\nprint(f\"  Textlaenge (Zeichen): {len(example_text)}\")\nprint(f\"  Token-Anzahl:         {len(example_tokens['input_ids'])}\")\nprint(f\"  Max Length:           {MAX_LENGTH}\")\nprint(f\"\\nDataset-Groessen:\")\nprint(f\"  Train:      {len(train_dataset)}\")\nprint(f\"  Validation: {len(val_dataset)}\")\nprint(f\"  Test:       {len(test_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== MODELL INITIALISIEREN =====\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoConfig\n\n# EuroBERT-Fix: Das custom modeling_eurobert.py setzt rope_type=\"default\" wenn\n# config.rope_scaling null ist. Aber aeltere transformers-Versionen haben \"default\"\n# nicht in ROPE_INIT_FUNCTIONS. Deshalb patchen wir das Dict vorab.\n# Zusaetzlich: EuroBertConfig erbt von LlamaConfig, deren attribute_map den\n# direkten Zugriff auf config.rope_theta verhindert. Daher config.to_dict() nutzen.\nfrom transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS\n\nif \"default\" not in ROPE_INIT_FUNCTIONS:\n    def _default_rope_init(config, device=None, **kwargs):\n        \"\"\"Standard-RoPE ohne Skalierung (aequivalent zu neueren transformers).\"\"\"\n        cd = config.to_dict() if hasattr(config, \"to_dict\") else vars(config)\n        base = cd.get(\"rope_theta\", 10000.0)\n        partial_rotary_factor = cd.get(\"partial_rotary_factor\", 1.0)\n        head_dim = cd.get(\"head_dim\") or (cd.get(\"hidden_size\", 768) // cd.get(\"num_attention_heads\", 12))\n        dim = int(head_dim * partial_rotary_factor)\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n        return inv_freq, 1.0\n\n    ROPE_INIT_FUNCTIONS[\"default\"] = _default_rope_init\n    print(\"ROPE_INIT_FUNCTIONS gepatcht: 'default' hinzugefuegt.\")\n\n# Config laden (trust_remote_code fuer custom EuroBertConfig)\nconfig = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\nconfig.num_labels = len(ALL_LABELS)\nconfig.id2label = id2label\nconfig.label2id = label2id\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_ID,\n    config=config,\n    ignore_mismatched_sizes=True,\n    trust_remote_code=True,\n)\n\n# Parameter zaehlen\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Modell: {MODEL_ID}\")\nprint(f\"Parameter gesamt:     {total_params:>12,}\")\nprint(f\"Parameter trainierbar:{trainable_params:>12,}\")\nprint(f\"Device: {device}\")\n\ngpu_info = pu.get_gpu_info()\nprint(f\"GPU: {gpu_info['gpu_name']} ({gpu_info['gpu_vram_gb']} GB)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== METRICS-FUNKTION =====\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Metriken fuer den Trainer: Accuracy, Balanced Accuracy, F1, Precision, Recall.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"precision_micro\": precision_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_micro\": recall_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "print(\"compute_metrics definiert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINER EINRICHTEN =====\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\n",
    "\n",
    "OUTPUT_DIR = \"/content/eurobert_finetune_output\"\n",
    "LOGGING_DIR = \"/content/drive/MyDrive/thesis_reports/training_logs/eurobert_210m\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_SIZE_EVAL,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=FP16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    seed=RANDOM_SEED,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "print(\"Trainer konfiguriert.\")\n",
    "print(f\"  Output:     {OUTPUT_DIR}\")\n",
    "print(f\"  Logging:    {LOGGING_DIR}\")\n",
    "print(f\"  Epochen:    {NUM_EPOCHS}\")\n",
    "print(f\"  LR:         {LEARNING_RATE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE_TRAIN} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE_TRAIN * GRADIENT_ACCUMULATION_STEPS} effektiv\")\n",
    "print(f\"  Early Stop: patience=2, metric=f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING STARTEN =====\n",
    "timer = pu.ExperimentTimer()\n",
    "with timer:\n",
    "    train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining abgeschlossen: {timer.duration_formatted}\")\n",
    "print(f\"\\nTraining-Metriken:\")\n",
    "for key, val in train_result.metrics.items():\n",
    "    print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING-VERLAUF PLOTTEN =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Training Loss extrahieren (aus den Step-Logs)\n",
    "train_steps = [e[\"step\"] for e in log_history if \"loss\" in e]\n",
    "train_losses = [e[\"loss\"] for e in log_history if \"loss\" in e]\n",
    "\n",
    "# Eval-Metriken extrahieren (aus den Epoch-Logs)\n",
    "eval_logs = [e for e in log_history if \"eval_loss\" in e]\n",
    "eval_epochs = [e[\"epoch\"] for e in eval_logs]\n",
    "eval_losses = [e[\"eval_loss\"] for e in eval_logs]\n",
    "eval_f1_macro = [e.get(\"eval_f1_macro\", 0) for e in eval_logs]\n",
    "eval_accuracy = [e.get(\"eval_accuracy\", 0) for e in eval_logs]\n",
    "eval_balanced_acc = [e.get(\"eval_balanced_accuracy\", 0) for e in eval_logs]\n",
    "eval_precision_macro = [e.get(\"eval_precision_macro\", 0) for e in eval_logs]\n",
    "eval_recall_macro = [e.get(\"eval_recall_macro\", 0) for e in eval_logs]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Loss\n",
    "ax1.plot(train_steps, train_losses, alpha=0.4, label=\"Train Loss (Steps)\", color=\"steelblue\")\n",
    "# Eval Loss auf der Step-Achse (am Ende jeder Epoch)\n",
    "eval_steps_approx = [e.get(\"step\", 0) for e in eval_logs]\n",
    "ax1.plot(eval_steps_approx, eval_losses, \"o-\", label=\"Eval Loss (Epoch)\", color=\"orangered\", linewidth=2)\n",
    "ax1.set_xlabel(\"Steps\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training vs. Eval Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Eval-Metriken ueber Epochen\n",
    "ax2.plot(eval_epochs, eval_f1_macro, \"o-\", label=\"F1 Macro\", linewidth=2)\n",
    "ax2.plot(eval_epochs, eval_accuracy, \"s-\", label=\"Accuracy\", linewidth=2)\n",
    "ax2.plot(eval_epochs, eval_balanced_acc, \"^-\", label=\"Balanced Accuracy\", linewidth=2)\n",
    "ax2.plot(eval_epochs, eval_precision_macro, \"d-\", label=\"Precision Macro\", linewidth=1.5, alpha=0.7)\n",
    "ax2.plot(eval_epochs, eval_recall_macro, \"v-\", label=\"Recall Macro\", linewidth=1.5, alpha=0.7)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Score\")\n",
    "ax2.set_title(\"Eval-Metriken pro Epoch\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle(\"EuroBERT-210m Fine-Tuning Verlauf\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabellarisch\n",
    "print(\"\\nEval-Metriken pro Epoch:\")\n",
    "print(f\"{'Epoch':>6} {'Loss':>8} {'F1 Macro':>10} {'Accuracy':>10} {'Bal. Acc.':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for log in eval_logs:\n",
    "    print(f\"{log['epoch']:>6.0f} {log['eval_loss']:>8.4f} {log.get('eval_f1_macro', 0):>10.4f} {log.get('eval_accuracy', 0):>10.4f} {log.get('eval_balanced_accuracy', 0):>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EVALUATION AUF TEST-SET =====\n",
    "print(\"Evaluation auf Test-Set mit bestem Modell...\")\n",
    "\n",
    "test_preds = trainer.predict(test_dataset)\n",
    "pred_ids = np.argmax(test_preds.predictions, axis=-1)\n",
    "pred_labels = [id2label[i] for i in pred_ids]\n",
    "true_labels = [id2label[i] for i in test_preds.label_ids]\n",
    "\n",
    "# Standardisierte Evaluation mit pipeline_utils\n",
    "metrics = pu.evaluate(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    labels=ALL_LABELS,\n",
    "    experiment_name=\"test\",\n",
    ")\n",
    "pu.print_metrics(metrics, \"Fine-Tuned EuroBERT-210m — Test Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFUSION MATRIX =====\n",
    "pu.plot_confusion_matrix(\n",
    "    metrics,\n",
    "    title=\"Fine-Tuned EuroBERT-210m (Test)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PER-CLASS METRICS BARPLOT =====\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pc_df = metrics[\"per_class_df\"].copy()\n",
    "pc_df = pc_df.sort_values(\"F1\", ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "y_pos = np.arange(len(pc_df))\n",
    "bar_height = 0.25\n",
    "\n",
    "ax.barh(y_pos - bar_height, pc_df[\"Precision\"], bar_height, label=\"Precision\", color=\"#2196F3\", alpha=0.85)\n",
    "ax.barh(y_pos, pc_df[\"Recall\"], bar_height, label=\"Recall\", color=\"#FF9800\", alpha=0.85)\n",
    "ax.barh(y_pos + bar_height, pc_df[\"F1\"], bar_height, label=\"F1\", color=\"#4CAF50\", alpha=0.85)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(pc_df[\"Label\"])\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_title(\"Per-Class Metrics: Fine-Tuned EuroBERT-210m\", fontsize=13, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# F1-Werte als Text\n",
    "for i, (_, row) in enumerate(pc_df.iterrows()):\n",
    "    ax.text(row[\"F1\"] + 0.01, y_pos[i] + bar_height, f\"{row['F1']:.2f}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== REPORT GENERIEREN =====\n",
    "\n",
    "# Training-Parameter fuer Report\n",
    "training_params = {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size_train\": BATCH_SIZE_TRAIN,\n",
    "    \"batch_size_eval\": BATCH_SIZE_EVAL,\n",
    "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "    \"effective_batch_size\": BATCH_SIZE_TRAIN * GRADIENT_ACCUMULATION_STEPS,\n",
    "    \"warmup_ratio\": WARMUP_RATIO,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"fp16\": FP16,\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"best_checkpoint\": trainer.state.best_model_checkpoint,\n",
    "    \"best_metric\": round(trainer.state.best_metric, 4) if trainer.state.best_metric else None,\n",
    "}\n",
    "\n",
    "# Model-Config manuell extrahieren (extract_model_config erwartet Pipeline-Objekt)\n",
    "config_dict = model.config.to_dict()\n",
    "model_config = {}\n",
    "for field in [\"architectures\", \"model_type\", \"hidden_size\", \"num_hidden_layers\",\n",
    "              \"num_attention_heads\", \"vocab_size\", \"max_position_embeddings\"]:\n",
    "    if field in config_dict:\n",
    "        val = config_dict[field]\n",
    "        if field == \"architectures\" and isinstance(val, list):\n",
    "            val = val[0] if len(val) == 1 else \", \".join(val)\n",
    "        model_config[field] = val\n",
    "\n",
    "report_path = pu.generate_report(\n",
    "    model_name=f\"{MODEL_SHORT_NAME}_finetune\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    metrics=metrics,\n",
    "    timer=timer,\n",
    "    model_info=MODEL_INFO,\n",
    "    candidate_labels=ALL_LABELS,\n",
    "    hypothesis_template=None,\n",
    "    split_config=split_config,\n",
    "    label_mapping={l: l for l in ALL_LABELS},\n",
    "    model_config=model_config,\n",
    "    training_params=training_params,\n",
    "    experiment_notes=(\n",
    "        f\"Fine-Tuned EuroBERT-210m auf {len(train_df)} Trainingsartikeln. \"\n",
    "        f\"Max Length {MAX_LENGTH}, FP16, EarlyStoppingCallback(patience=2). \"\n",
    "        f\"Custom Split: {TEST_PER_CLASS} Test/Klasse, Rest 80/20 Train/Val.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"\\nReport gespeichert: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODELL AUF HUGGINGFACE HOCHLADEN (optional) =====\n",
    "UPLOAD = False  # auf True setzen zum Hochladen\n",
    "\n",
    "if UPLOAD:\n",
    "    REPO_NAME = \"Zorryy/eurobert-210m-news-classifier-v1\"\n",
    "    url = pu.upload_model_to_hub(\n",
    "        model=trainer.model,\n",
    "        tokenizer=tokenizer,\n",
    "        repo_name=REPO_NAME,\n",
    "        private=True,\n",
    "        training_params=training_params,\n",
    "    )\n",
    "    print(f\"Model uploaded: {url}\")\n",
    "else:\n",
    "    print(\"Upload uebersprungen (UPLOAD = False).\")\n",
    "    print(\"Setze UPLOAD = True und fuehre die Zelle erneut aus, um das Modell hochzuladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SUMMARY =====\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Model:           {MODEL_ID}\")\n",
    "print(f\"  Type:            {MODEL_TYPE}\")\n",
    "print(f\"  Train:           {len(train_df)} Artikel\")\n",
    "print(f\"  Validation:      {len(val_df)} Artikel\")\n",
    "print(f\"  Test:            {len(test_df)} Artikel\")\n",
    "print(f\"  Epochen:         {NUM_EPOCHS} (best: {trainer.state.best_model_checkpoint})\")\n",
    "print(f\"  F1 Macro:        {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Weighted:     {metrics['f1_weighted']:.4f}\")\n",
    "print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision Macro: {metrics['precision_macro']:.4f}\")\n",
    "print(f\"  Recall Macro:    {metrics['recall_macro']:.4f}\")\n",
    "print(f\"  Dauer:           {timer.duration_formatted}\")\n",
    "print(f\"  Report:          {report_path}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ]
}