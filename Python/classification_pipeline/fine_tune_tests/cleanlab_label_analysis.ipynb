{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanlab Label Quality Analyse\n",
    "## K-Fold CV mit EuroBERT-210M → Out-of-Fold Predictions → Cleanlab\n",
    "\n",
    "Dieses Notebook fuehrt Option 1 (Confident Learning) durch:\n",
    "1. 3-Fold Stratified CV mit EuroBERT-210M + HPT-Params\n",
    "2. Out-of-Fold `pred_probs` sammeln (jedes Sample wird einmal predicted)\n",
    "3. Cleanlab: `find_label_issues()`, `health_summary()`, `get_label_quality_scores()`\n",
    "4. Verdaechtige Samples inspizieren\n",
    "\n",
    "**Voraussetzung:** GPU-Runtime (L4 empfohlen), `HF_TOKEN` in Colab Secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "import os, sys\n",
    "\n",
    "REPO = \"/content/news_articles_classification_thesis\"\n",
    "if not os.path.exists(REPO):\n",
    "    !git clone https://github.com/ZorbeyOezcan/news_articles_classification_thesis.git {REPO}\n",
    "else:\n",
    "    !cd {REPO} && git pull -q\n",
    "\n",
    "!pip install -q transformers[sentencepiece] datasets huggingface_hub \\\n",
    "    scikit-learn matplotlib seaborn tqdm pandas accelerate evaluate cleanlab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "PIPELINE_DIR = f\"{REPO}/Python/classification_pipeline\"\n",
    "if PIPELINE_DIR not in sys.path:\n",
    "    sys.path.insert(0, PIPELINE_DIR)\n",
    "\n",
    "import importlib\n",
    "import pipeline_utils as pu\n",
    "importlib.reload(pu)\n",
    "\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "login(token=userdata.get(\"HF_TOKEN\"))\n",
    "\n",
    "print(\"Setup abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === KONFIGURATION ===\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "MODEL_ID = \"EuroBERT/EuroBERT-210m\"\n",
    "MAX_LENGTH = 2048\n",
    "RANDOM_SEED = 42\n",
    "N_FOLDS = 3\n",
    "\n",
    "# HPT-optimierte Parameter (identisch mit train_newsbert_euro_210m.ipynb)\n",
    "LEARNING_RATE = 3.76e-05\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "NUM_EPOCHS = 13\n",
    "BATCH_SIZE_TRAIN = 4\n",
    "WARMUP_RATIO = 0.0880168\n",
    "WEIGHT_DECAY = 0.0439249\n",
    "LABEL_SMOOTHING = 0.0320202\n",
    "EFFECTIVE_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = EFFECTIVE_BATCH_SIZE // BATCH_SIZE_TRAIN\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "\n",
    "ALL_LABELS = [\n",
    "    \"Klima / Energie\", \"Zuwanderung\", \"Renten\", \"Soziales Gef\\u00e4lle\",\n",
    "    \"AfD/Rechte\", \"Arbeitslosigkeit\", \"Wirtschaftslage\", \"Politikverdruss\",\n",
    "    \"Gesundheitswesen, Pflege\", \"Kosten/L\\u00f6hne/Preise\",\n",
    "    \"Ukraine/Krieg/Russland\", \"Bundeswehr/Verteidigung\", \"Andere\",\n",
    "]\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(ALL_LABELS)}\n",
    "id2label = {idx: label for idx, label in enumerate(ALL_LABELS)}\n",
    "\n",
    "# GPU\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU benoetigt!\")\n",
    "\n",
    "_gpu_cap = torch.cuda.get_device_capability()\n",
    "_gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "USE_BF16 = _gpu_cap[0] >= 8\n",
    "USE_FP16 = not USE_BF16\n",
    "BATCH_SIZE_EVAL = 32 if _gpu_mem >= 40 else (16 if _gpu_mem >= 20 else 8)\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)} ({_gpu_mem:.1f} GB)\")\n",
    "print(f\"BF16={USE_BF16}, Eval Batch={BATCH_SIZE_EVAL}\")\n",
    "print(f\"{N_FOLDS}-Fold CV mit {len(ALL_LABELS)} Klassen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATEN LADEN ===\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "ds = load_dataset(pu.DATASET_ID)\n",
    "train_hf = ds[\"train\"].to_pandas()\n",
    "test_hf = ds[\"test\"].to_pandas()\n",
    "all_labelled = pd.concat([train_hf, test_hf], ignore_index=True)\n",
    "\n",
    "# Label IDs\n",
    "all_labelled[\"label_id\"] = all_labelled[\"label\"].map(label2id)\n",
    "assert all_labelled[\"label_id\"].isna().sum() == 0\n",
    "\n",
    "print(f\"Gesamtpool: {len(all_labelled)} Artikel, {all_labelled['label'].nunique()} Klassen\")\n",
    "print(all_labelled[\"label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOKENIZER + ROPE FIX ===\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "def _default_rope_init(config, device=None, **kwargs):\n",
    "    base = getattr(config, \"rope_theta\", 10000.0)\n",
    "    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n",
    "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "    dim = int(head_dim * partial_rotary_factor)\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "    return inv_freq, 1.0\n",
    "\n",
    "ROPE_INIT_FUNCTIONS[\"default\"] = _default_rope_init\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "print(\"Tokenizer + RoPE Fix OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === K-FOLD CV: OUT-OF-FOLD PREDICTIONS SAMMELN ===\n",
    "import gc\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, AutoConfig,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def create_model(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    config.num_labels = len(ALL_LABELS)\n",
    "    config.id2label = id2label\n",
    "    config.label2id = label2id\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID, config=config,\n",
    "        ignore_mismatched_sizes=True, trust_remote_code=True,\n",
    "    )\n",
    "    for name, module in model.named_modules():\n",
    "        if name in (\"dense\", \"classifier\") and isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    return model\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "fold_indices = list(skf.split(all_labelled, all_labelled[\"label_id\"]))\n",
    "\n",
    "# Arrays fuer Out-of-Fold Predictions\n",
    "oof_pred_probs = np.zeros((len(all_labelled), len(ALL_LABELS)))\n",
    "oof_predicted = np.full(len(all_labelled), -1, dtype=int)\n",
    "\n",
    "print(f\"Starte {N_FOLDS}-Fold CV...\\n\")\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(fold_indices):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Fold {fold_idx + 1}/{N_FOLDS}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Datasets erstellen\n",
    "    fold_train_df = all_labelled.iloc[train_idx]\n",
    "    fold_val_df = all_labelled.iloc[val_idx]\n",
    "\n",
    "    train_ds = Dataset.from_pandas(\n",
    "        fold_train_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"labels\"})\n",
    "    )\n",
    "    val_ds = Dataset.from_pandas(\n",
    "        fold_val_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"labels\"})\n",
    "    )\n",
    "\n",
    "    train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "    val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "    train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Modell\n",
    "    model = create_model(seed=RANDOM_SEED + fold_idx)\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    fold_output_dir = f\"/content/cleanlab_cv_tmp/fold_{fold_idx}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=fold_output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "        per_device_train_batch_size=BATCH_SIZE_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_SIZE_EVAL,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING,\n",
    "        fp16=USE_FP16,\n",
    "        bf16=USE_BF16,\n",
    "        gradient_checkpointing=False,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        group_by_length=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=25,\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_SEED + fold_idx,\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_pin_memory=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)],\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    trainer.train()\n",
    "\n",
    "    # Out-of-Fold Predictions (bestes Modell)\n",
    "    preds = trainer.predict(val_ds)\n",
    "    fold_probs = softmax(preds.predictions, axis=-1)\n",
    "    fold_preds = np.argmax(fold_probs, axis=-1)\n",
    "\n",
    "    fold_f1 = f1_score(preds.label_ids, fold_preds, average=\"macro\", zero_division=0)\n",
    "    print(f\"  Fold {fold_idx + 1} F1 Macro: {fold_f1:.4f}\")\n",
    "\n",
    "    # In OOF-Arrays speichern\n",
    "    oof_pred_probs[val_idx] = fold_probs\n",
    "    oof_predicted[val_idx] = fold_preds\n",
    "\n",
    "    # Cleanup\n",
    "    del trainer, model, training_args, train_ds, val_ds\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    if os.path.exists(fold_output_dir):\n",
    "        shutil.rmtree(fold_output_dir, ignore_errors=True)\n",
    "\n",
    "# Gesamt-F1\n",
    "overall_f1 = f1_score(all_labelled[\"label_id\"].values, oof_predicted, average=\"macro\", zero_division=0)\n",
    "print(f\"\\nGesamt OOF F1 Macro: {overall_f1:.4f}\")\n",
    "print(\"Out-of-Fold Predictions gesammelt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLEANLAB: LABEL ISSUES FINDEN ===\n",
    "from cleanlab.filter import find_label_issues\n",
    "from cleanlab.rank import get_label_quality_scores\n",
    "from cleanlab.dataset import health_summary\n",
    "\n",
    "labels = all_labelled[\"label_id\"].values\n",
    "\n",
    "# Label Quality Scores (0 = sehr verdaechtig, 1 = sicher korrekt)\n",
    "quality_scores = get_label_quality_scores(labels, oof_pred_probs)\n",
    "all_labelled[\"label_quality_score\"] = quality_scores\n",
    "\n",
    "# Label Issues identifizieren\n",
    "issue_mask = find_label_issues(\n",
    "    labels=labels,\n",
    "    pred_probs=oof_pred_probs,\n",
    "    return_indices_ranked_by=\"self_confidence\",\n",
    ")\n",
    "\n",
    "print(f\"Cleanlab hat {len(issue_mask)} potenzielle Label-Fehler gefunden.\")\n",
    "print(f\"Das sind {len(issue_mask) / len(all_labelled) * 100:.1f}% aller gelabelten Daten.\\n\")\n",
    "\n",
    "# Issue-Flag hinzufuegen\n",
    "all_labelled[\"is_label_issue\"] = False\n",
    "all_labelled.loc[issue_mask, \"is_label_issue\"] = True\n",
    "\n",
    "# Predicted Label hinzufuegen\n",
    "all_labelled[\"predicted_label\"] = [id2label[i] for i in oof_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLEANLAB: HEALTH SUMMARY ===\n",
    "print(\"Dataset Health Summary:\")\n",
    "print(\"=\" * 60)\n",
    "health = health_summary(labels, oof_pred_probs, class_names=ALL_LABELS)\n",
    "health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LABEL ISSUES PRO KLASSE ===\n",
    "print(\"Label Issues pro Klasse:\")\n",
    "print(f\"{'Klasse':35s}  {'Total':>5s}  {'Issues':>6s}  {'%':>6s}  {'Mean Score':>10s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for lbl in ALL_LABELS:\n",
    "    mask = all_labelled[\"label\"] == lbl\n",
    "    n_total = mask.sum()\n",
    "    n_issues = all_labelled.loc[mask, \"is_label_issue\"].sum()\n",
    "    mean_score = all_labelled.loc[mask, \"label_quality_score\"].mean()\n",
    "    pct = n_issues / n_total * 100 if n_total > 0 else 0\n",
    "    print(f\"  {lbl:35s}  {n_total:4d}    {n_issues:4d}   {pct:5.1f}%     {mean_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFUSION: GIVEN LABEL vs. PREDICTED LABEL ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Nur fuer die als Issue identifizierten Samples\n",
    "issue_df = all_labelled[all_labelled[\"is_label_issue\"]].copy()\n",
    "\n",
    "if len(issue_df) > 0:\n",
    "    cm = pd.crosstab(\n",
    "        issue_df[\"label\"],\n",
    "        issue_df[\"predicted_label\"],\n",
    "        margins=True,\n",
    "    )\n",
    "    print(\"Confusion: Given Label (Zeilen) vs. Predicted Label (Spalten)\")\n",
    "    print(\"Nur fuer als Issue identifizierte Samples:\\n\")\n",
    "    print(cm.to_string())\n",
    "\n",
    "    # Heatmap (ohne margins)\n",
    "    cm_numeric = pd.crosstab(issue_df[\"label\"], issue_df[\"predicted_label\"])\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    sns.heatmap(cm_numeric, annot=True, fmt=\"d\", cmap=\"YlOrRd\", ax=ax)\n",
    "    ax.set_title(\"Label Issues: Given vs. Predicted Label\", fontsize=13)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"Given Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Keine Label Issues gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LABEL QUALITY SCORE VERTEILUNG ===\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogramm\n",
    "ax1.hist(quality_scores, bins=50, edgecolor=\"black\", alpha=0.7, color=\"steelblue\")\n",
    "ax1.axvline(x=np.median(quality_scores), color=\"red\", linestyle=\"--\", label=f\"Median: {np.median(quality_scores):.3f}\")\n",
    "ax1.set_xlabel(\"Label Quality Score\")\n",
    "ax1.set_ylabel(\"Anzahl\")\n",
    "ax1.set_title(\"Verteilung Label Quality Scores\")\n",
    "ax1.legend()\n",
    "\n",
    "# Boxplot pro Klasse\n",
    "score_data = []\n",
    "for lbl in ALL_LABELS:\n",
    "    mask = all_labelled[\"label\"] == lbl\n",
    "    for s in quality_scores[mask]:\n",
    "        score_data.append({\"label\": lbl, \"score\": s})\n",
    "score_df = pd.DataFrame(score_data)\n",
    "score_df.boxplot(column=\"score\", by=\"label\", ax=ax2, rot=45, grid=False)\n",
    "ax2.set_title(\"Label Quality Score pro Klasse\")\n",
    "ax2.set_xlabel(\"\")\n",
    "ax2.set_ylabel(\"Quality Score\")\n",
    "fig.suptitle(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Score Statistiken:\")\n",
    "print(f\"  Mean:   {quality_scores.mean():.3f}\")\n",
    "print(f\"  Median: {np.median(quality_scores):.3f}\")\n",
    "print(f\"  Min:    {quality_scores.min():.3f}\")\n",
    "print(f\"  <0.5:   {(quality_scores < 0.5).sum()} Artikel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOP VERDAECHTIGE SAMPLES INSPIZIEREN ===\n",
    "issue_df_sorted = all_labelled[all_labelled[\"is_label_issue\"]].sort_values(\n",
    "    \"label_quality_score\", ascending=True\n",
    ")\n",
    "\n",
    "N_SHOW = 30  # Anzahl angezeigter Samples\n",
    "\n",
    "print(f\"Top {min(N_SHOW, len(issue_df_sorted))} verdaechtigste Label Issues:\\n\")\n",
    "\n",
    "for i, (_, row) in enumerate(issue_df_sorted.head(N_SHOW).iterrows()):\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"[{i+1}]  Score: {row['label_quality_score']:.3f}\")\n",
    "    print(f\"  Given Label:     {row['label']}\")\n",
    "    print(f\"  Predicted Label: {row['predicted_label']}\")\n",
    "    print(f\"  ID: {row['id']}  |  Domain: {row.get('domain', 'N/A')}\")\n",
    "    print(f\"  Headline: {row['headline']}\")\n",
    "    print(f\"  Text: {str(row['text'])[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EINZELNE KLASSE INSPIZIEREN ===\n",
    "INSPECT_CLASS = \"Andere\"  # <-- hier aendern\n",
    "\n",
    "class_issues = all_labelled[\n",
    "    (all_labelled[\"label\"] == INSPECT_CLASS) &\n",
    "    (all_labelled[\"is_label_issue\"])\n",
    "].sort_values(\"label_quality_score\")\n",
    "\n",
    "print(f\"Label Issues in '{INSPECT_CLASS}': {len(class_issues)}\\n\")\n",
    "\n",
    "for i, (_, row) in enumerate(class_issues.iterrows()):\n",
    "    print(f\"[{i+1}] Score={row['label_quality_score']:.3f}  -> Predicted: {row['predicted_label']}\")\n",
    "    print(f\"    Headline: {row['headline']}\")\n",
    "    print(f\"    Text: {str(row['text'])[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === REPORT SPEICHERN ===\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/thesis_reports/cleanlab_analysis\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Vollstaendiger DataFrame\n",
    "full_path = os.path.join(OUTPUT_DIR, \"labeled_with_cleanlab_scores.csv\")\n",
    "all_labelled.to_csv(full_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Vollstaendiger DataFrame: {full_path}\")\n",
    "\n",
    "# Nur Issues\n",
    "issues_path = os.path.join(OUTPUT_DIR, \"label_issues.csv\")\n",
    "issue_export = all_labelled[all_labelled[\"is_label_issue\"]].sort_values(\"label_quality_score\")\n",
    "issue_export[\"text_preview\"] = issue_export[\"text\"].str[:200] + \"...\"\n",
    "export_cols = [\"id\", \"label\", \"predicted_label\", \"label_quality_score\",\n",
    "               \"headline\", \"text_preview\", \"domain\"]\n",
    "export_cols = [c for c in export_cols if c in issue_export.columns]\n",
    "issue_export[export_cols].to_csv(issues_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Label Issues:            {issues_path} ({len(issue_export)} Artikel)\")\n",
    "\n",
    "# OOF Predictions speichern (fuer spaetere Analyse)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"oof_pred_probs.npy\"), oof_pred_probs)\n",
    "print(f\"OOF Predictions:         {os.path.join(OUTPUT_DIR, 'oof_pred_probs.npy')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY ===\n",
    "print(\"=\" * 70)\n",
    "print(\"CLEANLAB ANALYSE ZUSAMMENFASSUNG\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Gesamte Artikel:        {len(all_labelled)}\")\n",
    "print(f\"  OOF F1 Macro:           {overall_f1:.4f}\")\n",
    "print(f\"  Label Issues gefunden:  {all_labelled['is_label_issue'].sum()}\")\n",
    "print(f\"  Anteil Issues:          {all_labelled['is_label_issue'].mean() * 100:.1f}%\")\n",
    "print(f\"  Mean Quality Score:     {quality_scores.mean():.3f}\")\n",
    "print(f\"  Scores < 0.5:           {(quality_scores < 0.5).sum()}\")\n",
    "print(f\"\\nErgebnisse gespeichert in: {OUTPUT_DIR}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}