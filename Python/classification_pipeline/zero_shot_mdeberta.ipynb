{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification: German News Articles\n",
    "## Model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n",
    "\n",
    "This notebook evaluates zero-shot classification performance on the test split (617 articles)\n",
    "of the `Zorryy/news_articles_2025_elections_germany` dataset.\n",
    "\n",
    "**Two experiments:**\n",
    "1. Classification using **headline** only\n",
    "2. Classification using **full article text** (truncated to 512 tokens)\n",
    "\n",
    "**Primary metric:** F1 macro score across 13 German topic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets huggingface_hub scikit-learn matplotlib seaborn tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    print(\"Token loaded from Colab secrets.\")\n",
    "except Exception:\n",
    "    hf_token = input(\"Enter your HuggingFace token: \")\n",
    "\n",
    "login(token=hf_token)\n",
    "print(\"Authenticated with HuggingFace.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"Zorryy/news_articles_2025_elections_germany\"\n",
    "\n",
    "ds = load_dataset(DATASET_ID, split=\"test\", token=hf_token)\n",
    "print(f\"Test split loaded: {len(ds)} articles\")\n",
    "print(f\"Columns: {ds.column_names}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "label_counts = pd.Series(ds[\"label\"]).value_counts()\n",
    "print(label_counts.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()\n",
    "\n",
    "print(f\"Total articles: {len(df)}\")\n",
    "print(f\"\\nText length statistics (characters):\")\n",
    "print(df[\"text\"].str.len().describe().to_string())\n",
    "print(f\"\\nHeadline length statistics (characters):\")\n",
    "print(df[\"headline\"].str.len().describe().to_string())\n",
    "print(f\"\\nMissing headlines: {df['headline'].isna().sum()}\")\n",
    "print(f\"Missing text: {df['text'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_LABELS = [\n",
    "    \"Klima / Energie\",\n",
    "    \"Zuwanderung\",\n",
    "    \"Renten\",\n",
    "    \"Soziales Gef\\u00e4lle\",\n",
    "    \"AfD/Rechte\",\n",
    "    \"Arbeitslosigkeit\",\n",
    "    \"Wirtschaftslage\",\n",
    "    \"Politikverdruss\",\n",
    "    \"Gesundheitswesen, Pflege\",\n",
    "    \"Kosten/L\\u00f6hne/Preise\",\n",
    "    \"Ukraine/Krieg/Russland\",\n",
    "    \"Bundeswehr/Verteidigung\",\n",
    "    \"Andere\",\n",
    "]\n",
    "\n",
    "HYPOTHESIS_TEMPLATE = \"Dieser Text handelt von {}.\"\n",
    "\n",
    "print(f\"Number of candidate labels: {len(CANDIDATE_LABELS)}\")\n",
    "print(f\"Hypothesis template: '{HYPOTHESIS_TEMPLATE}'\")\n",
    "print(f\"\\nExample hypotheses:\")\n",
    "for label in CANDIDATE_LABELS[:3]:\n",
    "    print(f\"  '{HYPOTHESIS_TEMPLATE.format(label)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "device_name = \"GPU\" if device == 0 else \"CPU\"\n",
    "print(f\"Using device: {device_name}\")\n",
    "\n",
    "# mDeBERTa does NOT support FP16 (produces NaN). Always use FP32.\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=MODEL_ID,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_ID}\")\n",
    "print(f\"Tokenizer max length: {classifier.tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Classification Using Headlines Only\n",
    "\n",
    "Headlines are short (typically <20 tokens), well within the 512 token limit.\n",
    "No truncation needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def classify_batch(texts, classifier, candidate_labels, hypothesis_template, batch_size=16):\n    \"\"\"Run zero-shot classification with progress tracking.\n    Skips empty texts and assigns 'Andere' as default for those.\n    \"\"\"\n    predictions = [None] * len(texts)\n    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]\n    non_empty_texts = [texts[i] for i in non_empty_indices]\n\n    for i in tqdm(range(0, len(non_empty_texts), batch_size), desc=\"Classifying\"):\n        batch = non_empty_texts[i : i + batch_size]\n        batch_indices = non_empty_indices[i : i + batch_size]\n        results = classifier(\n            batch,\n            candidate_labels=candidate_labels,\n            hypothesis_template=hypothesis_template,\n            multi_label=False,\n        )\n        if isinstance(results, dict):\n            results = [results]\n        for idx, r in zip(batch_indices, results):\n            predictions[idx] = r[\"labels\"][0]\n\n    # Fill empty texts with \"Andere\"\n    empty_count = sum(1 for p in predictions if p is None)\n    if empty_count > 0:\n        print(f\"  {empty_count} empty texts -> defaulting to 'Andere'\")\n    predictions = [p if p is not None else \"Andere\" for p in predictions]\n\n    return predictions\n\n\nheadlines = df[\"headline\"].fillna(\"\").tolist()\n\nstart_time = time.time()\nheadline_predictions = classify_batch(\n    headlines, classifier, CANDIDATE_LABELS, HYPOTHESIS_TEMPLATE, batch_size=16\n)\nheadline_time = time.time() - start_time\n\ndf[\"pred_headline\"] = headline_predictions\nprint(f\"\\nHeadline classification completed in {headline_time:.1f} seconds\")\nprint(f\"({headline_time / len(headlines):.2f} sec/article)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Classification Using Full Article Text\n",
    "\n",
    "mDeBERTa has a 512 token limit. Most articles exceed this significantly.\n",
    "The transformers pipeline handles truncation automatically (keeps the first 512 tokens).\n",
    "The first 512 tokens typically cover the headline, lede, and first several paragraphs —\n",
    "the most information-dense portion of a news article (inverted pyramid structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = classifier.tokenizer\n",
    "token_lengths = []\n",
    "\n",
    "for text in tqdm(df[\"text\"].fillna(\"\").tolist(), desc=\"Tokenizing\"):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "df[\"token_count\"] = token_lengths\n",
    "\n",
    "print(f\"Token length statistics:\")\n",
    "print(pd.Series(token_lengths).describe().to_string())\n",
    "print(f\"\\nArticles exceeding 512 tokens: {sum(1 for t in token_lengths if t > 512)} / {len(token_lengths)}\")\n",
    "print(f\"Percentage truncated: {sum(1 for t in token_lengths if t > 512) / len(token_lengths) * 100:.1f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(token_lengths, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "ax.axvline(x=512, color=\"red\", linestyle=\"--\", linewidth=2, label=\"512 token limit\")\n",
    "ax.set_xlabel(\"Token count\")\n",
    "ax.set_ylabel(\"Number of articles\")\n",
    "ax.set_title(\"Article Token Length Distribution (Test Set)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"text\"].fillna(\"\").tolist()\n",
    "\n",
    "start_time = time.time()\n",
    "text_predictions = classify_batch(\n",
    "    texts, classifier, CANDIDATE_LABELS, HYPOTHESIS_TEMPLATE, batch_size=8\n",
    ")\n",
    "text_time = time.time() - start_time\n",
    "\n",
    "df[\"pred_text\"] = text_predictions\n",
    "print(f\"\\nFull-text classification completed in {text_time:.1f} seconds\")\n",
    "print(f\"({text_time / len(texts):.2f} sec/article)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Computing per-class and aggregate metrics for both experiments:\n",
    "- **F1 macro** (primary metric — accounts for class imbalance)\n",
    "- **F1 per class**\n",
    "- **Precision and Recall** (macro and per-class)\n",
    "- **Confusion matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(y_true, y_pred, labels, experiment_name):\n",
    "    \"\"\"Compute and display classification metrics.\"\"\"\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"  {experiment_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, labels=labels, average=\"macro\", zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, labels=labels, average=\"weighted\", zero_division=0)\n",
    "    precision_macro = precision_score(y_true, y_pred, labels=labels, average=\"macro\", zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, labels=labels, average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(f\"\\n  F1 Macro:        {f1_macro:.4f}\")\n",
    "    print(f\"  F1 Weighted:     {f1_weighted:.4f}\")\n",
    "    print(f\"  Precision Macro: {precision_macro:.4f}\")\n",
    "    print(f\"  Recall Macro:    {recall_macro:.4f}\")\n",
    "\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, labels=labels, output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    per_class = pd.DataFrame({\n",
    "        \"Label\": labels,\n",
    "        \"Precision\": [report[l][\"precision\"] for l in labels],\n",
    "        \"Recall\": [report[l][\"recall\"] for l in labels],\n",
    "        \"F1\": [report[l][\"f1-score\"] for l in labels],\n",
    "        \"Support\": [report[l][\"support\"] for l in labels],\n",
    "    })\n",
    "\n",
    "    print(f\"\\n  Per-Class Metrics:\")\n",
    "    print(per_class.to_string(index=False))\n",
    "\n",
    "    return {\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"per_class\": per_class,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"label\"].tolist()\n",
    "\n",
    "headline_metrics = compute_all_metrics(\n",
    "    true_labels,\n",
    "    df[\"pred_headline\"].tolist(),\n",
    "    CANDIDATE_LABELS,\n",
    "    \"Experiment 1: Headline-Based Zero-Shot Classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_metrics = compute_all_metrics(\n",
    "    true_labels,\n",
    "    df[\"pred_text\"].tolist(),\n",
    "    CANDIDATE_LABELS,\n",
    "    \"Experiment 2: Full-Text Zero-Shot Classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
    "    \"\"\"Plot raw and normalized confusion matrices side by side.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    short_labels = [l[:20] for l in labels]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(28, 11))\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=short_labels, yticklabels=short_labels,\n",
    "        ax=axes[0],\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True\")\n",
    "    axes[0].set_title(f\"{title} (Counts)\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[0].tick_params(axis=\"y\", rotation=0)\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "        xticklabels=short_labels, yticklabels=short_labels,\n",
    "        ax=axes[1],\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"True\")\n",
    "    axes[1].set_title(f\"{title} (Normalized)\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[1].tick_params(axis=\"y\", rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return cm\n",
    "\n",
    "\n",
    "headline_cm = plot_confusion_matrix(\n",
    "    true_labels,\n",
    "    df[\"pred_headline\"].tolist(),\n",
    "    CANDIDATE_LABELS,\n",
    "    \"Headline Zero-Shot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cm = plot_confusion_matrix(\n",
    "    true_labels,\n",
    "    df[\"pred_text\"].tolist(),\n",
    "    CANDIDATE_LABELS,\n",
    "    \"Full-Text Zero-Shot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Headline vs. Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    \"Label\": CANDIDATE_LABELS,\n",
    "    \"F1 (Headline)\": headline_metrics[\"per_class\"][\"F1\"].values,\n",
    "    \"F1 (Text)\": text_metrics[\"per_class\"][\"F1\"].values,\n",
    "    \"Prec (Headline)\": headline_metrics[\"per_class\"][\"Precision\"].values,\n",
    "    \"Prec (Text)\": text_metrics[\"per_class\"][\"Precision\"].values,\n",
    "    \"Rec (Headline)\": headline_metrics[\"per_class\"][\"Recall\"].values,\n",
    "    \"Rec (Text)\": text_metrics[\"per_class\"][\"Recall\"].values,\n",
    "    \"Support\": headline_metrics[\"per_class\"][\"Support\"].values.astype(int),\n",
    "})\n",
    "\n",
    "comparison[\"F1 Delta\"] = comparison[\"F1 (Text)\"] - comparison[\"F1 (Headline)\"]\n",
    "\n",
    "summary_row = pd.DataFrame([{\n",
    "    \"Label\": \"MACRO AVERAGE\",\n",
    "    \"F1 (Headline)\": headline_metrics[\"f1_macro\"],\n",
    "    \"F1 (Text)\": text_metrics[\"f1_macro\"],\n",
    "    \"Prec (Headline)\": headline_metrics[\"precision_macro\"],\n",
    "    \"Prec (Text)\": text_metrics[\"precision_macro\"],\n",
    "    \"Rec (Headline)\": headline_metrics[\"recall_macro\"],\n",
    "    \"Rec (Text)\": text_metrics[\"recall_macro\"],\n",
    "    \"Support\": len(df),\n",
    "    \"F1 Delta\": text_metrics[\"f1_macro\"] - headline_metrics[\"f1_macro\"],\n",
    "}])\n",
    "\n",
    "comparison = pd.concat([comparison, summary_row], ignore_index=True)\n",
    "\n",
    "print(\"Comparison: Headline vs. Full-Text Zero-Shot Classification\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison.to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "x = np.arange(len(CANDIDATE_LABELS))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(\n",
    "    x - width / 2,\n",
    "    headline_metrics[\"per_class\"][\"F1\"].values,\n",
    "    width,\n",
    "    label=f'Headline (F1 macro={headline_metrics[\"f1_macro\"]:.3f})',\n",
    "    color=\"#4C72B0\",\n",
    "    alpha=0.85,\n",
    ")\n",
    "ax.bar(\n",
    "    x + width / 2,\n",
    "    text_metrics[\"per_class\"][\"F1\"].values,\n",
    "    width,\n",
    "    label=f'Full Text (F1 macro={text_metrics[\"f1_macro\"]:.3f})',\n",
    "    color=\"#DD8452\",\n",
    "    alpha=0.85,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Category\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\n",
    "    \"Per-Class F1: Headline vs. Full-Text Zero-Shot Classification\\n\"\n",
    "    f\"Model: {MODEL_ID}\"\n",
    ")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([l[:18] for l in CANDIDATE_LABELS], rotation=45, ha=\"right\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = df[[\"id\", \"domain\", \"headline\", \"label\", \"pred_headline\", \"pred_text\", \"token_count\"]].copy()\n",
    "output_df.rename(columns={\"label\": \"true_label\"}, inplace=True)\n",
    "\n",
    "output_df[\"headline_correct\"] = output_df[\"true_label\"] == output_df[\"pred_headline\"]\n",
    "output_df[\"text_correct\"] = output_df[\"true_label\"] == output_df[\"pred_text\"]\n",
    "\n",
    "output_path = \"zero_shot_results_mdeberta.csv\"\n",
    "output_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "print(f\"Total rows: {len(output_df)}\")\n",
    "\n",
    "both_correct = (output_df[\"headline_correct\"] & output_df[\"text_correct\"]).sum()\n",
    "headline_only = (output_df[\"headline_correct\"] & ~output_df[\"text_correct\"]).sum()\n",
    "text_only = (~output_df[\"headline_correct\"] & output_df[\"text_correct\"]).sum()\n",
    "neither = (~output_df[\"headline_correct\"] & ~output_df[\"text_correct\"]).sum()\n",
    "\n",
    "print(f\"\\nPrediction agreement:\")\n",
    "print(f\"  Both correct:       {both_correct} ({both_correct/len(output_df)*100:.1f}%)\")\n",
    "print(f\"  Headline only:      {headline_only} ({headline_only/len(output_df)*100:.1f}%)\")\n",
    "print(f\"  Text only:          {text_only} ({text_only/len(output_df)*100:.1f}%)\")\n",
    "print(f\"  Neither correct:    {neither} ({neither/len(output_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"  FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Model:              {MODEL_ID}\")\n",
    "print(f\"  Test set size:      {len(df)} articles\")\n",
    "print(f\"  Candidate labels:   {len(CANDIDATE_LABELS)}\")\n",
    "print(f\"  Hypothesis:         '{HYPOTHESIS_TEMPLATE}'\")\n",
    "print(f\"\")\n",
    "print(f\"  HEADLINE-based:\")\n",
    "print(f\"    F1 Macro:         {headline_metrics['f1_macro']:.4f}\")\n",
    "print(f\"    F1 Weighted:      {headline_metrics['f1_weighted']:.4f}\")\n",
    "print(f\"    Runtime:          {headline_time:.1f}s\")\n",
    "print(f\"\")\n",
    "print(f\"  FULL-TEXT-based:\")\n",
    "print(f\"    F1 Macro:         {text_metrics['f1_macro']:.4f}\")\n",
    "print(f\"    F1 Weighted:      {text_metrics['f1_weighted']:.4f}\")\n",
    "print(f\"    Runtime:          {text_time:.1f}s\")\n",
    "print(f\"\")\n",
    "print(f\"  Articles truncated: {sum(1 for t in token_lengths if t > 512)} / {len(token_lengths)}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}