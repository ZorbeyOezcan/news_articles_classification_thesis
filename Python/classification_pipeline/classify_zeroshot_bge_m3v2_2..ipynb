{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification: BGE-M3\n",
    "## Model: MoritzLaurer/bge-m3-zeroshot-v2.0\n",
    "\n",
    "Zero-Shot NLI-basierte Klassifikation auf Volltext.\n",
    "Context window: 8.192 Tokens — ideal für lange Artikel.\n",
    "\n",
    "**Voraussetzung:** GPU-Runtime aktiviert, `HF_TOKEN` in Colab Secrets hinterlegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP (in jedem Notebook identisch) ===\n",
    "import os, sys\n",
    "\n",
    "# Repo klonen / aktualisieren\n",
    "REPO = \"/content/news_articles_classification_thesis\"\n",
    "if not os.path.exists(REPO):\n",
    "    !git clone https://github.com/ZorbeyOezcan/news_articles_classification_thesis.git {REPO}\n",
    "else:\n",
    "    !cd {REPO} && git pull -q\n",
    "\n",
    "# Dependencies\n",
    "!pip install -q transformers[sentencepiece] datasets huggingface_hub scikit-learn matplotlib seaborn tqdm pandas\n",
    "\n",
    "# Google Drive mounten (persistente Reports)\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "# pipeline_utils importierbar machen\n",
    "PIPELINE_DIR = f\"{REPO}/Python/classification_pipeline\"\n",
    "if PIPELINE_DIR not in sys.path:\n",
    "    sys.path.insert(0, PIPELINE_DIR)\n",
    "\n",
    "import importlib\n",
    "import pipeline_utils as pu\n",
    "importlib.reload(pu)\n",
    "\n",
    "# HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "login(token=userdata.get(\"HF_TOKEN\"))\n",
    "\n",
    "print(f\"Reports-Ordner: {pu.REPORTS_DIR}\")\n",
    "print(\"Setup abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL CONFIG =====\n",
    "MODEL_ID = \"MoritzLaurer/bge-m3-zeroshot-v2.0\"\n",
    "MODEL_SHORT_NAME = \"bge_m3_v2\"\n",
    "MODEL_TYPE = \"zero-shot\"  # \"zero-shot\" | \"few-shot\" | \"fine-tuned\"\n",
    "\n",
    "# Welcher Split wird evaluiert?\n",
    "EVALUATE_ON = \"test\"  # \"test\" oder \"eval\"\n",
    "\n",
    "# Batch-Größe für Klassifikation (kleiner = weniger VRAM, langsamer)\n",
    "# bge-m3 ist mit 0.6B Parametern größer als mDeBERTa (278M)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# ===== NLI CONFIG =====\n",
    "# Hypothesis Template für Zero-Shot NLI\n",
    "HYPOTHESIS_TEMPLATE = \"Dieser Text handelt {}.\"\n",
    "\n",
    "# Candidate Labels: None = Labels aus dem Datensatz verwenden\n",
    "# Oder eigene Liste angeben: [\"Label A\", \"Label B\", ...]\n",
    "CANDIDATE_LABELS = None\n",
    "\n",
    "# Confidence-Schwellenwert für \"Andere\"-Kategorie\n",
    "# Wenn der höchste NLI-Score unter diesem Wert liegt → \"Andere\"\n",
    "CONFIDENCE_THRESHOLD = 0.4\n",
    "\n",
    "# ===== MODEL INFO (für Report) =====\n",
    "MODEL_INFO = {\n",
    "    \"huggingface_id\": MODEL_ID,\n",
    "    \"language\": \"Multilingual (100+ Sprachen, inkl. Deutsch)\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"parameters\": \"0.6B\",\n",
    "    \"notes\": \"BGE-M3 basiert auf XLM-RoBERTa. Unterstützt FP16. 8K Context Window ideal für lange Artikel.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LABEL VERBALISIERUNG =====\n",
    "# Links: Original-Label im Datensatz\n",
    "# Rechts: NLI-Verbalisierung (wird in die Hypothese eingesetzt)\n",
    "#\n",
    "# Hypothese: \"Dieser Text handelt von {VERBALISIERUNG}.\"\n",
    "#\n",
    "# Passe die rechte Seite an, um die Verbalisierung zu ändern.\n",
    "# Tipp: Die Verbalisierung sollte grammatisch in den Satz passen.\n",
    "#\n",
    "# \"Andere\" wird NICHT verbalisiert — stattdessen per Confidence-Threshold zugewiesen.\n",
    "# Wenn der höchste NLI-Score < CONFIDENCE_THRESHOLD → \"Andere\"\n",
    "\n",
    "LABEL_MAPPING = {\n",
    "    \"Klima / Energie\":          \"vom Klima, Klimawandel oder der Energieversorgung\",          \n",
    "    \"Zuwanderung\":              \"von Zuwanderung oder Migration\",              \n",
    "    \"Renten\":                   \"von der Rente oder dem Rentensystem\",                   \n",
    "    \"Soziales Gefälle\":         \"vom Sozialen Gefälle oder von sozialer Ungleicheit\",       \n",
    "    \"AfD/Rechte\":               \"von der Brandmauer, der AfD oder dem Rechtsextremismus\",                \n",
    "    \"Arbeitslosigkeit\":         \"von Arbeitslosigkeit\",        \n",
    "    \"Wirtschaftslage\":          \"von der Wirtschaftslage oder der Zukunft der Deutschen Wirtschaft\",        \n",
    "    \"Politikverdruss\":          \"von Politikverdruss, dem Vertrauen in die Demokraite oder der Interesse für Politik bei den Bürgern\",       \n",
    "    \"Gesundheitswesen, Pflege\": \"vom Gesundheitswesen, der Pflege oder Krankenversicherungen\", \n",
    "    \"Kosten/Löhne/Preise\":      \"von steigenden Preisen und Lebenshaltungskosten oder von Löhnen\",     \n",
    "    \"Ukraine/Krieg/Russland\":   \"vom Ukraine Krieg oder von Russland\",    \n",
    "    \"Bundeswehr/Verteidigung\":  \"von der Bundeswehr, der Verteidigung Deutschlands oder Investitionen in die Rüstung\",                \n",
    "}\n",
    "# \"Andere\" → wird automatisch zugewiesen wenn max Score < CONFIDENCE_THRESHOLD\n",
    "\n",
    "# Vorschau der Hypothesen\n",
    "print(\"Vorschau der NLI-Hypothesen:\")\n",
    "print(\"-\" * 60)\n",
    "for orig, verb in LABEL_MAPPING.items():\n",
    "    changed = \" ✏️\" if orig != verb else \"\"\n",
    "    print(f\"  {orig:30s} → \\\"{HYPOTHESIS_TEMPLATE.format(verb)}\\\"{changed}\")\n",
    "print(f\"\\n  {'Andere':30s} → per Threshold (< {CONFIDENCE_THRESHOLD})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "data = pu.load_data(\n",
    "    split_mode=\"percentage\",\n",
    "    eval_fraction=0.2,\n",
    "    random_seed=42,\n",
    "    load_raw=False,\n",
    "    label_mapping=LABEL_MAPPING,\n",
    ")\n",
    "\n",
    "eval_df = data[EVALUATE_ON]\n",
    "\n",
    "# Candidate Labels: nur die 12 konkreten (ohne \"Andere\")\n",
    "if CANDIDATE_LABELS is None:\n",
    "    CANDIDATE_LABELS = list(data[\"label_mapping\"].values())\n",
    "\n",
    "# Alle Labels inkl. \"Andere\" für Evaluation\n",
    "ALL_LABELS = CANDIDATE_LABELS + [\"Andere\"]\n",
    "\n",
    "print(f\"\\nEvaluiere auf '{EVALUATE_ON}' Split: {len(eval_df)} Artikel\")\n",
    "print(f\"NLI Candidate Labels ({len(CANDIDATE_LABELS)}): {CANDIDATE_LABELS}\")\n",
    "print(f\"Evaluation Labels ({len(ALL_LABELS)}): inkl. 'Andere' per Threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell laden\n",
    "import torch\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "classifier = hf_pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=MODEL_ID,\n",
    "    device=device,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "\n",
    "print(f\"Modell geladen: {MODEL_ID}\")\n",
    "print(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "print(f\"Tokenizer max length: {classifier.tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikation (Volltext)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def classify_batch(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Zero-Shot Klassifikation mit Confidence-Threshold für 'Andere'.\"\"\"\n",
    "    predictions = [None] * len(texts)\n",
    "    threshold_count = 0\n",
    "    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]\n",
    "    non_empty_texts = [texts[i] for i in non_empty_indices]\n",
    "\n",
    "    for start in tqdm(range(0, len(non_empty_texts), batch_size), desc=\"Classifying\"):\n",
    "        batch_texts = non_empty_texts[start:start + batch_size]\n",
    "        batch_indices = non_empty_indices[start:start + batch_size]\n",
    "\n",
    "        results = classifier(\n",
    "            batch_texts,\n",
    "            candidate_labels=CANDIDATE_LABELS,\n",
    "            hypothesis_template=HYPOTHESIS_TEMPLATE,\n",
    "            multi_label=False,\n",
    "        )\n",
    "        if isinstance(results, dict):\n",
    "            results = [results]\n",
    "\n",
    "        for idx, r in zip(batch_indices, results):\n",
    "            top_label = r[\"labels\"][0]\n",
    "            top_score = r[\"scores\"][0]\n",
    "            if top_score < CONFIDENCE_THRESHOLD:\n",
    "                predictions[idx] = \"Andere\"\n",
    "                threshold_count += 1\n",
    "            else:\n",
    "                predictions[idx] = top_label\n",
    "\n",
    "    empty_count = sum(1 for p in predictions if p is None)\n",
    "    if empty_count > 0:\n",
    "        print(f\"  {empty_count} leere Texte -> 'Andere'\")\n",
    "    print(f\"  {threshold_count} Artikel per Threshold (< {CONFIDENCE_THRESHOLD}) -> 'Andere'\")\n",
    "    return [p if p is not None else \"Andere\" for p in predictions]\n",
    "\n",
    "\n",
    "texts = eval_df[\"text\"].fillna(\"\").tolist()\n",
    "true_labels = eval_df[\"label\"].tolist()\n",
    "\n",
    "timer = pu.ExperimentTimer()\n",
    "with timer:\n",
    "    predictions = classify_batch(texts)\n",
    "\n",
    "print(f\"\\nKlassifikation abgeschlossen: {timer.duration_formatted}\")\n",
    "print(f\"Durchsatz: {timer.articles_per_second(len(texts)):.2f} Artikel/Sekunde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "metrics = pu.evaluate(\n",
    "    true_labels,\n",
    "    predictions,\n",
    "    labels=ALL_LABELS,\n",
    "    experiment_name=EVALUATE_ON,\n",
    ")\n",
    "\n",
    "pu.print_metrics(metrics, f\"Zero-Shot BGE-M3 v2 — {EVALUATE_ON} Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "pu.plot_confusion_matrix(\n",
    "    metrics,\n",
    "    title=f\"Zero-Shot BGE-M3 v2 ({EVALUATE_ON})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report generieren\n",
    "report_path = pu.generate_report(\n",
    "    model_name=f\"{MODEL_SHORT_NAME}_zeroshot\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    metrics=metrics,\n",
    "    timer=timer,\n",
    "    model_info=MODEL_INFO,\n",
    "    candidate_labels=ALL_LABELS,\n",
    "    hypothesis_template=HYPOTHESIS_TEMPLATE,\n",
    "    split_config=data[\"split_config\"],\n",
    "    label_mapping=data[\"label_mapping\"],\n",
    "    experiment_notes=(\n",
    "        \"Zero-Shot NLI-Klassifikation auf Volltext mit BGE-M3 v2. \"\n",
    "        \"8K Context Window — Texte werden kaum gekürzt. FP16 aktiviert. \"\n",
    "        f\"'Andere' per Confidence-Threshold ({CONFIDENCE_THRESHOLD}) zugewiesen.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"\\nReport gespeichert: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Model:           {MODEL_ID}\")\n",
    "print(f\"  Type:            {MODEL_TYPE}\")\n",
    "print(f\"  Split:           {EVALUATE_ON} ({len(eval_df)} Artikel)\")\n",
    "print(f\"  F1 Macro:        {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Weighted:     {metrics['f1_weighted']:.4f}\")\n",
    "print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Dauer:           {timer.duration_formatted}\")\n",
    "print(f\"  Artikel/Sek:     {timer.articles_per_second(len(eval_df)):.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
