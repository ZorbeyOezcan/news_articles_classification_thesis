{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Zero-Shot Classification: BGE-M3\n## Model: MoritzLaurer/bge-m3-zeroshot-v2.0\n\nZero-Shot NLI-basierte Klassifikation auf Volltext.\nContext window: 8.192 Tokens — ideal für lange Artikel.\n\n**Voraussetzung:** GPU-Runtime aktiviert, `HF_TOKEN` in Colab Secrets hinterlegt."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === SETUP (in jedem Notebook identisch) ===\nimport os, sys\n\n# Repo klonen / aktualisieren\nREPO = \"/content/news_articles_classification_thesis\"\nif not os.path.exists(REPO):\n    !git clone https://github.com/ZorbeyOezcan/news_articles_classification_thesis.git {REPO}\nelse:\n    !cd {REPO} && git pull -q\n\n# Dependencies\n!pip install -q transformers[sentencepiece] datasets huggingface_hub scikit-learn matplotlib seaborn tqdm pandas\n\n# Google Drive mounten (persistente Reports)\nfrom google.colab import drive\ndrive.mount(\"/content/drive\", force_remount=False)\n\n# pipeline_utils importierbar machen\nPIPELINE_DIR = f\"{REPO}/Python/classification_pipeline\"\nif PIPELINE_DIR not in sys.path:\n    sys.path.insert(0, PIPELINE_DIR)\n\nimport importlib\nimport pipeline_utils as pu\nimportlib.reload(pu)\n\n# HuggingFace Login\nfrom huggingface_hub import login\nfrom google.colab import userdata\nlogin(token=userdata.get(\"HF_TOKEN\"))\n\nprint(f\"Reports-Ordner: {pu.REPORTS_DIR}\")\nprint(\"Setup abgeschlossen.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== MODEL CONFIG =====\nMODEL_ID = \"MoritzLaurer/bge-m3-zeroshot-v2.0\"\nMODEL_SHORT_NAME = \"bge_m3_v2\"\nMODEL_TYPE = \"zero-shot\"  # \"zero-shot\" | \"few-shot\" | \"fine-tuned\"\n\n# Welcher Split wird evaluiert?\nEVALUATE_ON = \"test\"  # \"test\" oder \"eval\"\n\n# Batch-Größe für Klassifikation (kleiner = weniger VRAM, langsamer)\n# bge-m3 ist mit 0.6B Parametern größer als mDeBERTa (278M)\nBATCH_SIZE = 4\n\n# ===== NLI CONFIG =====\n# Hypothesis Template für Zero-Shot NLI\nHYPOTHESIS_TEMPLATE = \"Dieser Text handelt von {}.\"\n\n# Candidate Labels: None = Labels aus dem Datensatz verwenden\n# Oder eigene Liste angeben: [\"Label A\", \"Label B\", ...]\nCANDIDATE_LABELS = None\n\n# ===== MODEL INFO (für Report) =====\nMODEL_INFO = {\n    \"huggingface_id\": MODEL_ID,\n    \"language\": \"Multilingual (100+ Sprachen, inkl. Deutsch)\",\n    \"max_tokens\": 8192,\n    \"parameters\": \"0.6B\",\n    \"notes\": \"BGE-M3 basiert auf XLM-RoBERTa. Unterstützt FP16. 8K Context Window ideal für lange Artikel.\",\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "data = pu.load_data(\n",
    "    split_mode=\"percentage\",\n",
    "    eval_fraction=0.2,\n",
    "    random_seed=42,\n",
    "    load_raw=False,\n",
    ")\n",
    "\n",
    "eval_df = data[EVALUATE_ON]\n",
    "\n",
    "# Labels bestimmen\n",
    "if CANDIDATE_LABELS is None:\n",
    "    CANDIDATE_LABELS = list(data[\"label_mapping\"].values())\n",
    "\n",
    "print(f\"\\nEvaluiere auf '{EVALUATE_ON}' Split: {len(eval_df)} Artikel\")\n",
    "print(f\"Labels: {len(CANDIDATE_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Modell laden\nimport torch\nfrom transformers import pipeline as hf_pipeline\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nclassifier = hf_pipeline(\n    \"zero-shot-classification\",\n    model=MODEL_ID,\n    device=device,\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n)\n\nprint(f\"Modell geladen: {MODEL_ID}\")\nprint(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\nprint(f\"Tokenizer max length: {classifier.tokenizer.model_max_length}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikation (Volltext)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def classify_batch(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Zero-Shot Klassifikation mit Progress-Tracking.\"\"\"\n",
    "    predictions = [None] * len(texts)\n",
    "    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]\n",
    "    non_empty_texts = [texts[i] for i in non_empty_indices]\n",
    "\n",
    "    for start in tqdm(range(0, len(non_empty_texts), batch_size), desc=\"Classifying\"):\n",
    "        batch_texts = non_empty_texts[start:start + batch_size]\n",
    "        batch_indices = non_empty_indices[start:start + batch_size]\n",
    "\n",
    "        results = classifier(\n",
    "            batch_texts,\n",
    "            candidate_labels=CANDIDATE_LABELS,\n",
    "            hypothesis_template=HYPOTHESIS_TEMPLATE,\n",
    "            multi_label=False,\n",
    "        )\n",
    "        if isinstance(results, dict):\n",
    "            results = [results]\n",
    "\n",
    "        for idx, r in zip(batch_indices, results):\n",
    "            predictions[idx] = r[\"labels\"][0]\n",
    "\n",
    "    empty_count = sum(1 for p in predictions if p is None)\n",
    "    if empty_count > 0:\n",
    "        print(f\"  {empty_count} leere Texte -> 'Andere'\")\n",
    "    return [p if p is not None else \"Andere\" for p in predictions]\n",
    "\n",
    "\n",
    "texts = eval_df[\"text\"].fillna(\"\").tolist()\n",
    "true_labels = eval_df[\"label\"].tolist()\n",
    "\n",
    "timer = pu.ExperimentTimer()\n",
    "with timer:\n",
    "    predictions = classify_batch(texts)\n",
    "\n",
    "print(f\"\\nKlassifikation abgeschlossen: {timer.duration_formatted}\")\n",
    "print(f\"Durchsatz: {timer.articles_per_second(len(texts)):.2f} Artikel/Sekunde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluation\nmetrics = pu.evaluate(\n    true_labels,\n    predictions,\n    labels=CANDIDATE_LABELS,\n    experiment_name=EVALUATE_ON,\n)\n\npu.print_metrics(metrics, f\"Zero-Shot BGE-M3 v2 — {EVALUATE_ON} Split\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion Matrix\npu.plot_confusion_matrix(\n    metrics,\n    title=f\"Zero-Shot BGE-M3 v2 ({EVALUATE_ON})\",\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Report generieren\nreport_path = pu.generate_report(\n    model_name=f\"{MODEL_SHORT_NAME}_zeroshot\",\n    model_type=MODEL_TYPE,\n    metrics=metrics,\n    timer=timer,\n    model_info=MODEL_INFO,\n    candidate_labels=CANDIDATE_LABELS,\n    hypothesis_template=HYPOTHESIS_TEMPLATE,\n    split_config=data[\"split_config\"],\n    label_mapping=data[\"label_mapping\"],\n    experiment_notes=(\n        \"Zero-Shot NLI-Klassifikation auf Volltext mit BGE-M3 v2. \"\n        \"8K Context Window — Texte werden kaum gekürzt. FP16 aktiviert.\"\n    ),\n)\n\nprint(f\"\\nReport gespeichert: {report_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Model:           {MODEL_ID}\")\n",
    "print(f\"  Type:            {MODEL_TYPE}\")\n",
    "print(f\"  Split:           {EVALUATE_ON} ({len(eval_df)} Artikel)\")\n",
    "print(f\"  F1 Macro:        {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Weighted:     {metrics['f1_weighted']:.4f}\")\n",
    "print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Dauer:           {timer.duration_formatted}\")\n",
    "print(f\"  Artikel/Sek:     {timer.articles_per_second(len(eval_df)):.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}